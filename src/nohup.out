Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.05, 'subsample': 1, 'min_child_weight': 1, 'n_estimators': 1000, 'silent': True}.
The result is 
     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0          5.195668       0.067562         5.195434        0.033067
1          5.083154       0.067959         5.082160        0.032811
2          4.979184       0.068269         4.977745        0.032512
3          4.883692       0.068358         4.881430        0.032354
4          4.795552       0.068486         4.792660        0.032229
5          4.714392       0.068921         4.710925        0.031987
6          4.639726       0.069475         4.635730        0.031945
7          4.570800       0.069763         4.566677        0.031593
8          4.508058       0.070061         4.503141        0.031434
9          4.450540       0.069703         4.445014        0.031430
10         4.397618       0.069875         4.391629        0.031323
11         4.349432       0.069893         4.342639        0.030999
12         4.305263       0.069725         4.297830        0.031146
13         4.265359       0.069643         4.256780        0.030977
14         4.228518       0.069703         4.219282        0.030687
15         4.195038       0.069670         4.184964        0.030682
16         4.164148       0.069378         4.153185        0.030849
17         4.135849       0.069207         4.124438        0.030570
18         4.110175       0.068589         4.097796        0.030468
19         4.086729       0.068226         4.073615        0.030509
20         4.065715       0.067730         4.051646        0.030598
21         4.045927       0.067456         4.031417        0.030410
22         4.027885       0.066881         4.012735        0.030777
23         4.011710       0.066739         3.995681        0.030758
24         3.996695       0.066212         3.980070        0.030822
25         3.982815       0.066037         3.965818        0.030862
26         3.970457       0.065792         3.952625        0.030915
27         3.958969       0.065858         3.940444        0.030681
28         3.948475       0.065414         3.929308        0.030844
29         3.938590       0.064921         3.919007        0.030859
..              ...            ...              ...             ...
131        3.787978       0.055180         3.726043        0.031094
132        3.787551       0.055232         3.725310        0.031130
133        3.787239       0.055036         3.724730        0.031193
134        3.787011       0.054939         3.724027        0.031360
135        3.786780       0.054918         3.723429        0.031336
136        3.786571       0.054820         3.722852        0.031330
137        3.786284       0.054746         3.722345        0.031288
138        3.786064       0.054611         3.721605        0.031396
139        3.785880       0.054698         3.721011        0.031419
140        3.785733       0.054795         3.720511        0.031389
141        3.785426       0.054891         3.719942        0.031223
142        3.785216       0.054876         3.719425        0.031186
143        3.784874       0.054926         3.718837        0.031090
144        3.784630       0.054770         3.718267        0.031120
145        3.784431       0.054791         3.717818        0.031050
146        3.784238       0.054934         3.717395        0.031070
147        3.783912       0.054961         3.716830        0.031116
148        3.783521       0.055022         3.716220        0.031056
149        3.783371       0.054904         3.715520        0.030961
150        3.783314       0.054949         3.715032        0.030939
151        3.783011       0.054933         3.714450        0.031030
152        3.782774       0.054799         3.713916        0.031051
153        3.782449       0.054732         3.713373        0.031001
154        3.782320       0.054564         3.712891        0.031043
155        3.782260       0.054557         3.712464        0.031013
156        3.782062       0.054420         3.711995        0.030934
157        3.781792       0.054428         3.711480        0.030920
158        3.781509       0.054437         3.710985        0.030938
159        3.781310       0.054551         3.710530        0.030911
160        3.781178       0.054484         3.710072        0.030878

[161 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.05, 'subsample': 1, 'min_child_weight': 3, 'n_estimators': 1000, 'silent': True}.
The result is 
     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0          5.195668       0.067562         5.195434        0.033067
1          5.083154       0.067959         5.082160        0.032811
2          4.979204       0.068273         4.977755        0.032510
3          4.883715       0.068363         4.881438        0.032352
4          4.795601       0.068496         4.792680        0.032225
5          4.714446       0.068932         4.710946        0.031982
6          4.639795       0.069489         4.635767        0.031936
7          4.570919       0.069789         4.566708        0.031585
8          4.508195       0.070091         4.503192        0.031420
9          4.450642       0.069726         4.445064        0.031416
10         4.397727       0.069901         4.391668        0.031312
11         4.349595       0.069845         4.342659        0.031033
12         4.305420       0.069674         4.297885        0.031172
13         4.265473       0.069577         4.256829        0.031005
14         4.228670       0.069638         4.219324        0.030754
15         4.195178       0.069596         4.185043        0.030740
16         4.164302       0.069286         4.153278        0.030942
17         4.136145       0.069023         4.124515        0.030692
18         4.110474       0.068403         4.097869        0.030591
19         4.087030       0.068037         4.073697        0.030641
20         4.066020       0.067539         4.051729        0.030734
21         4.046287       0.067326         4.031486        0.030579
22         4.028249       0.066754         4.012657        0.030742
23         4.012138       0.066674         3.995780        0.030917
24         3.997099       0.066142         3.980212        0.030919
25         3.983224       0.065967         3.965964        0.030959
26         3.970967       0.065774         3.952832        0.031009
27         3.959500       0.065788         3.940702        0.030714
28         3.949001       0.065342         3.929576        0.030899
29         3.939119       0.064849         3.919240        0.030930
..              ...            ...              ...             ...
113        3.794865       0.055860         3.738437        0.031718
114        3.794399       0.055781         3.737763        0.031693
115        3.794010       0.055757         3.737018        0.031492
116        3.793650       0.055732         3.736448        0.031559
117        3.793443       0.055821         3.735828        0.031605
118        3.793127       0.055743         3.735122        0.031620
119        3.792618       0.055616         3.734500        0.031616
120        3.792343       0.055572         3.733890        0.031710
121        3.792010       0.055683         3.733165        0.031710
122        3.791699       0.055589         3.732476        0.031770
123        3.791460       0.055851         3.731684        0.031707
124        3.791168       0.055831         3.731127        0.031674
125        3.790757       0.055751         3.730311        0.031741
126        3.790361       0.055831         3.729656        0.031761
127        3.790138       0.055708         3.729078        0.031681
128        3.789714       0.055795         3.728489        0.031670
129        3.789511       0.055790         3.727921        0.031690
130        3.789163       0.055753         3.727288        0.031672
131        3.788824       0.055812         3.726533        0.031656
132        3.788598       0.055702         3.725884        0.031676
133        3.788199       0.055602         3.725120        0.031784
134        3.787969       0.055671         3.724475        0.031948
135        3.787856       0.055692         3.723950        0.031891
136        3.787552       0.055610         3.723278        0.031827
137        3.787304       0.055693         3.722754        0.031918
138        3.786978       0.055535         3.722275        0.031891
139        3.786648       0.055556         3.721796        0.031927
140        3.786604       0.055632         3.721265        0.032040
141        3.786326       0.055619         3.720737        0.032001
142        3.785879       0.055651         3.720113        0.031850

[143 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.05, 'subsample': 1, 'min_child_weight': 5, 'n_estimators': 1000, 'silent': True}.
The result is 
     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0          5.195668       0.067562         5.195434        0.033067
1          5.083154       0.067959         5.082160        0.032811
2          4.979204       0.068273         4.977755        0.032510
3          4.883715       0.068363         4.881438        0.032352
4          4.795509       0.068393         4.792740        0.032159
5          4.714392       0.068872         4.711010        0.031913
6          4.639680       0.069361         4.635892        0.031801
7          4.570807       0.069666         4.566835        0.031449
8          4.508040       0.069920         4.503382        0.031218
9          4.450524       0.069597         4.445256        0.031212
10         4.397715       0.069891         4.391892        0.031103
11         4.349573       0.069828         4.342929        0.030802
12         4.305335       0.069588         4.298204        0.030889
13         4.265442       0.069514         4.257168        0.030707
14         4.228685       0.069492         4.219688        0.030474
15         4.195084       0.069329         4.185474        0.030392
16         4.164240       0.069187         4.153729        0.030594
17         4.136096       0.068944         4.124979        0.030355
18         4.110338       0.068230         4.098341        0.030249
19         4.086947       0.067922         4.074182        0.030309
20         4.065811       0.067290         4.052255        0.030360
21         4.046022       0.067018         4.031891        0.030329
22         4.027990       0.066802         4.013050        0.030403
23         4.011635       0.066619         3.996215        0.030472
24         3.996926       0.066241         3.980671        0.030467
25         3.982968       0.065947         3.966356        0.030545
26         3.970168       0.065401         3.953113        0.030623
27         3.958859       0.065301         3.940955        0.030626
28         3.948097       0.065077         3.929727        0.030393
29         3.938495       0.064697         3.919459        0.030269
..              ...            ...              ...             ...
144        3.785655       0.056382         3.720519        0.030713
145        3.785341       0.056210         3.719947        0.030882
146        3.785089       0.056130         3.719452        0.030895
147        3.784905       0.056065         3.719035        0.030899
148        3.784718       0.056016         3.718553        0.030906
149        3.784506       0.055898         3.718003        0.031084
150        3.784318       0.055919         3.717542        0.031019
151        3.784105       0.056044         3.717047        0.031000
152        3.783989       0.056101         3.716458        0.031008
153        3.783797       0.056140         3.716070        0.031011
154        3.783629       0.056308         3.715435        0.030931
155        3.783498       0.056220         3.714902        0.030802
156        3.783143       0.056106         3.714312        0.030800
157        3.783070       0.056071         3.713857        0.030834
158        3.782773       0.056036         3.713394        0.030855
159        3.782580       0.055985         3.712869        0.030866
160        3.782391       0.056093         3.712430        0.030832
161        3.782108       0.056083         3.712007        0.030862
162        3.781925       0.056152         3.711610        0.030920
163        3.781758       0.056014         3.711038        0.030931
164        3.781593       0.055922         3.710591        0.030954
165        3.781250       0.055823         3.710013        0.030954
166        3.781050       0.055869         3.709582        0.030977
167        3.780688       0.055701         3.709001        0.031069
168        3.780578       0.055748         3.708593        0.031128
169        3.780502       0.055810         3.708052        0.031096
170        3.780325       0.055685         3.707597        0.031140
171        3.780072       0.055750         3.707136        0.031026
172        3.779887       0.055593         3.706712        0.030884
173        3.779748       0.055715         3.706191        0.030976

[174 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.05, 'subsample': 0.7, 'min_child_weight': 1, 'n_estimators': 1000, 'silent': True}.
The result is 
     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0          5.195881       0.067589         5.195478        0.033134
1          5.082921       0.068127         5.081989        0.032833
2          4.978334       0.068604         4.976877        0.032145
3          4.882607       0.069235         4.880502        0.031623
4          4.794792       0.069982         4.791931        0.030991
5          4.713589       0.069572         4.710282        0.030966
6          4.639387       0.069200         4.635225        0.031067
7          4.570646       0.070020         4.566000        0.030255
8          4.507511       0.069636         4.502268        0.030592
9          4.449988       0.069594         4.444346        0.030372
10         4.397465       0.069476         4.391024        0.030540
11         4.349066       0.069662         4.341604        0.030381
12         4.305427       0.069682         4.296894        0.030489
13         4.264801       0.069104         4.255704        0.030481
14         4.227997       0.069410         4.217636        0.029936
15         4.193934       0.069318         4.182934        0.029369
16         4.163161       0.068985         4.151242        0.029299
17         4.135219       0.068758         4.122311        0.029232
18         4.109763       0.068341         4.096279        0.028997
19         4.086278       0.067842         4.072081        0.028993
20         4.064998       0.067209         4.049787        0.029245
21         4.045021       0.066724         4.029423        0.029078
22         4.027283       0.066761         4.010626        0.028982
23         4.010622       0.066598         3.993470        0.028896
24         3.995342       0.066463         3.977459        0.028848
25         3.981813       0.066059         3.963105        0.029185
26         3.969138       0.065861         3.949583        0.029440
27         3.957227       0.065074         3.937271        0.030000
28         3.946591       0.065117         3.926219        0.029904
29         3.936919       0.064980         3.916129        0.029609
..              ...            ...              ...             ...
155        3.777093       0.056690         3.702891        0.029600
156        3.776812       0.056620         3.702254        0.029663
157        3.776528       0.056492         3.701664        0.029600
158        3.776247       0.056379         3.701192        0.029577
159        3.776003       0.056395         3.700553        0.029696
160        3.775836       0.056326         3.700055        0.029678
161        3.775792       0.056294         3.699522        0.029760
162        3.775674       0.056549         3.698930        0.029908
163        3.775563       0.056419         3.698322        0.029814
164        3.775343       0.056378         3.697729        0.029750
165        3.775057       0.056436         3.697214        0.029742
166        3.774894       0.056487         3.696678        0.029836
167        3.774838       0.056290         3.696155        0.029792
168        3.774650       0.056282         3.695680        0.029842
169        3.774452       0.056272         3.695197        0.029805
170        3.774242       0.056251         3.694654        0.029921
171        3.774059       0.056388         3.694072        0.029959
172        3.773850       0.056549         3.693536        0.029943
173        3.773632       0.056550         3.693040        0.029892
174        3.773584       0.056598         3.692468        0.029913
175        3.773453       0.056587         3.691954        0.029975
176        3.773345       0.056454         3.691512        0.029993
177        3.773004       0.056567         3.691029        0.029936
178        3.772952       0.056485         3.690571        0.029926
179        3.772807       0.056540         3.690089        0.029898
180        3.772549       0.056453         3.689549        0.030009
181        3.772501       0.056490         3.689178        0.030107
182        3.772451       0.056506         3.688780        0.030128
183        3.772285       0.056655         3.688309        0.030061
184        3.772198       0.056500         3.687828        0.029971

[185 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.05, 'subsample': 0.7, 'min_child_weight': 3, 'n_estimators': 1000, 'silent': True}.
The result is 
     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0          5.195915       0.067595         5.195456        0.033138
1          5.082956       0.068132         5.081968        0.032838
2          4.978378       0.068611         4.976861        0.032149
3          4.882647       0.069242         4.880483        0.031628
4          4.795117       0.070220         4.792066        0.030858
5          4.714040       0.069902         4.710496        0.030779
6          4.639713       0.069426         4.635505        0.030798
7          4.570974       0.070249         4.566276        0.029995
8          4.507918       0.069881         4.502597        0.030314
9          4.450386       0.069837         4.444668        0.030099
10         4.397859       0.069715         4.391342        0.030271
11         4.349411       0.069891         4.341988        0.030097
12         4.305912       0.069990         4.297188        0.030217
13         4.265278       0.069412         4.255960        0.030222
14         4.228333       0.069607         4.217946        0.029643
15         4.194270       0.069512         4.183255        0.029075
16         4.163410       0.069157         4.151595        0.029000
17         4.135464       0.068929         4.122672        0.028926
18         4.110359       0.068924         4.096659        0.028824
19         4.086784       0.068390         4.072519        0.028810
20         4.065409       0.067657         4.050249        0.029042
21         4.045493       0.066989         4.029868        0.028937
22         4.027866       0.066905         4.011079        0.028871
23         4.011151       0.066708         3.993923        0.028769
24         3.995826       0.066500         3.977922        0.028715
25         3.981892       0.065660         3.963544        0.029085
26         3.969196       0.065468         3.949976        0.029327
27         3.957450       0.064853         3.937809        0.029750
28         3.946833       0.064896         3.926785        0.029650
29         3.937174       0.064756         3.916704        0.029349
..              ...            ...              ...             ...
176        3.774329       0.056713         3.693340        0.030802
177        3.774039       0.056870         3.692912        0.030728
178        3.773941       0.056882         3.692459        0.030717
179        3.773837       0.056875         3.691998        0.030713
180        3.773599       0.056799         3.691496        0.030807
181        3.773536       0.056860         3.691100        0.030901
182        3.773471       0.056852         3.690722        0.030922
183        3.773289       0.056962         3.690271        0.030865
184        3.773127       0.056929         3.689754        0.030749
185        3.773117       0.056934         3.689291        0.030791
186        3.773030       0.056977         3.688837        0.030749
187        3.772943       0.056961         3.688372        0.030721
188        3.772633       0.056887         3.687879        0.030680
189        3.772433       0.056801         3.687357        0.030749
190        3.772183       0.056732         3.686883        0.030768
191        3.771916       0.056658         3.686453        0.030738
192        3.771874       0.056648         3.685985        0.030736
193        3.771812       0.056560         3.685602        0.030687
194        3.771667       0.056648         3.685211        0.030608
195        3.771472       0.056447         3.684643        0.030590
196        3.771376       0.056590         3.684125        0.030511
197        3.771187       0.056689         3.683624        0.030512
198        3.771004       0.056801         3.683166        0.030467
199        3.770859       0.056870         3.682746        0.030426
200        3.770786       0.056982         3.682301        0.030376
201        3.770705       0.056884         3.681982        0.030303
202        3.770679       0.056889         3.681554        0.030249
203        3.770417       0.056989         3.681155        0.030168
204        3.770132       0.057028         3.680693        0.030189
205        3.770057       0.056930         3.680362        0.030200

[206 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.05, 'subsample': 0.7, 'min_child_weight': 5, 'n_estimators': 1000, 'silent': True}.
The result is 
     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0          5.195915       0.067595         5.195456        0.033138
1          5.082799       0.067982         5.081972        0.032828
2          4.978229       0.068470         4.976869        0.032132
3          4.882675       0.068874         4.880480        0.031597
4          4.795031       0.069840         4.792216        0.030781
5          4.713857       0.069613         4.710648        0.030666
6          4.639536       0.069126         4.635751        0.030649
7          4.570781       0.070018         4.566582        0.029930
8          4.507733       0.069656         4.502917        0.030247
9          4.450276       0.069533         4.444994        0.030029
10         4.397750       0.069423         4.391660        0.030190
11         4.349148       0.069824         4.342232        0.029902
12         4.305576       0.069897         4.297560        0.030005
13         4.264978       0.069298         4.256359        0.030026
14         4.228132       0.069560         4.218420        0.029560
15         4.194213       0.069295         4.183729        0.028978
16         4.163702       0.069097         4.152257        0.028853
17         4.135745       0.068841         4.123099        0.028896
18         4.110657       0.068693         4.097077        0.028818
19         4.086883       0.068113         4.072942        0.028803
20         4.065521       0.067377         4.050718        0.029011
21         4.045989       0.066887         4.030475        0.028866
22         4.028025       0.066645         4.011695        0.028780
23         4.011367       0.066483         3.994610        0.028616
24         3.996052       0.066348         3.978625        0.028523
25         3.982530       0.065953         3.964252        0.028868
26         3.969826       0.065773         3.950707        0.029089
27         3.958208       0.065143         3.938616        0.029264
28         3.947641       0.065157         3.927500        0.029259
29         3.937759       0.064963         3.917360        0.028970
..              ...            ...              ...             ...
152        3.778286       0.056280         3.707143        0.029998
153        3.778019       0.056245         3.706699        0.030022
154        3.777749       0.056154         3.706079        0.029983
155        3.777677       0.056174         3.705605        0.029959
156        3.777405       0.056116         3.704960        0.030138
157        3.777131       0.056011         3.704351        0.030059
158        3.776926       0.055862         3.703850        0.030054
159        3.776869       0.055853         3.703308        0.030153
160        3.776689       0.055770         3.702825        0.030221
161        3.776638       0.055821         3.702315        0.030310
162        3.776630       0.055879         3.701691        0.030515
163        3.776498       0.055843         3.701193        0.030504
164        3.776135       0.055909         3.700559        0.030401
165        3.775790       0.055896         3.700155        0.030362
166        3.775468       0.056007         3.699611        0.030449
167        3.775438       0.055846         3.699055        0.030395
168        3.775252       0.055793         3.698632        0.030439
169        3.775159       0.055973         3.698039        0.030496
170        3.774974       0.055958         3.697525        0.030604
171        3.774741       0.056089         3.696944        0.030603
172        3.774548       0.056254         3.696419        0.030526
173        3.774399       0.056249         3.695922        0.030456
174        3.774295       0.056272         3.695461        0.030471
175        3.774111       0.056229         3.694919        0.030550
176        3.774031       0.056119         3.694536        0.030563
177        3.773681       0.056278         3.694115        0.030483
178        3.773651       0.056217         3.693788        0.030415
179        3.773458       0.056129         3.693342        0.030391
180        3.773296       0.056060         3.692867        0.030498
181        3.773185       0.056092         3.692540        0.030585

[182 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.1, 'subsample': 1, 'min_child_weight': 1, 'n_estimators': 1000, 'silent': True}.
The result is 
    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0         5.077227       0.067917         5.076418        0.032667
1         4.873637       0.068470         4.871546        0.032346
2         4.701224       0.068522         4.698492        0.031751
3         4.557298       0.069177         4.552467        0.031430
4         4.436018       0.069861         4.430420        0.031176
5         4.334982       0.069320         4.327725        0.030952
6         4.251337       0.068896         4.242603        0.030674
7         4.181221       0.068640         4.171648        0.030842
8         4.123169       0.068424         4.111716        0.030774
9         4.074997       0.068124         4.061906        0.031000
10        4.035676       0.066801         4.020995        0.031140
11        4.001627       0.065794         3.986078        0.031036
12        3.973747       0.065386         3.956366        0.030957
13        3.950945       0.064892         3.931928        0.030914
14        3.931203       0.064197         3.911082        0.031163
15        3.915173       0.063562         3.893639        0.031332
16        3.900769       0.063413         3.878642        0.031292
17        3.889387       0.062627         3.865861        0.031173
18        3.879242       0.062238         3.854847        0.031044
19        3.871048       0.062018         3.845553        0.031484
20        3.864571       0.061533         3.837847        0.031084
21        3.858784       0.060859         3.830542        0.031298
22        3.853318       0.060109         3.824346        0.031510
23        3.848793       0.059837         3.818681        0.031507
24        3.843926       0.059935         3.813240        0.031438
25        3.840446       0.059852         3.808857        0.031176
26        3.837094       0.058959         3.804308        0.031361
27        3.834130       0.059351         3.800058        0.031003
28        3.831318       0.059328         3.796028        0.030775
29        3.828409       0.059103         3.792762        0.031043
..             ...            ...              ...             ...
57        3.793495       0.054949         3.734985        0.031551
58        3.792967       0.055101         3.733466        0.031576
59        3.791928       0.055238         3.731946        0.031491
60        3.790816       0.054983         3.730577        0.031471
61        3.790037       0.054883         3.729320        0.031602
62        3.789938       0.054897         3.727979        0.031722
63        3.789132       0.054890         3.726530        0.031718
64        3.788556       0.054964         3.725404        0.031910
65        3.787641       0.054940         3.723897        0.031686
66        3.787367       0.054972         3.722767        0.031625
67        3.786964       0.054739         3.721375        0.031346
68        3.786732       0.054680         3.720156        0.031499
69        3.785840       0.054825         3.718732        0.031414
70        3.785407       0.055056         3.717422        0.031470
71        3.785180       0.054859         3.716465        0.031483
72        3.784480       0.054678         3.715134        0.031510
73        3.784001       0.054569         3.714211        0.031356
74        3.783170       0.054715         3.713089        0.031143
75        3.782947       0.054566         3.712169        0.031120
76        3.782342       0.054392         3.711150        0.031231
77        3.782161       0.054334         3.710440        0.031225
78        3.781923       0.054067         3.709346        0.030959
79        3.781526       0.054441         3.708097        0.031105
80        3.781137       0.054445         3.706941        0.031151
81        3.780828       0.054327         3.705952        0.031301
82        3.780440       0.054526         3.704956        0.031023
83        3.780108       0.054584         3.704204        0.030988
84        3.779615       0.054257         3.703314        0.030833
85        3.779417       0.054353         3.702501        0.030890
86        3.779358       0.054390         3.701622        0.030661

[87 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.1, 'subsample': 1, 'min_child_weight': 3, 'n_estimators': 1000, 'silent': True}.
The result is 
    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0         5.077227       0.067917         5.076418        0.032667
1         4.873681       0.068479         4.871568        0.032341
2         4.701366       0.068552         4.698504        0.031748
3         4.557472       0.069213         4.552522        0.031415
4         4.436129       0.069885         4.430464        0.031163
5         4.335155       0.069360         4.327686        0.030964
6         4.251503       0.068935         4.242640        0.030662
7         4.181511       0.068711         4.171612        0.030854
8         4.123466       0.068497         4.111680        0.030788
9         4.075200       0.068244         4.062013        0.031040
10        4.035885       0.066917         4.021106        0.031184
11        4.001949       0.065932         3.986136        0.031103
12        3.974068       0.065520         3.956423        0.031024
13        3.951042       0.064996         3.931821        0.031054
14        3.931589       0.064498         3.911551        0.030884
15        3.914955       0.063275         3.894049        0.031160
16        3.900791       0.063471         3.879033        0.031092
17        3.889363       0.062518         3.866260        0.030969
18        3.879429       0.062357         3.855483        0.030752
19        3.871063       0.062002         3.846048        0.031248
20        3.864010       0.061002         3.837851        0.031244
21        3.858124       0.060138         3.830957        0.031340
22        3.852870       0.059554         3.824835        0.031490
23        3.848481       0.059423         3.819369        0.031353
24        3.843698       0.059360         3.813715        0.031425
25        3.839728       0.058353         3.808834        0.031531
26        3.835580       0.058405         3.804313        0.031435
27        3.832116       0.058350         3.800291        0.030903
28        3.829413       0.057444         3.796551        0.031121
29        3.826655       0.057437         3.792977        0.030908
..             ...            ...              ...             ...
65        3.786707       0.055009         3.725695        0.030652
66        3.786387       0.055037         3.724729        0.030520
67        3.785629       0.054988         3.723296        0.030539
68        3.784707       0.054967         3.722064        0.030798
69        3.784084       0.055191         3.720808        0.030394
70        3.783521       0.055435         3.719591        0.030230
71        3.783445       0.055431         3.718631        0.030417
72        3.783129       0.055257         3.717394        0.030430
73        3.782633       0.055254         3.716394        0.030353
74        3.781903       0.055150         3.715375        0.030478
75        3.781561       0.055250         3.714304        0.030692
76        3.781151       0.055010         3.713476        0.030719
77        3.780646       0.054519         3.712488        0.030740
78        3.780060       0.054570         3.711135        0.030687
79        3.780055       0.054598         3.710240        0.030599
80        3.779744       0.054385         3.709010        0.030654
81        3.779140       0.054502         3.708255        0.030465
82        3.778994       0.054301         3.707129        0.030252
83        3.778673       0.054306         3.706283        0.030079
84        3.778551       0.054060         3.705474        0.030000
85        3.778248       0.053959         3.704275        0.030035
86        3.777896       0.053979         3.703392        0.030113
87        3.777665       0.054091         3.702308        0.030150
88        3.777656       0.054352         3.701486        0.029937
89        3.777420       0.054085         3.700525        0.030122
90        3.776904       0.053942         3.699490        0.030241
91        3.776771       0.053870         3.698794        0.030125
92        3.776413       0.053784         3.698021        0.030009
93        3.776131       0.053665         3.697359        0.030006
94        3.775759       0.053642         3.696610        0.029891

[95 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.1, 'subsample': 1, 'min_child_weight': 5, 'n_estimators': 1000, 'silent': True}.
The result is 
     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0          5.077227       0.067917         5.076418        0.032667
1          4.873681       0.068479         4.871568        0.032341
2          4.701366       0.068552         4.698504        0.031748
3          4.557349       0.069077         4.552650        0.031279
4          4.436022       0.069767         4.430599        0.031021
5          4.335034       0.069236         4.328022        0.030664
6          4.251721       0.069180         4.242829        0.030518
7          4.181696       0.068930         4.171963        0.030589
8          4.123431       0.068478         4.112149        0.030401
9          4.075225       0.067973         4.062469        0.030688
10         4.035958       0.066691         4.021583        0.030809
11         4.002269       0.065856         3.986732        0.030617
12         3.974500       0.065558         3.957003        0.030555
13         3.951734       0.065309         3.932406        0.030583
14         3.932140       0.064639         3.911917        0.030625
15         3.916025       0.064050         3.894623        0.030753
16         3.901790       0.064167         3.879821        0.030493
17         3.890418       0.063273         3.867140        0.030289
18         3.880603       0.063236         3.856130        0.030289
19         3.871860       0.062490         3.846619        0.030863
20         3.865224       0.061916         3.838658        0.030645
21         3.859456       0.061167         3.831572        0.030912
22         3.853993       0.060371         3.825462        0.031047
23         3.849315       0.060021         3.819873        0.031072
24         3.844702       0.060138         3.814166        0.031194
25         3.839925       0.059953         3.809035        0.031352
26         3.836546       0.060035         3.804623        0.030867
27         3.833594       0.059039         3.800794        0.031160
28         3.830419       0.058684         3.796983        0.031369
29         3.827869       0.058034         3.793654        0.031297
..              ...            ...              ...             ...
74         3.783613       0.056701         3.716607        0.030028
75         3.783307       0.056541         3.715465        0.030218
76         3.782761       0.056522         3.714426        0.030059
77         3.782370       0.056150         3.713292        0.029995
78         3.782177       0.056023         3.712280        0.030085
79         3.781624       0.055995         3.711401        0.030267
80         3.781390       0.056057         3.710467        0.030355
81         3.780855       0.055852         3.709645        0.030520
82         3.780667       0.055895         3.708881        0.030409
83         3.780365       0.055877         3.708055        0.030383
84         3.779975       0.055836         3.707211        0.030328
85         3.779712       0.056176         3.706312        0.030188
86         3.779328       0.055953         3.705119        0.030159
87         3.779010       0.056127         3.703999        0.030009
88         3.778515       0.055888         3.702962        0.030116
89         3.778416       0.055788         3.702046        0.030241
90         3.777713       0.055487         3.700969        0.030533
91         3.777375       0.055576         3.700238        0.030538
92         3.777045       0.055637         3.699535        0.030509
93         3.776144       0.055917         3.698317        0.030495
94         3.776006       0.055907         3.697635        0.030560
95         3.775872       0.055937         3.696678        0.030634
96         3.775512       0.055697         3.695861        0.030677
97         3.775106       0.055566         3.694858        0.030912
98         3.775078       0.055436         3.694359        0.030855
99         3.774845       0.055398         3.693636        0.030647
100        3.774541       0.055185         3.692721        0.030432
101        3.774358       0.055024         3.691865        0.030577
102        3.773796       0.054860         3.691162        0.030530
103        3.773710       0.054951         3.690344        0.030624

[104 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.1, 'subsample': 0.7, 'min_child_weight': 1, 'n_estimators': 1000, 'silent': True}.
The result is 
     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0          5.077700       0.067988         5.076519        0.032801
1          4.873132       0.068621         4.871165        0.032266
2          4.699639       0.070474         4.696871        0.031095
3          4.555858       0.070589         4.551283        0.030454
4          4.435424       0.071119         4.429789        0.029764
5          4.334265       0.070196         4.327834        0.029967
6          4.250970       0.069208         4.242969        0.030439
7          4.181171       0.068940         4.171090        0.029694
8          4.122537       0.068205         4.111184        0.029957
9          4.074448       0.067159         4.061399        0.029992
10         4.034685       0.065394         4.020075        0.030104
11         4.000894       0.064887         3.985231        0.029993
12         3.973248       0.064761         3.956390        0.029486
13         3.950209       0.064690         3.931945        0.029276
14         3.930173       0.064034         3.910685        0.028875
15         3.913044       0.062860         3.892846        0.028896
16         3.900088       0.062348         3.877912        0.029068
17         3.888291       0.061974         3.865201        0.029551
18         3.878699       0.060794         3.853970        0.029715
19         3.870684       0.060242         3.844730        0.029830
20         3.863747       0.059456         3.836500        0.030008
21         3.856660       0.059093         3.828773        0.030260
22         3.851636       0.059633         3.822235        0.030542
23         3.846592       0.059448         3.816471        0.030256
24         3.841787       0.059554         3.810464        0.030358
25         3.837736       0.058815         3.805777        0.030460
26         3.834048       0.059279         3.800930        0.029904
27         3.830816       0.058795         3.796850        0.030075
28         3.828667       0.058796         3.793056        0.030141
29         3.826037       0.057960         3.789439        0.030242
..              ...            ...              ...             ...
76         3.779779       0.054368         3.706125        0.029777
77         3.779719       0.054093         3.704955        0.029794
78         3.779611       0.054182         3.703913        0.029883
79         3.779517       0.054166         3.702974        0.029853
80         3.779106       0.054267         3.702141        0.029814
81         3.778867       0.054637         3.701135        0.029563
82         3.778802       0.054438         3.700015        0.029171
83         3.778699       0.054424         3.698933        0.029056
84         3.778616       0.054502         3.698141        0.028927
85         3.778458       0.054492         3.697179        0.028670
86         3.778154       0.054647         3.696096        0.028682
87         3.778139       0.054912         3.695172        0.029039
88         3.777930       0.054516         3.694253        0.029044
89         3.777741       0.054429         3.693369        0.029047
90         3.777608       0.054298         3.692288        0.029081
91         3.777228       0.054270         3.691241        0.029163
92         3.777069       0.054262         3.690200        0.029124
93         3.776605       0.054169         3.689139        0.029243
94         3.776519       0.054247         3.688366        0.029408
95         3.776477       0.054254         3.687541        0.029243
96         3.776448       0.054121         3.686811        0.029114
97         3.776181       0.053943         3.685795        0.029179
98         3.775872       0.054139         3.684885        0.028922
99         3.775678       0.054051         3.683751        0.029077
100        3.775447       0.053784         3.682956        0.029248
101        3.775148       0.053503         3.682044        0.029331
102        3.774808       0.053382         3.681241        0.029366
103        3.774467       0.053477         3.680548        0.029152
104        3.774375       0.053698         3.679731        0.029246
105        3.774260       0.053543         3.679078        0.029390

[106 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.1, 'subsample': 0.7, 'min_child_weight': 3, 'n_estimators': 1000, 'silent': True}.
The result is 
    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0         5.077764       0.068000         5.076479        0.032807
1         4.873199       0.068631         4.871128        0.032276
2         4.699708       0.070485         4.696853        0.031100
3         4.556022       0.070616         4.551276        0.030457
4         4.436307       0.071833         4.429920        0.029666
5         4.335187       0.070899         4.327983        0.029875
6         4.251489       0.069700         4.243064        0.030284
7         4.181716       0.069439         4.171090        0.029603
8         4.122902       0.068660         4.111180        0.029868
9         4.074843       0.067619         4.061408        0.029906
10        4.035029       0.065692         4.020184        0.029992
11        4.001407       0.065452         3.984993        0.030110
12        3.974020       0.065390         3.955641        0.030003
13        3.950404       0.064605         3.930835        0.030079
14        3.930171       0.064038         3.909578        0.029678
15        3.913418       0.063128         3.891988        0.029478
16        3.900190       0.062604         3.877098        0.029594
17        3.888235       0.062186         3.864124        0.030246
18        3.879094       0.061548         3.853573        0.029788
19        3.870779       0.060686         3.844226        0.030035
20        3.864011       0.059953         3.835922        0.030245
21        3.856579       0.059349         3.827951        0.030697
22        3.850698       0.059318         3.821571        0.030880
23        3.845579       0.059035         3.815605        0.030801
24        3.840790       0.059158         3.809496        0.030989
25        3.836909       0.058488         3.804690        0.031220
26        3.832944       0.058758         3.799716        0.030730
27        3.829701       0.058209         3.795824        0.030790
28        3.827247       0.058028         3.792257        0.030734
29        3.824595       0.057244         3.788713        0.030642
..             ...            ...              ...             ...
49        3.796727       0.053865         3.742516        0.031104
50        3.795766       0.053448         3.740520        0.031270
51        3.794531       0.053901         3.738785        0.030701
52        3.793606       0.053738         3.737153        0.031006
53        3.792723       0.054283         3.735362        0.030742
54        3.792287       0.054293         3.734118        0.030623
55        3.791420       0.054355         3.732449        0.030643
56        3.790168       0.054424         3.730762        0.030615
57        3.789132       0.054600         3.729489        0.030507
58        3.788879       0.054193         3.728265        0.030342
59        3.788418       0.054281         3.726800        0.030361
60        3.787927       0.054275         3.725470        0.030092
61        3.787436       0.054252         3.724262        0.030015
62        3.787055       0.054290         3.722943        0.030074
63        3.786147       0.053912         3.721766        0.030314
64        3.785783       0.053835         3.720538        0.030292
65        3.785112       0.054418         3.719153        0.030394
66        3.784679       0.054544         3.717924        0.030547
67        3.784048       0.054600         3.716487        0.030331
68        3.783565       0.054373         3.715356        0.030206
69        3.782813       0.054180         3.714139        0.030166
70        3.782265       0.053889         3.712937        0.030478
71        3.781830       0.053760         3.711517        0.030904
72        3.781182       0.053857         3.710489        0.030955
73        3.781169       0.054215         3.709077        0.030812
74        3.780860       0.054398         3.707912        0.030624
75        3.779998       0.055229         3.706837        0.030054
76        3.779685       0.055240         3.705707        0.030171
77        3.779488       0.055267         3.704557        0.030255
78        3.779305       0.055124         3.703735        0.030285

[79 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.1, 'subsample': 0.7, 'min_child_weight': 5, 'n_estimators': 1000, 'silent': True}.
The result is 
    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0         5.077764       0.068000         5.076479        0.032807
1         4.872910       0.068347         4.871141        0.032252
2         4.699450       0.070234         4.696879        0.031050
3         4.555772       0.070369         4.551299        0.030405
4         4.435858       0.071551         4.430260        0.029502
5         4.334710       0.070609         4.328387        0.029677
6         4.250960       0.069327         4.243452        0.030116
7         4.181568       0.069139         4.171638        0.029358
8         4.122898       0.068447         4.111748        0.029591
9         4.074957       0.067516         4.062022        0.029606
10        4.035142       0.065569         4.020750        0.029764
11        4.001744       0.065571         3.985675        0.029761
12        3.974421       0.065608         3.956333        0.029693
13        3.951192       0.065069         3.931866        0.029534
14        3.931053       0.064481         3.910581        0.029158
15        3.914634       0.063974         3.892810        0.029099
16        3.901360       0.063307         3.877989        0.029186
17        3.889611       0.062983         3.865181        0.029767
18        3.879801       0.061968         3.854453        0.029859
19        3.871496       0.061129         3.844984        0.030131
20        3.864804       0.060426         3.836683        0.030371
21        3.857689       0.060127         3.829030        0.030541
22        3.851899       0.060206         3.822540        0.030791
23        3.846927       0.060070         3.816612        0.030672
24        3.842591       0.060503         3.811032        0.030518
25        3.838621       0.059919         3.806087        0.030582
26        3.835307       0.059357         3.801405        0.030496
27        3.831495       0.058661         3.797140        0.030746
28        3.828772       0.059055         3.793387        0.030440
29        3.826136       0.058406         3.789761        0.030476
..             ...            ...              ...             ...
43        3.804546       0.056479         3.755639        0.030453
44        3.803504       0.055837         3.753770        0.030274
45        3.802657       0.056327         3.752251        0.030445
46        3.801702       0.056238         3.750427        0.030487
47        3.800609       0.055960         3.748392        0.030943
48        3.799196       0.055973         3.746679        0.030915
49        3.798162       0.056202         3.744938        0.030605
50        3.796876       0.055779         3.742893        0.030825
51        3.796014       0.056083         3.741305        0.030345
52        3.795568       0.055943         3.739864        0.030326
53        3.794909       0.056289         3.738009        0.030308
54        3.793976       0.055983         3.736460        0.030406
55        3.793663       0.055673         3.735117        0.030430
56        3.792139       0.055476         3.733491        0.030385
57        3.791283       0.055239         3.731857        0.030426
58        3.790777       0.055221         3.730344        0.030117
59        3.789998       0.055303         3.728919        0.029927
60        3.789077       0.055500         3.727338        0.029780
61        3.788489       0.055271         3.725998        0.029825
62        3.787969       0.055147         3.724661        0.029943
63        3.787205       0.055110         3.723448        0.030068
64        3.786778       0.054980         3.722344        0.029961
65        3.786262       0.055404         3.721054        0.030238
66        3.785810       0.055603         3.719919        0.030370
67        3.785603       0.055641         3.718813        0.030239
68        3.785017       0.055364         3.717606        0.030171
69        3.784732       0.055138         3.716316        0.030427
70        3.784074       0.054833         3.715117        0.030622
71        3.783658       0.054659         3.714123        0.030651
72        3.783245       0.054535         3.713147        0.030763

[73 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.2, 'subsample': 1, 'min_child_weight': 1, 'n_estimators': 1000, 'silent': True}.
The result is 
    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0         4.852074       0.068420         4.850050        0.031952
1         4.527697       0.070571         4.522153        0.031109
2         4.305736       0.070058         4.297631        0.030418
3         4.154205       0.069432         4.143902        0.030702
4         4.052305       0.066640         4.038199        0.030684
5         3.983368       0.065374         3.967001        0.030328
6         3.936038       0.062818         3.917788        0.030729
7         3.903780       0.061199         3.882942        0.031364
8         3.878920       0.060983         3.856938        0.030683
9         3.863639       0.061097         3.838492        0.031193
10        3.852735       0.059746         3.824514        0.031084
11        3.843629       0.059585         3.813696        0.031015
12        3.835295       0.058603         3.804512        0.031172
13        3.828729       0.057290         3.797283        0.032023
14        3.823430       0.058115         3.790420        0.031041
15        3.819900       0.057320         3.785236        0.030696
16        3.816069       0.056360         3.779837        0.030880
17        3.812416       0.055302         3.774683        0.031042
18        3.809042       0.055520         3.769536        0.031416
19        3.806424       0.055324         3.766101        0.031692
20        3.804171       0.054821         3.761486        0.031818
21        3.801868       0.055191         3.757618        0.031223
22        3.800082       0.055393         3.753884        0.031088
23        3.798402       0.055049         3.750266        0.030576
24        3.796331       0.054970         3.746575        0.030823
25        3.795296       0.055412         3.743339        0.031257
26        3.793489       0.055690         3.740367        0.030518
27        3.792392       0.055164         3.737622        0.030647
28        3.791140       0.054547         3.734789        0.030728
29        3.789012       0.053752         3.732084        0.030879
30        3.788646       0.053909         3.729474        0.031284
31        3.786397       0.054298         3.726552        0.030543
32        3.784779       0.054278         3.723501        0.030819
33        3.783782       0.053939         3.721704        0.030884
34        3.783205       0.054108         3.719579        0.030524
35        3.782660       0.054468         3.716856        0.031034
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.2, 'subsample': 1, 'min_child_weight': 3, 'n_estimators': 1000, 'silent': True}.
The result is 
    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0         4.852074       0.068420         4.850050        0.031952
1         4.527817       0.070596         4.522191        0.031099
2         4.305896       0.070094         4.297667        0.030407
3         4.154495       0.069504         4.143769        0.030744
4         4.052570       0.066721         4.038119        0.030709
5         3.983940       0.065562         3.967059        0.030307
6         3.936618       0.063025         3.917840        0.030711
7         3.904410       0.061432         3.882939        0.031365
8         3.879613       0.061235         3.856920        0.030690
9         3.864160       0.061247         3.838433        0.031226
10        3.853239       0.059898         3.824427        0.031131
11        3.844128       0.059741         3.813592        0.031073
12        3.836089       0.058887         3.804319        0.031268
13        3.829990       0.057785         3.797570        0.031926
14        3.824646       0.058602         3.790644        0.030968
15        3.820997       0.057784         3.785010        0.030819
16        3.817224       0.056843         3.779380        0.031103
17        3.813454       0.055723         3.774250        0.031265
18        3.810286       0.055799         3.769279        0.031686
19        3.808077       0.055765         3.765827        0.031643
20        3.805594       0.055121         3.761306        0.032100
21        3.803382       0.055743         3.757626        0.031259
22        3.802040       0.055856         3.754039        0.031012
23        3.799467       0.055513         3.750731        0.030465
24        3.797290       0.055816         3.746767        0.029846
25        3.795275       0.055252         3.742890        0.030353
26        3.793695       0.054952         3.739891        0.030318
27        3.791697       0.054710         3.736997        0.030496
28        3.790721       0.055135         3.734630        0.030409
29        3.789661       0.055334         3.732385        0.030640
30        3.789099       0.054925         3.730254        0.030539
31        3.786989       0.054335         3.728028        0.030847
32        3.786126       0.054091         3.725502        0.030829
33        3.784554       0.053811         3.722694        0.030748
34        3.784220       0.053901         3.720247        0.030802
35        3.783351       0.053314         3.717295        0.030561
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.2, 'subsample': 1, 'min_child_weight': 5, 'n_estimators': 1000, 'silent': True}.
The result is 
    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0         4.852074       0.068420         4.850050        0.031952
1         4.527591       0.070346         4.522446        0.030825
2         4.305724       0.069906         4.297933        0.030128
3         4.154382       0.069401         4.144384        0.030210
4         4.052473       0.066637         4.038685        0.030215
5         3.983437       0.065128         3.967748        0.029820
6         3.936024       0.062670         3.918548        0.030280
7         3.904463       0.061716         3.883386        0.031191
8         3.879697       0.061597         3.856989        0.030886
9         3.863649       0.060955         3.838950        0.030987
10        3.852871       0.059751         3.825122        0.030716
11        3.843078       0.059079         3.813636        0.031343
12        3.835909       0.059162         3.804039        0.031729
13        3.829852       0.058498         3.796922        0.032552
14        3.824217       0.059218         3.789705        0.031925
15        3.820625       0.058744         3.784250        0.031632
16        3.817750       0.058527         3.778992        0.031611
17        3.814350       0.057584         3.774054        0.031665
18        3.811862       0.057726         3.769363        0.031974
19        3.809837       0.058037         3.766213        0.031736
20        3.807578       0.057700         3.762593        0.031445
21        3.805401       0.057512         3.759038        0.031004
22        3.802952       0.057953         3.754897        0.030569
23        3.800990       0.057795         3.750938        0.030113
24        3.799508       0.057242         3.747761        0.030186
25        3.797842       0.057365         3.744858        0.030159
26        3.795043       0.056624         3.741348        0.029963
27        3.793853       0.056534         3.738795        0.030193
28        3.793370       0.056141         3.736121        0.030186
29        3.791103       0.055869         3.732791        0.030424
30        3.790778       0.056104         3.730246        0.030845
31        3.790053       0.055740         3.728050        0.031039
32        3.788148       0.056372         3.725347        0.030541
33        3.787434       0.055769         3.723364        0.030735
34        3.786767       0.055840         3.720951        0.030832
35        3.785894       0.056319         3.718774        0.030370
36        3.785252       0.056462         3.716883        0.030444
37        3.784602       0.056702         3.714795        0.031028
38        3.784311       0.056801         3.712340        0.030726
39        3.783616       0.056470         3.710269        0.031003
40        3.782586       0.056456         3.708625        0.031075
41        3.781971       0.056099         3.706601        0.031521
42        3.781617       0.055890         3.704994        0.031428
43        3.780934       0.056238         3.703582        0.031131
44        3.780170       0.055676         3.701515        0.031065
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.2, 'subsample': 0.7, 'min_child_weight': 1, 'n_estimators': 1000, 'silent': True}.
The result is 
    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0         4.853214       0.068631         4.850294        0.032216
1         4.527100       0.069231         4.522289        0.031852
2         4.302676       0.071149         4.295482        0.030106
3         4.153854       0.068778         4.141863        0.030270
4         4.053484       0.069114         4.038201        0.027901
5         3.984964       0.067042         3.966855        0.028808
6         3.937491       0.065039         3.918085        0.030403
7         3.903969       0.062799         3.882962        0.030184
8         3.881488       0.061841         3.857244        0.031255
9         3.866175       0.062546         3.839337        0.030263
10        3.853647       0.060358         3.824081        0.031178
11        3.844159       0.060571         3.812516        0.031104
12        3.835608       0.060110         3.802197        0.031113
13        3.829958       0.060051         3.794625        0.030687
14        3.826028       0.058762         3.787546        0.031334
15        3.821773       0.059162         3.781365        0.030899
16        3.816694       0.057927         3.775236        0.030993
17        3.814209       0.058236         3.770549        0.031393
18        3.812591       0.058418         3.766473        0.031031
19        3.809961       0.058706         3.762359        0.030766
20        3.807234       0.059336         3.757937        0.030709
21        3.805921       0.059231         3.754233        0.031177
22        3.805473       0.060428         3.750617        0.030885
23        3.802579       0.060445         3.746779        0.030736
24        3.801526       0.060436         3.743404        0.031347
25        3.799272       0.059366         3.740163        0.031850
26        3.798892       0.058890         3.736574        0.032642
27        3.797069       0.059289         3.734249        0.032445
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.2, 'subsample': 0.7, 'min_child_weight': 3, 'n_estimators': 1000, 'silent': True}.
The result is 
    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0         4.853329       0.068655         4.850231        0.032226
1         4.527524       0.068861         4.522241        0.031876
2         4.303072       0.070703         4.295528        0.030151
3         4.154278       0.068279         4.141834        0.030389
4         4.053935       0.068584         4.038136        0.028066
5         3.985467       0.066488         3.966856        0.028947
6         3.938041       0.064374         3.918158        0.030606
7         3.904440       0.062135         3.883042        0.030307
8         3.881213       0.061445         3.857408        0.031528
9         3.865864       0.062139         3.839476        0.030519
10        3.853470       0.059439         3.824648        0.030948
11        3.844653       0.058832         3.813469        0.030829
12        3.836078       0.058347         3.803196        0.030658
13        3.830504       0.058283         3.795695        0.030269
14        3.826199       0.056228         3.788809        0.031289
15        3.821965       0.055793         3.782636        0.031563
16        3.815585       0.056335         3.775515        0.030606
17        3.813206       0.056677         3.770841        0.031102
18        3.811985       0.056377         3.767104        0.031237
19        3.809678       0.055924         3.763593        0.031200
20        3.808085       0.057116         3.759475        0.030587
21        3.806380       0.056962         3.755404        0.030962
22        3.804594       0.057698         3.751787        0.031083
23        3.802272       0.057754         3.748514        0.031175
24        3.801302       0.057832         3.744833        0.031393
25        3.798159       0.057369         3.740806        0.031364
26        3.796424       0.057287         3.737938        0.031740
27        3.794724       0.056855         3.735757        0.031867
28        3.793210       0.056611         3.732485        0.031844
29        3.791856       0.056121         3.729492        0.031092
30        3.790324       0.056027         3.727029        0.031321
31        3.789112       0.056031         3.724782        0.030976
32        3.788420       0.055130         3.721765        0.031187
33        3.787230       0.055005         3.720141        0.031146
34        3.785573       0.054222         3.717575        0.031276
35        3.784504       0.053566         3.715457        0.031246
36        3.783653       0.053030         3.713268        0.030775
37        3.783051       0.052845         3.711373        0.031084
38        3.782061       0.052999         3.709496        0.031485
39        3.781019       0.052235         3.707656        0.031831
40        3.780535       0.052811         3.705546        0.031865
41        3.780172       0.052729         3.703317        0.031727
42        3.779797       0.052762         3.701952        0.031833
43        3.778944       0.052460         3.700043        0.031501
44        3.777945       0.051904         3.698029        0.031458
45        3.777299       0.051556         3.696660        0.031605
46        3.777139       0.051208         3.694977        0.031873
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'reg:linear', 'learning_rate': 0.2, 'subsample': 0.7, 'min_child_weight': 5, 'n_estimators': 1000, 'silent': True}.
The result is 
    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0         4.853329       0.068655         4.850231        0.032226
1         4.527273       0.068573         4.522423        0.031693
2         4.302854       0.070456         4.295683        0.030004
3         4.154115       0.068090         4.142164        0.030172
4         4.053488       0.068148         4.038500        0.027823
5         3.985136       0.066122         3.967214        0.028717
6         3.937770       0.064061         3.918535        0.030376
7         3.904112       0.061811         3.883556        0.030013
8         3.881340       0.061604         3.857870        0.031270
9         3.865972       0.062260         3.839929        0.030279
10        3.853159       0.059669         3.825001        0.030994
11        3.844406       0.059091         3.813895        0.030832
12        3.836108       0.058792         3.803327        0.030793
13        3.830690       0.058442         3.795930        0.030316
14        3.826462       0.057047         3.789098        0.031325
15        3.820542       0.057680         3.781738        0.030958
16        3.817030       0.057831         3.776094        0.030610
17        3.814473       0.058175         3.771898        0.030666
18        3.811145       0.058829         3.767030        0.030007
19        3.808051       0.058383         3.762708        0.030359
20        3.806628       0.060056         3.758652        0.029842
21        3.804876       0.059652         3.754990        0.030272
22        3.803361       0.060307         3.751230        0.029827
23        3.801867       0.060638         3.748270        0.029852
24        3.801468       0.060806         3.745306        0.030306
25        3.798635       0.061338         3.741494        0.029952
26        3.796748       0.060620         3.738418        0.029735
27        3.795330       0.060100         3.735847        0.030137
28        3.793810       0.059494         3.733103        0.030813
29        3.792717       0.058899         3.730392        0.029926
30        3.791490       0.059060         3.728118        0.029821
31        3.789735       0.059513         3.726029        0.029186
32        3.789259       0.059567         3.723458        0.029128
33        3.787953       0.059433         3.721120        0.028716
34        3.787393       0.058746         3.718828        0.028949
35        3.786688       0.058323         3.716744        0.029284
36        3.784788       0.057134         3.713881        0.029218
37        3.784221       0.057098         3.711919        0.029045
38        3.783460       0.056532         3.710009        0.028948
39        3.782838       0.056538         3.708422        0.028422
40        3.781998       0.055739         3.706475        0.028723
41        3.781824       0.055687         3.704358        0.029102
42        3.781700       0.055555         3.702902        0.029298
43        3.780897       0.055393         3.700778        0.029461
44        3.780648       0.055336         3.698833        0.029324
45        3.780438       0.054625         3.697004        0.029324
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'count:poisson', 'learning_rate': 0.05, 'subsample': 1, 'min_child_weight': 1, 'n_estimators': 1000, 'silent': True}.
The result is 
     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0          5.305931       0.067146         5.306250        0.033492
1          5.293807       0.067143         5.294127        0.033491
2          5.281290       0.067139         5.281610        0.033488
3          5.268368       0.067135         5.268689        0.033486
4          5.255032       0.067130         5.255354        0.033483
5          5.241268       0.067125         5.241591        0.033479
6          5.227068       0.067118         5.227391        0.033476
7          5.212418       0.067110         5.212742        0.033472
8          5.197308       0.067101         5.197633        0.033467
9          5.181726       0.067091         5.182052        0.033461
10         5.165661       0.067080         5.165988        0.033455
11         5.149102       0.067067         5.149430        0.033449
12         5.132037       0.067052         5.132366        0.033441
13         5.114456       0.067036         5.114786        0.033431
14         5.096348       0.067018         5.096679        0.033422
15         5.077703       0.066998         5.078034        0.033411
16         5.058510       0.066976         5.058842        0.033399
17         5.038761       0.066951         5.039092        0.033386
18         5.018444       0.066924         5.018776        0.033371
19         4.997553       0.066893         4.997885        0.033354
20         4.976096       0.066862         4.976426        0.033333
21         4.954084       0.066824         4.954407        0.033314
22         4.931486       0.066783         4.931801        0.033293
23         4.908304       0.066741         4.908609        0.033262
24         4.884530       0.066696         4.884825        0.033227
25         4.860164       0.066649         4.860448        0.033187
26         4.835220       0.066594         4.835483        0.033142
27         4.809698       0.066537         4.809941        0.033091
28         4.783704       0.066512         4.783926        0.032992
29         4.757249       0.066414         4.757450        0.032943
..              ...            ...              ...             ...
324        3.777392       0.056003         3.702411        0.030122
325        3.777227       0.055975         3.702179        0.030150
326        3.777104       0.056029         3.701875        0.030112
327        3.776987       0.055944         3.701556        0.030201
328        3.776938       0.055978         3.701298        0.030221
329        3.776767       0.056081         3.701029        0.030208
330        3.776681       0.056066         3.700807        0.030185
331        3.776602       0.056079         3.700545        0.030219
332        3.776486       0.056128         3.700249        0.030124
333        3.776430       0.056084         3.700048        0.030072
334        3.776328       0.056022         3.699762        0.030118
335        3.776135       0.055984         3.699468        0.030166
336        3.775972       0.055995         3.699027        0.030255
337        3.775890       0.055914         3.698736        0.030181
338        3.775771       0.055953         3.698440        0.030107
339        3.775725       0.055912         3.698171        0.030127
340        3.775660       0.055899         3.697939        0.030138
341        3.775562       0.055905         3.697697        0.030168
342        3.775545       0.055911         3.697462        0.030128
343        3.775503       0.055828         3.697242        0.030116
344        3.775442       0.055857         3.696988        0.030150
345        3.775358       0.055943         3.696708        0.030183
346        3.775206       0.055928         3.696363        0.030175
347        3.775065       0.055966         3.696109        0.030144
348        3.774924       0.055991         3.695693        0.030162
349        3.774833       0.055987         3.695477        0.030172
350        3.774731       0.055948         3.695220        0.030211
351        3.774646       0.055994         3.694939        0.030197
352        3.774521       0.055996         3.694669        0.030188
353        3.774432       0.055961         3.694375        0.030158

[354 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'count:poisson', 'learning_rate': 0.05, 'subsample': 1, 'min_child_weight': 3, 'n_estimators': 1000, 'silent': True}.
The result is 
     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0          5.305931       0.067146         5.306250        0.033492
1          5.293807       0.067143         5.294127        0.033491
2          5.281290       0.067139         5.281610        0.033488
3          5.268368       0.067135         5.268689        0.033486
4          5.255032       0.067130         5.255354        0.033483
5          5.241268       0.067125         5.241591        0.033479
6          5.227068       0.067118         5.227391        0.033476
7          5.212418       0.067110         5.212742        0.033472
8          5.197308       0.067101         5.197633        0.033467
9          5.181726       0.067091         5.182052        0.033461
10         5.165661       0.067080         5.165988        0.033455
11         5.149102       0.067067         5.149430        0.033449
12         5.132037       0.067052         5.132366        0.033441
13         5.114456       0.067036         5.114786        0.033431
14         5.096348       0.067018         5.096679        0.033422
15         5.077703       0.066997         5.078034        0.033411
16         5.058509       0.066975         5.058842        0.033399
17         5.038760       0.066950         5.039092        0.033385
18         5.018442       0.066922         5.018776        0.033370
19         4.997551       0.066891         4.997885        0.033354
20         4.976093       0.066859         4.976426        0.033333
21         4.954081       0.066821         4.954407        0.033314
22         4.931482       0.066779         4.931802        0.033292
23         4.908299       0.066735         4.908609        0.033262
24         4.884526       0.066692         4.884828        0.033225
25         4.860160       0.066645         4.860450        0.033184
26         4.835216       0.066589         4.835485        0.033139
27         4.809693       0.066532         4.809943        0.033088
28         4.783700       0.066507         4.783929        0.032989
29         4.757244       0.066409         4.757453        0.032940
..              ...            ...              ...             ...
366        3.773578       0.056170         3.691004        0.030192
367        3.773513       0.056201         3.690723        0.030155
368        3.773352       0.056069         3.690413        0.030207
369        3.773297       0.056080         3.690184        0.030249
370        3.773236       0.056036         3.689939        0.030237
371        3.773163       0.055906         3.689605        0.030222
372        3.773140       0.055890         3.689429        0.030234
373        3.773112       0.055884         3.689231        0.030251
374        3.773110       0.055867         3.688980        0.030317
375        3.773037       0.055848         3.688784        0.030327
376        3.772931       0.055847         3.688537        0.030279
377        3.772889       0.055801         3.688234        0.030332
378        3.772859       0.055761         3.688007        0.030331
379        3.772751       0.055770         3.687808        0.030340
380        3.772712       0.055824         3.687596        0.030355
381        3.772630       0.055791         3.687348        0.030401
382        3.772533       0.055740         3.687055        0.030393
383        3.772470       0.055764         3.686796        0.030429
384        3.772453       0.055768         3.686599        0.030440
385        3.772426       0.055696         3.686352        0.030414
386        3.772342       0.055655         3.686136        0.030424
387        3.772207       0.055672         3.685844        0.030472
388        3.772189       0.055681         3.685651        0.030409
389        3.772147       0.055687         3.685452        0.030416
390        3.772088       0.055692         3.685254        0.030443
391        3.771935       0.055724         3.684953        0.030446
392        3.771896       0.055690         3.684716        0.030464
393        3.771856       0.055648         3.684478        0.030489
394        3.771824       0.055635         3.684265        0.030465
395        3.771754       0.055638         3.684079        0.030484

[396 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'count:poisson', 'learning_rate': 0.05, 'subsample': 1, 'min_child_weight': 5, 'n_estimators': 1000, 'silent': True}.
The result is 
     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0          5.305931       0.067146         5.306250        0.033492
1          5.293807       0.067143         5.294127        0.033491
2          5.281290       0.067139         5.281610        0.033488
3          5.268368       0.067135         5.268689        0.033486
4          5.255032       0.067130         5.255354        0.033483
5          5.241268       0.067125         5.241591        0.033479
6          5.227068       0.067118         5.227391        0.033476
7          5.212418       0.067110         5.212742        0.033472
8          5.197308       0.067101         5.197633        0.033467
9          5.181726       0.067091         5.182052        0.033461
10         5.165661       0.067080         5.165988        0.033455
11         5.149102       0.067067         5.149430        0.033449
12         5.132037       0.067052         5.132366        0.033441
13         5.114456       0.067036         5.114786        0.033431
14         5.096348       0.067018         5.096679        0.033422
15         5.077703       0.066997         5.078034        0.033411
16         5.058509       0.066975         5.058842        0.033399
17         5.038760       0.066950         5.039092        0.033385
18         5.018442       0.066922         5.018776        0.033370
19         4.997551       0.066891         4.997885        0.033354
20         4.976093       0.066859         4.976426        0.033333
21         4.954084       0.066820         4.954409        0.033315
22         4.931486       0.066777         4.931805        0.033293
23         4.908303       0.066734         4.908612        0.033263
24         4.884530       0.066692         4.884831        0.033226
25         4.860165       0.066644         4.860453        0.033185
26         4.835220       0.066588         4.835488        0.033140
27         4.809698       0.066531         4.809946        0.033089
28         4.783705       0.066506         4.783931        0.032990
29         4.757249       0.066408         4.757456        0.032940
..              ...            ...              ...             ...
335        3.776617       0.056062         3.699387        0.030224
336        3.776427       0.056100         3.699137        0.030272
337        3.776320       0.056136         3.698813        0.030302
338        3.776209       0.056159         3.698608        0.030301
339        3.776143       0.056129         3.698346        0.030380
340        3.776017       0.055997         3.698042        0.030416
341        3.775914       0.055986         3.697754        0.030409
342        3.775791       0.055952         3.697513        0.030427
343        3.775636       0.055874         3.697207        0.030487
344        3.775632       0.055854         3.696968        0.030481
345        3.775598       0.055840         3.696770        0.030467
346        3.775460       0.055873         3.696454        0.030420
347        3.775440       0.055953         3.696208        0.030441
348        3.775316       0.055956         3.695923        0.030464
349        3.775187       0.055957         3.695716        0.030480
350        3.775066       0.055949         3.695456        0.030515
351        3.775007       0.055887         3.695255        0.030480
352        3.774894       0.055867         3.694995        0.030531
353        3.774843       0.055792         3.694702        0.030499
354        3.774744       0.055774         3.694476        0.030489
355        3.774728       0.055770         3.694282        0.030475
356        3.774628       0.055786         3.693950        0.030563
357        3.774550       0.055794         3.693655        0.030527
358        3.774416       0.055740         3.693418        0.030562
359        3.774323       0.055690         3.693178        0.030594
360        3.774240       0.055629         3.692910        0.030651
361        3.774085       0.055752         3.692653        0.030648
362        3.773941       0.055826         3.692429        0.030604
363        3.773940       0.055770         3.692178        0.030628
364        3.773839       0.055743         3.691994        0.030602

[365 rows x 4 columns]
Now we are training and validating hyperperameters: {'max_depth': 3, 'objective': 'count:poisson', 'learning_rate': 0.05, 'subsample': 0.7, 'min_child_weight': 1, 'n_estimators': 1000, 'silent': True}.
The result is 
     test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std
0          5.305931       0.067146         5.306250        0.033492
1          5.293807       0.067143         5.294127        0.033491
2          5.281290       0.067139         5.281610        0.033488
3          5.268368       0.067135         5.268689        0.033486
4          5.255032       0.067130         5.255354        0.033483
5          5.241268       0.067125         5.241591        0.033479
6          5.227068       0.067118         5.227391        0.033476
7          5.212418       0.067110         5.212743        0.033471
8          5.197309       0.067102         5.197633        0.033466
9          5.181727       0.067093         5.182052        0.033461
10         5.165662       0.067081         5.165989        0.033455
11         5.149103       0.067068         5.149431        0.033448
12         5.132039       0.067053         5.132367        0.033441
13         5.114458       0.067037         5.114788        0.033432
14         5.096349       0.067018         5.096681        0.033422
15         5.077705       0.066998         5.078037        0.033412
16         5.058513       0.066976         5.058845        0.033400
17         5.038762       0.066951         5.039095        0.033386
18         5.018446       0.066923         5.018779        0.033371
19         4.997554       0.066892         4.997888        0.033355
20         4.976100       0.066831         4.976434        0.033360
21         4.954099       0.066792         4.954428        0.033343
22         4.931506       0.066750         4.931831        0.033317
23         4.908327       0.066711         4.908635        0.033290
24         4.884551       0.066674         4.884849        0.033254
25         4.860214       0.066633         4.860499        0.033206
26         4.835265       0.066590         4.835542        0.033159
27         4.809761       0.066502         4.810004        0.033136
28         4.783799       0.066529         4.784023        0.032987
29         4.757358       0.066436         4.757561        0.032940
..              ...            ...              ...             ...
359        3.766983       0.055035         3.680959        0.030388
360        3.766910       0.055045         3.680635        0.030483
361        3.766742       0.055051         3.680374        0.030479
362        3.766682       0.055036         3.680098        0.030571
363        3.766557       0.054947         3.679821        0.030587
364        3.766479       0.054989         3.679559        0.030598
365        3.766409       0.054980         3.679311        0.030608
366        3.766270       0.055059         3.678980        0.030588
367        3.766117       0.055159         3.678739        0.030595
368        3.766056       0.055132         3.678524        0.030566
369        3.765996       0.055093         3.678245        0.030559
370        3.765931       0.055111         3.677948        0.030544
371        3.765827       0.055100         3.677680        0.030529
372        3.765782       0.055077         3.677436        0.030480
373        3.765706       0.055067         3.677118        0.030596
374        3.765610       0.055042         3.676894        0.030632
375        3.765579       0.055055         3.676741        0.030615
376        3.765522       0.054990         3.676509        0.030582
377        3.765484       0.054976         3.676298        0.030615
378        3.765275       0.055029         3.675971        0.030565
379        3.765258       0.054946         3.675677        0.030608
380        3.765191       0.054982         3.675463        0.030566
381        3.765137       0.054992         3.675133        0.030660
382        3.765125       0.054970         3.674884        0.030781
383        3.764964       0.054940         3.674589        0.030789
384        3.764950       0.054970         3.674375        0.030815
385        3.764813       0.054984         3.674061        0.030855
386        3.764681       0.054993         3.673772        0.030842
387        3.764584       0.054915         3.673515        0.030861
388        3.764492       0.054890         3.673238        0.030870

[389 rows x 4 columns]