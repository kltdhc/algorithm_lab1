Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 2, 'learning_rate': 0.001, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33593	dev-rmse:5.26883
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.10974	dev-rmse:5.04229
[200]	train-rmse:4.91644	dev-rmse:4.84884
[300]	train-rmse:4.75209	dev-rmse:4.68433
[400]	train-rmse:4.61276	dev-rmse:4.54481
[500]	train-rmse:4.49504	dev-rmse:4.42716
[600]	train-rmse:4.39589	dev-rmse:4.32816
[700]	train-rmse:4.31243	dev-rmse:4.2449
[800]	train-rmse:4.24215	dev-rmse:4.17516
[900]	train-rmse:4.18317	dev-rmse:4.11681
[1000]	train-rmse:4.1336	dev-rmse:4.06792
[1100]	train-rmse:4.09192	dev-rmse:4.02684
[1200]	train-rmse:4.05697	dev-rmse:3.99246
[1300]	train-rmse:4.02747	dev-rmse:3.9636
[1400]	train-rmse:4.00261	dev-rmse:3.93935
[1500]	train-rmse:3.98155	dev-rmse:3.91892
[1600]	train-rmse:3.96364	dev-rmse:3.90163
[1700]	train-rmse:3.94848	dev-rmse:3.88699
[1800]	train-rmse:3.93554	dev-rmse:3.87453
[1900]	train-rmse:3.92445	dev-rmse:3.86395
[2000]	train-rmse:3.91487	dev-rmse:3.85495
[2100]	train-rmse:3.90655	dev-rmse:3.84723
[2200]	train-rmse:3.89931	dev-rmse:3.84054
[2300]	train-rmse:3.89292	dev-rmse:3.8346
[2400]	train-rmse:3.88728	dev-rmse:3.82948
[2500]	train-rmse:3.88226	dev-rmse:3.82496
[2600]	train-rmse:3.87772	dev-rmse:3.82088
[2700]	train-rmse:3.87362	dev-rmse:3.81729
[2800]	train-rmse:3.86994	dev-rmse:3.81403
[2900]	train-rmse:3.86657	dev-rmse:3.81119
[3000]	train-rmse:3.86343	dev-rmse:3.80839
[3100]	train-rmse:3.86051	dev-rmse:3.80591
[3200]	train-rmse:3.85781	dev-rmse:3.8037
[3300]	train-rmse:3.85527	dev-rmse:3.80149
[3400]	train-rmse:3.85289	dev-rmse:3.79952
[3500]	train-rmse:3.85064	dev-rmse:3.79761
[3600]	train-rmse:3.84848	dev-rmse:3.79581
[3700]	train-rmse:3.84639	dev-rmse:3.79407
[3800]	train-rmse:3.84443	dev-rmse:3.79245
[3900]	train-rmse:3.84251	dev-rmse:3.79086
[4000]	train-rmse:3.84068	dev-rmse:3.78945
[4100]	train-rmse:3.83892	dev-rmse:3.78802
[4200]	train-rmse:3.83723	dev-rmse:3.78668
[4300]	train-rmse:3.83559	dev-rmse:3.78539
[4400]	train-rmse:3.83398	dev-rmse:3.78408
[4500]	train-rmse:3.83246	dev-rmse:3.78294
[4600]	train-rmse:3.83091	dev-rmse:3.78172
[4700]	train-rmse:3.82944	dev-rmse:3.78062
[4800]	train-rmse:3.82804	dev-rmse:3.77958
[4900]	train-rmse:3.82667	dev-rmse:3.77863
[5000]	train-rmse:3.82529	dev-rmse:3.77757
[5100]	train-rmse:3.82396	dev-rmse:3.77651
[5200]	train-rmse:3.82265	dev-rmse:3.77552
[5300]	train-rmse:3.82138	dev-rmse:3.77456
[5400]	train-rmse:3.82015	dev-rmse:3.77361
[5500]	train-rmse:3.8189	dev-rmse:3.77264
[5600]	train-rmse:3.81772	dev-rmse:3.77174
[5700]	train-rmse:3.81656	dev-rmse:3.77088
[5800]	train-rmse:3.81543	dev-rmse:3.77002
[5900]	train-rmse:3.81429	dev-rmse:3.76911
[6000]	train-rmse:3.81319	dev-rmse:3.76828
[6100]	train-rmse:3.81208	dev-rmse:3.76749
[6200]	train-rmse:3.81101	dev-rmse:3.76672
[6300]	train-rmse:3.80997	dev-rmse:3.76593
[6400]	train-rmse:3.80894	dev-rmse:3.76517
[6500]	train-rmse:3.80791	dev-rmse:3.76442
[6600]	train-rmse:3.80691	dev-rmse:3.76367
[6700]	train-rmse:3.80592	dev-rmse:3.76294
[6800]	train-rmse:3.80498	dev-rmse:3.76223
[6900]	train-rmse:3.80403	dev-rmse:3.76152
[7000]	train-rmse:3.80306	dev-rmse:3.76077
[7100]	train-rmse:3.80211	dev-rmse:3.76006
[7200]	train-rmse:3.80119	dev-rmse:3.75936
[7300]	train-rmse:3.80024	dev-rmse:3.75861
[7400]	train-rmse:3.79935	dev-rmse:3.7579
[7500]	train-rmse:3.79844	dev-rmse:3.75722
[7600]	train-rmse:3.79756	dev-rmse:3.7566
[7700]	train-rmse:3.79671	dev-rmse:3.75597
[7800]	train-rmse:3.79588	dev-rmse:3.75538
[7900]	train-rmse:3.79507	dev-rmse:3.7548
[8000]	train-rmse:3.79427	dev-rmse:3.75422
[8100]	train-rmse:3.79348	dev-rmse:3.75364
[8200]	train-rmse:3.79269	dev-rmse:3.75306
[8300]	train-rmse:3.79193	dev-rmse:3.75254
[8400]	train-rmse:3.79117	dev-rmse:3.75195
[8500]	train-rmse:3.79041	dev-rmse:3.75141
[8600]	train-rmse:3.78968	dev-rmse:3.75092
[8700]	train-rmse:3.78894	dev-rmse:3.75041
[8800]	train-rmse:3.78821	dev-rmse:3.74989
[8900]	train-rmse:3.7875	dev-rmse:3.74935
[9000]	train-rmse:3.78677	dev-rmse:3.74884
[9100]	train-rmse:3.78609	dev-rmse:3.74835
[9200]	train-rmse:3.78541	dev-rmse:3.74791
[9300]	train-rmse:3.78472	dev-rmse:3.74741
[9400]	train-rmse:3.78405	dev-rmse:3.747
[9500]	train-rmse:3.78336	dev-rmse:3.74658
[9600]	train-rmse:3.78272	dev-rmse:3.74614
[9700]	train-rmse:3.78207	dev-rmse:3.7457
[9800]	train-rmse:3.78141	dev-rmse:3.74524
[9900]	train-rmse:3.78079	dev-rmse:3.7448
[10000]	train-rmse:3.78016	dev-rmse:3.74436
[10100]	train-rmse:3.77951	dev-rmse:3.74388
[10200]	train-rmse:3.7789	dev-rmse:3.74351
[10300]	train-rmse:3.77826	dev-rmse:3.74308
[10400]	train-rmse:3.77767	dev-rmse:3.74274
[10500]	train-rmse:3.7771	dev-rmse:3.74239
[10600]	train-rmse:3.77652	dev-rmse:3.74201
[10700]	train-rmse:3.77593	dev-rmse:3.74162
[10800]	train-rmse:3.77535	dev-rmse:3.74127
[10900]	train-rmse:3.7748	dev-rmse:3.74093
[11000]	train-rmse:3.77423	dev-rmse:3.74058
[11100]	train-rmse:3.77368	dev-rmse:3.74023
[11200]	train-rmse:3.77313	dev-rmse:3.73989
[11300]	train-rmse:3.77259	dev-rmse:3.73956
[11400]	train-rmse:3.77205	dev-rmse:3.73925
[11500]	train-rmse:3.77151	dev-rmse:3.73892
[11600]	train-rmse:3.77098	dev-rmse:3.73858
[11700]	train-rmse:3.77044	dev-rmse:3.73822
[11800]	train-rmse:3.76994	dev-rmse:3.73792
[11900]	train-rmse:3.76943	dev-rmse:3.73763
[12000]	train-rmse:3.7689	dev-rmse:3.73729
[12100]	train-rmse:3.76842	dev-rmse:3.737
[12200]	train-rmse:3.76792	dev-rmse:3.73669
[12300]	train-rmse:3.76742	dev-rmse:3.73636
[12400]	train-rmse:3.76695	dev-rmse:3.73612
[12500]	train-rmse:3.76649	dev-rmse:3.73587
[12600]	train-rmse:3.76601	dev-rmse:3.73557
[12700]	train-rmse:3.76554	dev-rmse:3.73529
[12800]	train-rmse:3.76505	dev-rmse:3.735
[12900]	train-rmse:3.76458	dev-rmse:3.73473
[13000]	train-rmse:3.76413	dev-rmse:3.73451
[13100]	train-rmse:3.76365	dev-rmse:3.73423
[13200]	train-rmse:3.7632	dev-rmse:3.73398
[13300]	train-rmse:3.76278	dev-rmse:3.73375
[13400]	train-rmse:3.76235	dev-rmse:3.73352
[13500]	train-rmse:3.76192	dev-rmse:3.73333
[13600]	train-rmse:3.76149	dev-rmse:3.7331
[13700]	train-rmse:3.76108	dev-rmse:3.73289
[13800]	train-rmse:3.76066	dev-rmse:3.7327
[13900]	train-rmse:3.76025	dev-rmse:3.73246
[14000]	train-rmse:3.75982	dev-rmse:3.73222
[14100]	train-rmse:3.75941	dev-rmse:3.73201
[14200]	train-rmse:3.759	dev-rmse:3.7318
[14300]	train-rmse:3.7586	dev-rmse:3.73163
[14400]	train-rmse:3.7582	dev-rmse:3.73144
[14500]	train-rmse:3.7578	dev-rmse:3.73125
[14600]	train-rmse:3.75742	dev-rmse:3.73107
[14700]	train-rmse:3.75702	dev-rmse:3.73084
[14800]	train-rmse:3.75663	dev-rmse:3.73063
[14900]	train-rmse:3.75625	dev-rmse:3.73045
[15000]	train-rmse:3.75587	dev-rmse:3.73026
[15100]	train-rmse:3.7555	dev-rmse:3.73006
[15200]	train-rmse:3.75512	dev-rmse:3.72988
[15300]	train-rmse:3.75473	dev-rmse:3.72972
[15400]	train-rmse:3.75436	dev-rmse:3.72954
[15500]	train-rmse:3.754	dev-rmse:3.72938
[15600]	train-rmse:3.75364	dev-rmse:3.72924
[15700]	train-rmse:3.75328	dev-rmse:3.72909
[15800]	train-rmse:3.75292	dev-rmse:3.72896
[15900]	train-rmse:3.75256	dev-rmse:3.72878
[16000]	train-rmse:3.7522	dev-rmse:3.72862
[16100]	train-rmse:3.75185	dev-rmse:3.72844
[16200]	train-rmse:3.75149	dev-rmse:3.7283
[16300]	train-rmse:3.75117	dev-rmse:3.72818
[16400]	train-rmse:3.75083	dev-rmse:3.72803
[16500]	train-rmse:3.75049	dev-rmse:3.72788
[16600]	train-rmse:3.75016	dev-rmse:3.72776
[16700]	train-rmse:3.74982	dev-rmse:3.72764
[16800]	train-rmse:3.74948	dev-rmse:3.72747
[16900]	train-rmse:3.74917	dev-rmse:3.72733
[17000]	train-rmse:3.74884	dev-rmse:3.72721
[17100]	train-rmse:3.74852	dev-rmse:3.72708
[17200]	train-rmse:3.74822	dev-rmse:3.72694
[17300]	train-rmse:3.7479	dev-rmse:3.72684
[17400]	train-rmse:3.74759	dev-rmse:3.72671
[17500]	train-rmse:3.74728	dev-rmse:3.72661
[17600]	train-rmse:3.74698	dev-rmse:3.72652
[17700]	train-rmse:3.74667	dev-rmse:3.72641
[17800]	train-rmse:3.74637	dev-rmse:3.72631
[17900]	train-rmse:3.74606	dev-rmse:3.7262
[18000]	train-rmse:3.74577	dev-rmse:3.72607
[18100]	train-rmse:3.74546	dev-rmse:3.72596
Stopping. Best iteration:
[18136]	train-rmse:3.74535	dev-rmse:3.72592

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 2, 'learning_rate': 0.001, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33592	dev-rmse:5.26882
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.10969	dev-rmse:5.04199
[200]	train-rmse:4.91664	dev-rmse:4.8488
[300]	train-rmse:4.75244	dev-rmse:4.68438
[400]	train-rmse:4.61327	dev-rmse:4.54494
[500]	train-rmse:4.49572	dev-rmse:4.42724
[600]	train-rmse:4.39675	dev-rmse:4.32813
[700]	train-rmse:4.31342	dev-rmse:4.2446
[800]	train-rmse:4.24341	dev-rmse:4.17449
[900]	train-rmse:4.1847	dev-rmse:4.11637
[1000]	train-rmse:4.13546	dev-rmse:4.06783
[1100]	train-rmse:4.09415	dev-rmse:4.02723
[1200]	train-rmse:4.05945	dev-rmse:3.99325
[1300]	train-rmse:4.0302	dev-rmse:3.96463
[1400]	train-rmse:4.00547	dev-rmse:3.94032
[1500]	train-rmse:3.98461	dev-rmse:3.91985
[1600]	train-rmse:3.96689	dev-rmse:3.90267
[1700]	train-rmse:3.95184	dev-rmse:3.88815
[1800]	train-rmse:3.93901	dev-rmse:3.87585
[1900]	train-rmse:3.92802	dev-rmse:3.86535
[2000]	train-rmse:3.91858	dev-rmse:3.8564
[2100]	train-rmse:3.9104	dev-rmse:3.84871
[2200]	train-rmse:3.9032	dev-rmse:3.84194
[2300]	train-rmse:3.89689	dev-rmse:3.83605
[2400]	train-rmse:3.89131	dev-rmse:3.83085
[2500]	train-rmse:3.88633	dev-rmse:3.82627
[2600]	train-rmse:3.88184	dev-rmse:3.82215
[2700]	train-rmse:3.87777	dev-rmse:3.81844
[2800]	train-rmse:3.87408	dev-rmse:3.81511
[2900]	train-rmse:3.87073	dev-rmse:3.81212
[3000]	train-rmse:3.86764	dev-rmse:3.80939
[3100]	train-rmse:3.86478	dev-rmse:3.80688
[3200]	train-rmse:3.86211	dev-rmse:3.80458
[3300]	train-rmse:3.85961	dev-rmse:3.80243
[3400]	train-rmse:3.85728	dev-rmse:3.80044
[3500]	train-rmse:3.85507	dev-rmse:3.79859
[3600]	train-rmse:3.85298	dev-rmse:3.79685
[3700]	train-rmse:3.85098	dev-rmse:3.7952
[3800]	train-rmse:3.84904	dev-rmse:3.79359
[3900]	train-rmse:3.84719	dev-rmse:3.7921
[4000]	train-rmse:3.84541	dev-rmse:3.79065
[4100]	train-rmse:3.84369	dev-rmse:3.78924
[4200]	train-rmse:3.84204	dev-rmse:3.78788
[4300]	train-rmse:3.84044	dev-rmse:3.78657
[4400]	train-rmse:3.83888	dev-rmse:3.78528
[4500]	train-rmse:3.83738	dev-rmse:3.78403
[4600]	train-rmse:3.83592	dev-rmse:3.78287
[4700]	train-rmse:3.83451	dev-rmse:3.78176
[4800]	train-rmse:3.83313	dev-rmse:3.78066
[4900]	train-rmse:3.83178	dev-rmse:3.77956
[5000]	train-rmse:3.83046	dev-rmse:3.77851
[5100]	train-rmse:3.82918	dev-rmse:3.77746
[5200]	train-rmse:3.82791	dev-rmse:3.77643
[5300]	train-rmse:3.82668	dev-rmse:3.77541
[5400]	train-rmse:3.82549	dev-rmse:3.77443
[5500]	train-rmse:3.82431	dev-rmse:3.77347
[5600]	train-rmse:3.82317	dev-rmse:3.77255
[5700]	train-rmse:3.82206	dev-rmse:3.77169
[5800]	train-rmse:3.82097	dev-rmse:3.77085
[5900]	train-rmse:3.8199	dev-rmse:3.77002
[6000]	train-rmse:3.81887	dev-rmse:3.76924
[6100]	train-rmse:3.81784	dev-rmse:3.76843
[6200]	train-rmse:3.81683	dev-rmse:3.76765
[6300]	train-rmse:3.81584	dev-rmse:3.7669
[6400]	train-rmse:3.81488	dev-rmse:3.76617
[6500]	train-rmse:3.81393	dev-rmse:3.76545
[6600]	train-rmse:3.813	dev-rmse:3.76476
[6700]	train-rmse:3.81209	dev-rmse:3.76408
[6800]	train-rmse:3.81121	dev-rmse:3.76342
[6900]	train-rmse:3.81031	dev-rmse:3.76273
[7000]	train-rmse:3.8094	dev-rmse:3.76204
[7100]	train-rmse:3.80853	dev-rmse:3.76137
[7200]	train-rmse:3.80768	dev-rmse:3.76072
[7300]	train-rmse:3.80685	dev-rmse:3.76009
[7400]	train-rmse:3.80602	dev-rmse:3.75947
[7500]	train-rmse:3.80521	dev-rmse:3.75888
[7600]	train-rmse:3.80441	dev-rmse:3.75827
[7700]	train-rmse:3.80361	dev-rmse:3.75767
[7800]	train-rmse:3.80283	dev-rmse:3.75709
[7900]	train-rmse:3.80206	dev-rmse:3.75651
[8000]	train-rmse:3.80131	dev-rmse:3.75596
[8100]	train-rmse:3.80056	dev-rmse:3.75539
[8200]	train-rmse:3.79982	dev-rmse:3.75485
[8300]	train-rmse:3.79906	dev-rmse:3.75428
[8400]	train-rmse:3.79833	dev-rmse:3.75376
[8500]	train-rmse:3.79761	dev-rmse:3.75325
[8600]	train-rmse:3.79689	dev-rmse:3.75274
[8700]	train-rmse:3.79621	dev-rmse:3.75228
[8800]	train-rmse:3.79553	dev-rmse:3.75183
[8900]	train-rmse:3.79485	dev-rmse:3.75136
[9000]	train-rmse:3.79417	dev-rmse:3.7509
[9100]	train-rmse:3.7935	dev-rmse:3.75045
[9200]	train-rmse:3.79285	dev-rmse:3.75
[9300]	train-rmse:3.79219	dev-rmse:3.74957
[9400]	train-rmse:3.79155	dev-rmse:3.74913
[9500]	train-rmse:3.79092	dev-rmse:3.74869
[9600]	train-rmse:3.79029	dev-rmse:3.74827
[9700]	train-rmse:3.78968	dev-rmse:3.74786
[9800]	train-rmse:3.78906	dev-rmse:3.74745
[9900]	train-rmse:3.78845	dev-rmse:3.74704
[10000]	train-rmse:3.78785	dev-rmse:3.74662
[10100]	train-rmse:3.78726	dev-rmse:3.74622
[10200]	train-rmse:3.78667	dev-rmse:3.74582
[10300]	train-rmse:3.78608	dev-rmse:3.74542
[10400]	train-rmse:3.78551	dev-rmse:3.74504
[10500]	train-rmse:3.78488	dev-rmse:3.74453
[10600]	train-rmse:3.7843	dev-rmse:3.7441
[10700]	train-rmse:3.78375	dev-rmse:3.74373
[10800]	train-rmse:3.78322	dev-rmse:3.7434
[10900]	train-rmse:3.78269	dev-rmse:3.74306
[11000]	train-rmse:3.78215	dev-rmse:3.74272
[11100]	train-rmse:3.78162	dev-rmse:3.74237
[11200]	train-rmse:3.78111	dev-rmse:3.74203
[11300]	train-rmse:3.78058	dev-rmse:3.74169
[11400]	train-rmse:3.78008	dev-rmse:3.74137
[11500]	train-rmse:3.77958	dev-rmse:3.74105
[11600]	train-rmse:3.77909	dev-rmse:3.74074
[11700]	train-rmse:3.77859	dev-rmse:3.74042
[11800]	train-rmse:3.7781	dev-rmse:3.74011
[11900]	train-rmse:3.77761	dev-rmse:3.73978
[12000]	train-rmse:3.77711	dev-rmse:3.73944
[12100]	train-rmse:3.77664	dev-rmse:3.73912
[12200]	train-rmse:3.77616	dev-rmse:3.73881
[12300]	train-rmse:3.77569	dev-rmse:3.73848
[12400]	train-rmse:3.77522	dev-rmse:3.73816
[12500]	train-rmse:3.77475	dev-rmse:3.73784
[12600]	train-rmse:3.77429	dev-rmse:3.73752
[12700]	train-rmse:3.77382	dev-rmse:3.73719
[12800]	train-rmse:3.77337	dev-rmse:3.7369
[12900]	train-rmse:3.77291	dev-rmse:3.73659
[13000]	train-rmse:3.77246	dev-rmse:3.7363
[13100]	train-rmse:3.77202	dev-rmse:3.73601
[13200]	train-rmse:3.77157	dev-rmse:3.73573
[13300]	train-rmse:3.77114	dev-rmse:3.73543
[13400]	train-rmse:3.7707	dev-rmse:3.73516
[13500]	train-rmse:3.77028	dev-rmse:3.73488
[13600]	train-rmse:3.76986	dev-rmse:3.73463
[13700]	train-rmse:3.76943	dev-rmse:3.73435
[13800]	train-rmse:3.76902	dev-rmse:3.73411
[13900]	train-rmse:3.76862	dev-rmse:3.73389
[14000]	train-rmse:3.76821	dev-rmse:3.73364
[14100]	train-rmse:3.76781	dev-rmse:3.7334
[14200]	train-rmse:3.7674	dev-rmse:3.73317
[14300]	train-rmse:3.76701	dev-rmse:3.73295
[14400]	train-rmse:3.76661	dev-rmse:3.73272
[14500]	train-rmse:3.76623	dev-rmse:3.73251
[14600]	train-rmse:3.76585	dev-rmse:3.7323
[14700]	train-rmse:3.76546	dev-rmse:3.73209
[14800]	train-rmse:3.7651	dev-rmse:3.73191
[14900]	train-rmse:3.76475	dev-rmse:3.73172
[15000]	train-rmse:3.76438	dev-rmse:3.73152
[15100]	train-rmse:3.76403	dev-rmse:3.73134
[15200]	train-rmse:3.76368	dev-rmse:3.73116
[15300]	train-rmse:3.76333	dev-rmse:3.73097
[15400]	train-rmse:3.76298	dev-rmse:3.7308
[15500]	train-rmse:3.76264	dev-rmse:3.73061
[15600]	train-rmse:3.76229	dev-rmse:3.73044
[15700]	train-rmse:3.76195	dev-rmse:3.73027
[15800]	train-rmse:3.76161	dev-rmse:3.7301
[15900]	train-rmse:3.76127	dev-rmse:3.72994
[16000]	train-rmse:3.76093	dev-rmse:3.72978
[16100]	train-rmse:3.7606	dev-rmse:3.72962
[16200]	train-rmse:3.76027	dev-rmse:3.72946
[16300]	train-rmse:3.75994	dev-rmse:3.72931
[16400]	train-rmse:3.75962	dev-rmse:3.72915
[16500]	train-rmse:3.75929	dev-rmse:3.729
[16600]	train-rmse:3.75897	dev-rmse:3.72886
[16700]	train-rmse:3.75866	dev-rmse:3.72872
[16800]	train-rmse:3.75834	dev-rmse:3.72858
[16900]	train-rmse:3.75803	dev-rmse:3.72845
[17000]	train-rmse:3.75771	dev-rmse:3.72831
[17100]	train-rmse:3.75739	dev-rmse:3.72817
[17200]	train-rmse:3.75709	dev-rmse:3.72805
[17300]	train-rmse:3.75679	dev-rmse:3.72793
[17400]	train-rmse:3.75649	dev-rmse:3.7278
[17500]	train-rmse:3.75619	dev-rmse:3.72768
[17600]	train-rmse:3.7559	dev-rmse:3.72757
[17700]	train-rmse:3.75561	dev-rmse:3.72744
[17800]	train-rmse:3.75532	dev-rmse:3.72732
[17900]	train-rmse:3.75503	dev-rmse:3.72721
[18000]	train-rmse:3.75474	dev-rmse:3.72708
[18100]	train-rmse:3.75445	dev-rmse:3.72697
[18200]	train-rmse:3.75416	dev-rmse:3.72685
[18300]	train-rmse:3.75388	dev-rmse:3.72673
[18400]	train-rmse:3.7536	dev-rmse:3.72662
[18500]	train-rmse:3.7533	dev-rmse:3.72651
[18600]	train-rmse:3.75303	dev-rmse:3.7264
[18700]	train-rmse:3.75274	dev-rmse:3.72631
[18800]	train-rmse:3.75247	dev-rmse:3.72621
[18900]	train-rmse:3.7522	dev-rmse:3.72612
[19000]	train-rmse:3.75192	dev-rmse:3.72602
[19100]	train-rmse:3.75166	dev-rmse:3.72594
[19200]	train-rmse:3.75138	dev-rmse:3.72584
[19300]	train-rmse:3.75112	dev-rmse:3.72577
[19400]	train-rmse:3.75085	dev-rmse:3.72568
[19500]	train-rmse:3.75058	dev-rmse:3.72559
[19600]	train-rmse:3.75032	dev-rmse:3.7255
[19700]	train-rmse:3.75005	dev-rmse:3.72542
[19800]	train-rmse:3.7498	dev-rmse:3.72535
[19900]	train-rmse:3.74953	dev-rmse:3.72527
[20000]	train-rmse:3.74927	dev-rmse:3.72519
[20100]	train-rmse:3.74901	dev-rmse:3.72512
[20200]	train-rmse:3.74875	dev-rmse:3.72505
[20300]	train-rmse:3.74849	dev-rmse:3.72498
[20400]	train-rmse:3.74824	dev-rmse:3.72492
[20500]	train-rmse:3.74798	dev-rmse:3.72485
[20600]	train-rmse:3.74772	dev-rmse:3.72478
[20700]	train-rmse:3.74747	dev-rmse:3.72472
[20800]	train-rmse:3.74723	dev-rmse:3.72465
[20900]	train-rmse:3.74699	dev-rmse:3.7246
[21000]	train-rmse:3.74675	dev-rmse:3.72453
[21100]	train-rmse:3.74651	dev-rmse:3.72446
[21200]	train-rmse:3.74627	dev-rmse:3.72438
[21300]	train-rmse:3.74604	dev-rmse:3.72431
[21400]	train-rmse:3.7458	dev-rmse:3.72424
[21500]	train-rmse:3.74558	dev-rmse:3.72417
[21600]	train-rmse:3.74534	dev-rmse:3.7241
[21700]	train-rmse:3.74512	dev-rmse:3.72403
[21800]	train-rmse:3.74489	dev-rmse:3.72396
[21900]	train-rmse:3.74466	dev-rmse:3.7239
[22000]	train-rmse:3.74444	dev-rmse:3.72383
[22100]	train-rmse:3.74422	dev-rmse:3.72376
[22200]	train-rmse:3.74399	dev-rmse:3.72369
[22300]	train-rmse:3.74377	dev-rmse:3.72362
[22400]	train-rmse:3.74355	dev-rmse:3.72356
[22500]	train-rmse:3.74333	dev-rmse:3.72349
[22600]	train-rmse:3.7431	dev-rmse:3.72342
[22700]	train-rmse:3.74288	dev-rmse:3.72336
[22800]	train-rmse:3.74267	dev-rmse:3.7233
[22900]	train-rmse:3.74244	dev-rmse:3.72324
[23000]	train-rmse:3.74223	dev-rmse:3.72318
[23100]	train-rmse:3.74202	dev-rmse:3.72312
[23200]	train-rmse:3.7418	dev-rmse:3.72306
[23300]	train-rmse:3.74158	dev-rmse:3.723
[23400]	train-rmse:3.74137	dev-rmse:3.72293
[23500]	train-rmse:3.74115	dev-rmse:3.72289
[23600]	train-rmse:3.74093	dev-rmse:3.72283
Stopping. Best iteration:
[23624]	train-rmse:3.74088	dev-rmse:3.72282

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 2, 'learning_rate': 0.01, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.31405	dev-rmse:5.24688
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:4.12781	dev-rmse:4.06122
[200]	train-rmse:3.91308	dev-rmse:3.85319
[300]	train-rmse:3.86273	dev-rmse:3.80754
[400]	train-rmse:3.84019	dev-rmse:3.78834
[500]	train-rmse:3.82478	dev-rmse:3.77707
[600]	train-rmse:3.81249	dev-rmse:3.76785
[700]	train-rmse:3.80256	dev-rmse:3.76005
[800]	train-rmse:3.79419	dev-rmse:3.75392
[900]	train-rmse:3.78639	dev-rmse:3.74801
[1000]	train-rmse:3.77967	dev-rmse:3.74322
[1100]	train-rmse:3.77382	dev-rmse:3.73983
[1200]	train-rmse:3.7686	dev-rmse:3.73642
[1300]	train-rmse:3.76389	dev-rmse:3.73386
[1400]	train-rmse:3.75948	dev-rmse:3.7317
[1500]	train-rmse:3.75574	dev-rmse:3.72984
[1600]	train-rmse:3.75196	dev-rmse:3.72842
[1700]	train-rmse:3.74864	dev-rmse:3.72728
[1800]	train-rmse:3.74546	dev-rmse:3.72599
[1900]	train-rmse:3.74249	dev-rmse:3.7248
Stopping. Best iteration:
[1952]	train-rmse:3.74105	dev-rmse:3.72433

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 2, 'learning_rate': 0.01, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.31398	dev-rmse:5.24681
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:4.12943	dev-rmse:4.06177
[200]	train-rmse:3.91709	dev-rmse:3.85507
[300]	train-rmse:3.8671	dev-rmse:3.80903
[400]	train-rmse:3.84512	dev-rmse:3.79052
[500]	train-rmse:3.83014	dev-rmse:3.77842
[600]	train-rmse:3.8186	dev-rmse:3.76918
[700]	train-rmse:3.8091	dev-rmse:3.76185
[800]	train-rmse:3.80076	dev-rmse:3.75536
[900]	train-rmse:3.79371	dev-rmse:3.75035
[1000]	train-rmse:3.78737	dev-rmse:3.74611
[1100]	train-rmse:3.78166	dev-rmse:3.74223
[1200]	train-rmse:3.77672	dev-rmse:3.73914
[1300]	train-rmse:3.77211	dev-rmse:3.73621
[1400]	train-rmse:3.76796	dev-rmse:3.73355
[1500]	train-rmse:3.76419	dev-rmse:3.7314
[1600]	train-rmse:3.7607	dev-rmse:3.72951
[1700]	train-rmse:3.75749	dev-rmse:3.72795
[1800]	train-rmse:3.75446	dev-rmse:3.7267
[1900]	train-rmse:3.75163	dev-rmse:3.72574
[2000]	train-rmse:3.74891	dev-rmse:3.72486
[2100]	train-rmse:3.74641	dev-rmse:3.72419
[2200]	train-rmse:3.74411	dev-rmse:3.72362
[2300]	train-rmse:3.74194	dev-rmse:3.72305
[2400]	train-rmse:3.73976	dev-rmse:3.7226
[2500]	train-rmse:3.73773	dev-rmse:3.72204
[2600]	train-rmse:3.73576	dev-rmse:3.72157
[2700]	train-rmse:3.73393	dev-rmse:3.72121
Stopping. Best iteration:
[2726]	train-rmse:3.73346	dev-rmse:3.72109

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 2, 'learning_rate': 0.005, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.32619	dev-rmse:5.25906
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:4.48997	dev-rmse:4.42198
[200]	train-rmse:4.13065	dev-rmse:4.0649
[300]	train-rmse:3.98019	dev-rmse:3.91802
[400]	train-rmse:3.91418	dev-rmse:3.85429
[500]	train-rmse:3.88196	dev-rmse:3.82447
[600]	train-rmse:3.86324	dev-rmse:3.80809
[700]	train-rmse:3.85061	dev-rmse:3.79743
[800]	train-rmse:3.84075	dev-rmse:3.78953
[900]	train-rmse:3.83224	dev-rmse:3.78248
[1000]	train-rmse:3.82485	dev-rmse:3.77646
[1100]	train-rmse:3.81857	dev-rmse:3.77157
[1200]	train-rmse:3.81296	dev-rmse:3.76715
[1300]	train-rmse:3.80775	dev-rmse:3.76304
[1400]	train-rmse:3.8029	dev-rmse:3.75945
[1500]	train-rmse:3.79853	dev-rmse:3.75634
[1600]	train-rmse:3.79433	dev-rmse:3.75343
[1700]	train-rmse:3.79043	dev-rmse:3.75083
[1800]	train-rmse:3.78682	dev-rmse:3.7483
[1900]	train-rmse:3.7832	dev-rmse:3.74575
[2000]	train-rmse:3.77995	dev-rmse:3.74373
[2100]	train-rmse:3.7771	dev-rmse:3.74196
[2200]	train-rmse:3.77422	dev-rmse:3.74034
[2300]	train-rmse:3.77147	dev-rmse:3.73874
[2400]	train-rmse:3.76896	dev-rmse:3.73729
[2500]	train-rmse:3.76651	dev-rmse:3.73583
[2600]	train-rmse:3.76419	dev-rmse:3.73445
[2700]	train-rmse:3.76194	dev-rmse:3.73322
[2800]	train-rmse:3.75981	dev-rmse:3.73207
[2900]	train-rmse:3.75783	dev-rmse:3.73114
[3000]	train-rmse:3.75579	dev-rmse:3.72999
[3100]	train-rmse:3.75386	dev-rmse:3.72904
[3200]	train-rmse:3.75203	dev-rmse:3.72819
[3300]	train-rmse:3.75034	dev-rmse:3.72742
[3400]	train-rmse:3.74876	dev-rmse:3.72668
[3500]	train-rmse:3.74721	dev-rmse:3.72615
[3600]	train-rmse:3.74567	dev-rmse:3.72566
[3700]	train-rmse:3.74415	dev-rmse:3.72512
[3800]	train-rmse:3.7427	dev-rmse:3.72462
[3900]	train-rmse:3.74137	dev-rmse:3.72397
[4000]	train-rmse:3.74003	dev-rmse:3.72356
[4100]	train-rmse:3.73875	dev-rmse:3.72305
[4200]	train-rmse:3.73744	dev-rmse:3.72262
[4300]	train-rmse:3.73618	dev-rmse:3.7222
[4400]	train-rmse:3.73502	dev-rmse:3.72183
[4500]	train-rmse:3.73389	dev-rmse:3.72154
[4600]	train-rmse:3.73267	dev-rmse:3.72115
[4700]	train-rmse:3.7315	dev-rmse:3.72084
[4800]	train-rmse:3.7304	dev-rmse:3.72056
[4900]	train-rmse:3.72929	dev-rmse:3.72023
[5000]	train-rmse:3.72811	dev-rmse:3.71983
[5100]	train-rmse:3.727	dev-rmse:3.7194
Stopping. Best iteration:
[5121]	train-rmse:3.7268	dev-rmse:3.71934

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 2, 'learning_rate': 0.005, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.32615	dev-rmse:5.25903
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:4.49034	dev-rmse:4.42185
[200]	train-rmse:4.13277	dev-rmse:4.06522
[300]	train-rmse:3.98326	dev-rmse:3.91855
[400]	train-rmse:3.91787	dev-rmse:3.85574
[500]	train-rmse:3.88592	dev-rmse:3.82589
[600]	train-rmse:3.86737	dev-rmse:3.80912
[700]	train-rmse:3.85489	dev-rmse:3.79847
[800]	train-rmse:3.84526	dev-rmse:3.79055
[900]	train-rmse:3.8372	dev-rmse:3.78392
[1000]	train-rmse:3.83031	dev-rmse:3.77837
[1100]	train-rmse:3.82414	dev-rmse:3.77337
[1200]	train-rmse:3.81871	dev-rmse:3.76913
[1300]	train-rmse:3.8138	dev-rmse:3.76534
[1400]	train-rmse:3.80928	dev-rmse:3.76193
[1500]	train-rmse:3.80501	dev-rmse:3.75867
[1600]	train-rmse:3.80113	dev-rmse:3.75577
[1700]	train-rmse:3.79741	dev-rmse:3.75304
[1800]	train-rmse:3.79394	dev-rmse:3.75063
[1900]	train-rmse:3.79074	dev-rmse:3.74849
[2000]	train-rmse:3.7877	dev-rmse:3.74643
[2100]	train-rmse:3.78477	dev-rmse:3.74447
[2200]	train-rmse:3.78203	dev-rmse:3.74267
[2300]	train-rmse:3.77945	dev-rmse:3.74101
[2400]	train-rmse:3.77705	dev-rmse:3.73946
[2500]	train-rmse:3.7747	dev-rmse:3.73787
[2600]	train-rmse:3.77242	dev-rmse:3.73633
[2700]	train-rmse:3.77022	dev-rmse:3.73492
[2800]	train-rmse:3.76818	dev-rmse:3.73361
[2900]	train-rmse:3.7662	dev-rmse:3.73249
[3000]	train-rmse:3.7644	dev-rmse:3.73153
[3100]	train-rmse:3.76266	dev-rmse:3.73064
[3200]	train-rmse:3.76094	dev-rmse:3.72979
[3300]	train-rmse:3.75931	dev-rmse:3.72901
[3400]	train-rmse:3.75772	dev-rmse:3.72833
[3500]	train-rmse:3.75618	dev-rmse:3.72765
[3600]	train-rmse:3.75472	dev-rmse:3.72704
[3700]	train-rmse:3.75328	dev-rmse:3.72648
[3800]	train-rmse:3.75187	dev-rmse:3.72598
[3900]	train-rmse:3.7505	dev-rmse:3.72551
[4000]	train-rmse:3.74924	dev-rmse:3.72519
[4100]	train-rmse:3.74794	dev-rmse:3.72477
[4200]	train-rmse:3.74672	dev-rmse:3.72443
[4300]	train-rmse:3.74555	dev-rmse:3.72411
[4400]	train-rmse:3.74441	dev-rmse:3.7238
[4500]	train-rmse:3.7433	dev-rmse:3.72348
[4600]	train-rmse:3.74223	dev-rmse:3.72322
Stopping. Best iteration:
[4584]	train-rmse:3.74239	dev-rmse:3.7232

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 2, 'learning_rate': 0.1, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.10143	dev-rmse:5.03368
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:3.7801	dev-rmse:3.74531
[200]	train-rmse:3.74138	dev-rmse:3.72614
[300]	train-rmse:3.71839	dev-rmse:3.72002
[400]	train-rmse:3.70089	dev-rmse:3.71463
Stopping. Best iteration:
[400]	train-rmse:3.70089	dev-rmse:3.71463

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 2, 'learning_rate': 0.1, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.10072	dev-rmse:5.033
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:3.78606	dev-rmse:3.74623
[200]	train-rmse:3.74912	dev-rmse:3.72728
[300]	train-rmse:3.72858	dev-rmse:3.7225
Stopping. Best iteration:
[344]	train-rmse:3.72131	dev-rmse:3.72013

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 3, 'learning_rate': 0.001, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33589	dev-rmse:5.26882
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.10584	dev-rmse:5.03958
[200]	train-rmse:4.90862	dev-rmse:4.84333
[300]	train-rmse:4.74049	dev-rmse:4.67637
[400]	train-rmse:4.59769	dev-rmse:4.53483
[500]	train-rmse:4.47666	dev-rmse:4.41547
[600]	train-rmse:4.37452	dev-rmse:4.31504
[700]	train-rmse:4.28835	dev-rmse:4.23033
[800]	train-rmse:4.21569	dev-rmse:4.15934
[900]	train-rmse:4.15453	dev-rmse:4.09975
[1000]	train-rmse:4.10285	dev-rmse:4.04961
[1100]	train-rmse:4.05931	dev-rmse:4.00742
[1200]	train-rmse:4.02257	dev-rmse:3.97204
[1300]	train-rmse:3.99149	dev-rmse:3.94232
[1400]	train-rmse:3.96517	dev-rmse:3.91744
[1500]	train-rmse:3.94279	dev-rmse:3.8965
[1600]	train-rmse:3.92366	dev-rmse:3.87869
[1700]	train-rmse:3.90738	dev-rmse:3.86371
[1800]	train-rmse:3.89338	dev-rmse:3.85087
[1900]	train-rmse:3.88124	dev-rmse:3.8399
[2000]	train-rmse:3.87064	dev-rmse:3.83048
[2100]	train-rmse:3.86138	dev-rmse:3.82227
[2200]	train-rmse:3.85323	dev-rmse:3.81518
[2300]	train-rmse:3.84595	dev-rmse:3.8089
[2400]	train-rmse:3.8394	dev-rmse:3.80346
[2500]	train-rmse:3.83356	dev-rmse:3.79866
[2600]	train-rmse:3.82829	dev-rmse:3.79441
[2700]	train-rmse:3.82349	dev-rmse:3.79068
[2800]	train-rmse:3.81913	dev-rmse:3.78725
[2900]	train-rmse:3.81502	dev-rmse:3.78412
[3000]	train-rmse:3.81122	dev-rmse:3.78119
[3100]	train-rmse:3.80766	dev-rmse:3.7786
[3200]	train-rmse:3.80433	dev-rmse:3.77624
[3300]	train-rmse:3.80119	dev-rmse:3.77388
[3400]	train-rmse:3.7982	dev-rmse:3.7718
[3500]	train-rmse:3.7954	dev-rmse:3.76975
[3600]	train-rmse:3.79276	dev-rmse:3.76792
[3700]	train-rmse:3.7901	dev-rmse:3.76609
[3800]	train-rmse:3.78766	dev-rmse:3.76445
[3900]	train-rmse:3.78525	dev-rmse:3.76275
[4000]	train-rmse:3.78301	dev-rmse:3.76124
[4100]	train-rmse:3.78087	dev-rmse:3.75983
[4200]	train-rmse:3.7787	dev-rmse:3.75835
[4300]	train-rmse:3.7767	dev-rmse:3.75709
[4400]	train-rmse:3.77471	dev-rmse:3.75574
[4500]	train-rmse:3.77277	dev-rmse:3.75459
[4600]	train-rmse:3.77084	dev-rmse:3.75343
[4700]	train-rmse:3.76893	dev-rmse:3.75229
[4800]	train-rmse:3.76709	dev-rmse:3.75124
[4900]	train-rmse:3.76534	dev-rmse:3.75029
[5000]	train-rmse:3.76357	dev-rmse:3.74919
[5100]	train-rmse:3.76189	dev-rmse:3.74821
[5200]	train-rmse:3.76024	dev-rmse:3.74726
[5300]	train-rmse:3.75863	dev-rmse:3.74639
[5400]	train-rmse:3.75702	dev-rmse:3.74537
[5500]	train-rmse:3.75539	dev-rmse:3.74437
[5600]	train-rmse:3.75381	dev-rmse:3.74343
[5700]	train-rmse:3.75232	dev-rmse:3.74265
[5800]	train-rmse:3.75085	dev-rmse:3.7418
[5900]	train-rmse:3.74939	dev-rmse:3.74101
[6000]	train-rmse:3.74799	dev-rmse:3.74027
[6100]	train-rmse:3.74656	dev-rmse:3.73949
[6200]	train-rmse:3.74518	dev-rmse:3.73877
[6300]	train-rmse:3.74383	dev-rmse:3.73802
[6400]	train-rmse:3.74248	dev-rmse:3.73726
[6500]	train-rmse:3.74116	dev-rmse:3.73661
[6600]	train-rmse:3.73991	dev-rmse:3.73605
[6700]	train-rmse:3.73862	dev-rmse:3.73539
[6800]	train-rmse:3.73738	dev-rmse:3.73481
[6900]	train-rmse:3.73616	dev-rmse:3.73425
[7000]	train-rmse:3.73493	dev-rmse:3.73367
[7100]	train-rmse:3.73373	dev-rmse:3.73308
[7200]	train-rmse:3.73254	dev-rmse:3.73248
[7300]	train-rmse:3.73133	dev-rmse:3.73194
[7400]	train-rmse:3.73016	dev-rmse:3.73136
[7500]	train-rmse:3.729	dev-rmse:3.73083
[7600]	train-rmse:3.72787	dev-rmse:3.7303
[7700]	train-rmse:3.72676	dev-rmse:3.72985
[7800]	train-rmse:3.7257	dev-rmse:3.72943
[7900]	train-rmse:3.72466	dev-rmse:3.72905
[8000]	train-rmse:3.72366	dev-rmse:3.72864
[8100]	train-rmse:3.72262	dev-rmse:3.72824
[8200]	train-rmse:3.72158	dev-rmse:3.72781
[8300]	train-rmse:3.72057	dev-rmse:3.72743
[8400]	train-rmse:3.71955	dev-rmse:3.727
[8500]	train-rmse:3.71855	dev-rmse:3.72658
[8600]	train-rmse:3.71758	dev-rmse:3.72625
[8700]	train-rmse:3.71658	dev-rmse:3.72585
[8800]	train-rmse:3.7156	dev-rmse:3.72553
[8900]	train-rmse:3.71468	dev-rmse:3.72518
[9000]	train-rmse:3.71374	dev-rmse:3.72487
[9100]	train-rmse:3.71281	dev-rmse:3.72453
[9200]	train-rmse:3.71193	dev-rmse:3.72426
[9300]	train-rmse:3.71101	dev-rmse:3.72389
[9400]	train-rmse:3.71009	dev-rmse:3.72356
[9500]	train-rmse:3.70919	dev-rmse:3.72326
[9600]	train-rmse:3.70833	dev-rmse:3.72298
[9700]	train-rmse:3.70745	dev-rmse:3.72272
[9800]	train-rmse:3.7066	dev-rmse:3.72243
[9900]	train-rmse:3.70577	dev-rmse:3.72216
[10000]	train-rmse:3.70497	dev-rmse:3.72194
[10100]	train-rmse:3.70411	dev-rmse:3.72168
[10200]	train-rmse:3.70327	dev-rmse:3.72145
[10300]	train-rmse:3.70246	dev-rmse:3.72124
[10400]	train-rmse:3.70165	dev-rmse:3.721
[10500]	train-rmse:3.70087	dev-rmse:3.72079
[10600]	train-rmse:3.7001	dev-rmse:3.72055
[10700]	train-rmse:3.69932	dev-rmse:3.72031
[10800]	train-rmse:3.69853	dev-rmse:3.72015
[10900]	train-rmse:3.69778	dev-rmse:3.71999
[11000]	train-rmse:3.69699	dev-rmse:3.71979
[11100]	train-rmse:3.69621	dev-rmse:3.71956
[11200]	train-rmse:3.69546	dev-rmse:3.71936
[11300]	train-rmse:3.69471	dev-rmse:3.71922
[11400]	train-rmse:3.69396	dev-rmse:3.719
[11500]	train-rmse:3.69325	dev-rmse:3.71884
[11600]	train-rmse:3.69251	dev-rmse:3.71867
[11700]	train-rmse:3.69177	dev-rmse:3.7185
[11800]	train-rmse:3.69106	dev-rmse:3.71834
[11900]	train-rmse:3.69035	dev-rmse:3.71818
[12000]	train-rmse:3.68963	dev-rmse:3.71799
[12100]	train-rmse:3.68894	dev-rmse:3.71783
[12200]	train-rmse:3.68822	dev-rmse:3.71765
[12300]	train-rmse:3.6875	dev-rmse:3.71747
[12400]	train-rmse:3.68684	dev-rmse:3.71734
[12500]	train-rmse:3.68617	dev-rmse:3.7172
[12600]	train-rmse:3.68548	dev-rmse:3.71708
[12700]	train-rmse:3.68483	dev-rmse:3.71691
[12800]	train-rmse:3.68417	dev-rmse:3.71676
[12900]	train-rmse:3.68352	dev-rmse:3.71661
[13000]	train-rmse:3.68285	dev-rmse:3.71649
[13100]	train-rmse:3.68218	dev-rmse:3.71636
[13200]	train-rmse:3.68156	dev-rmse:3.71621
[13300]	train-rmse:3.68094	dev-rmse:3.71609
[13400]	train-rmse:3.68029	dev-rmse:3.71596
Stopping. Best iteration:
[13470]	train-rmse:3.67982	dev-rmse:3.71588

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 3, 'learning_rate': 0.001, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33588	dev-rmse:5.26879
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.10576	dev-rmse:5.03957
[200]	train-rmse:4.90879	dev-rmse:4.84358
[300]	train-rmse:4.74083	dev-rmse:4.67692
[400]	train-rmse:4.59815	dev-rmse:4.53551
[500]	train-rmse:4.47743	dev-rmse:4.41614
[600]	train-rmse:4.37541	dev-rmse:4.31564
[700]	train-rmse:4.28949	dev-rmse:4.23112
[800]	train-rmse:4.21734	dev-rmse:4.16049
[900]	train-rmse:4.15673	dev-rmse:4.10132
[1000]	train-rmse:4.10534	dev-rmse:4.05117
[1100]	train-rmse:4.06194	dev-rmse:4.00929
[1200]	train-rmse:4.0253	dev-rmse:3.97409
[1300]	train-rmse:3.99447	dev-rmse:3.94453
[1400]	train-rmse:3.96845	dev-rmse:3.91967
[1500]	train-rmse:3.94637	dev-rmse:3.89863
[1600]	train-rmse:3.92755	dev-rmse:3.88083
[1700]	train-rmse:3.91147	dev-rmse:3.86574
[1800]	train-rmse:3.89759	dev-rmse:3.85279
[1900]	train-rmse:3.88559	dev-rmse:3.8417
[2000]	train-rmse:3.87519	dev-rmse:3.83211
[2100]	train-rmse:3.86615	dev-rmse:3.82389
[2200]	train-rmse:3.85814	dev-rmse:3.8167
[2300]	train-rmse:3.85105	dev-rmse:3.81037
[2400]	train-rmse:3.84473	dev-rmse:3.80488
[2500]	train-rmse:3.83906	dev-rmse:3.79998
[2600]	train-rmse:3.83396	dev-rmse:3.79558
[2700]	train-rmse:3.82932	dev-rmse:3.7916
[2800]	train-rmse:3.82503	dev-rmse:3.78796
[2900]	train-rmse:3.82095	dev-rmse:3.78453
[3000]	train-rmse:3.81713	dev-rmse:3.78133
[3100]	train-rmse:3.8136	dev-rmse:3.77845
[3200]	train-rmse:3.81032	dev-rmse:3.7759
[3300]	train-rmse:3.80727	dev-rmse:3.77357
[3400]	train-rmse:3.80447	dev-rmse:3.77154
[3500]	train-rmse:3.80177	dev-rmse:3.76962
[3600]	train-rmse:3.79921	dev-rmse:3.76779
[3700]	train-rmse:3.7968	dev-rmse:3.76612
[3800]	train-rmse:3.79452	dev-rmse:3.76457
[3900]	train-rmse:3.7923	dev-rmse:3.7631
[4000]	train-rmse:3.79016	dev-rmse:3.76173
[4100]	train-rmse:3.78804	dev-rmse:3.76037
[4200]	train-rmse:3.78597	dev-rmse:3.75902
[4300]	train-rmse:3.78396	dev-rmse:3.75769
[4400]	train-rmse:3.78208	dev-rmse:3.75648
[4500]	train-rmse:3.78026	dev-rmse:3.75538
[4600]	train-rmse:3.77849	dev-rmse:3.75431
[4700]	train-rmse:3.77677	dev-rmse:3.75328
[4800]	train-rmse:3.7751	dev-rmse:3.75228
[4900]	train-rmse:3.77348	dev-rmse:3.75133
[5000]	train-rmse:3.77189	dev-rmse:3.75039
[5100]	train-rmse:3.77036	dev-rmse:3.74955
[5200]	train-rmse:3.76886	dev-rmse:3.74872
[5300]	train-rmse:3.76737	dev-rmse:3.74789
[5400]	train-rmse:3.76592	dev-rmse:3.7471
[5500]	train-rmse:3.76452	dev-rmse:3.74633
[5600]	train-rmse:3.76314	dev-rmse:3.74551
[5700]	train-rmse:3.76179	dev-rmse:3.74476
[5800]	train-rmse:3.76044	dev-rmse:3.74403
[5900]	train-rmse:3.75906	dev-rmse:3.74326
[6000]	train-rmse:3.75768	dev-rmse:3.74248
[6100]	train-rmse:3.75632	dev-rmse:3.74179
[6200]	train-rmse:3.755	dev-rmse:3.74114
[6300]	train-rmse:3.75368	dev-rmse:3.74049
[6400]	train-rmse:3.75241	dev-rmse:3.73987
[6500]	train-rmse:3.75111	dev-rmse:3.7392
[6600]	train-rmse:3.74982	dev-rmse:3.73852
[6700]	train-rmse:3.74853	dev-rmse:3.73784
[6800]	train-rmse:3.74729	dev-rmse:3.73719
[6900]	train-rmse:3.7461	dev-rmse:3.73663
[7000]	train-rmse:3.74491	dev-rmse:3.73605
[7100]	train-rmse:3.74374	dev-rmse:3.73551
[7200]	train-rmse:3.74262	dev-rmse:3.73504
[7300]	train-rmse:3.74152	dev-rmse:3.73461
[7400]	train-rmse:3.74042	dev-rmse:3.73418
[7500]	train-rmse:3.73933	dev-rmse:3.73374
[7600]	train-rmse:3.73826	dev-rmse:3.73332
[7700]	train-rmse:3.73721	dev-rmse:3.73289
[7800]	train-rmse:3.73618	dev-rmse:3.73246
[7900]	train-rmse:3.73517	dev-rmse:3.73205
[8000]	train-rmse:3.73416	dev-rmse:3.73164
[8100]	train-rmse:3.73317	dev-rmse:3.73124
[8200]	train-rmse:3.73219	dev-rmse:3.73086
[8300]	train-rmse:3.73121	dev-rmse:3.73047
[8400]	train-rmse:3.73024	dev-rmse:3.7301
[8500]	train-rmse:3.72929	dev-rmse:3.72972
[8600]	train-rmse:3.72836	dev-rmse:3.72938
[8700]	train-rmse:3.72744	dev-rmse:3.72904
[8800]	train-rmse:3.72655	dev-rmse:3.72871
[8900]	train-rmse:3.72565	dev-rmse:3.72837
[9000]	train-rmse:3.72477	dev-rmse:3.72806
[9100]	train-rmse:3.7239	dev-rmse:3.72776
[9200]	train-rmse:3.72303	dev-rmse:3.72744
[9300]	train-rmse:3.72219	dev-rmse:3.72719
[9400]	train-rmse:3.72135	dev-rmse:3.72691
[9500]	train-rmse:3.7205	dev-rmse:3.72664
[9600]	train-rmse:3.71969	dev-rmse:3.72639
[9700]	train-rmse:3.71887	dev-rmse:3.72615
[9800]	train-rmse:3.71807	dev-rmse:3.72591
[9900]	train-rmse:3.71727	dev-rmse:3.72567
[10000]	train-rmse:3.71647	dev-rmse:3.72541
[10100]	train-rmse:3.7157	dev-rmse:3.72514
[10200]	train-rmse:3.71494	dev-rmse:3.72491
[10300]	train-rmse:3.71419	dev-rmse:3.72468
[10400]	train-rmse:3.71341	dev-rmse:3.72445
[10500]	train-rmse:3.71265	dev-rmse:3.72426
[10600]	train-rmse:3.71188	dev-rmse:3.72406
[10700]	train-rmse:3.71112	dev-rmse:3.72387
[10800]	train-rmse:3.71036	dev-rmse:3.7237
[10900]	train-rmse:3.70962	dev-rmse:3.72353
[11000]	train-rmse:3.7089	dev-rmse:3.72337
[11100]	train-rmse:3.7082	dev-rmse:3.72321
[11200]	train-rmse:3.70751	dev-rmse:3.72303
[11300]	train-rmse:3.70684	dev-rmse:3.72286
[11400]	train-rmse:3.70617	dev-rmse:3.72276
[11500]	train-rmse:3.70551	dev-rmse:3.72264
[11600]	train-rmse:3.70484	dev-rmse:3.72251
[11700]	train-rmse:3.70419	dev-rmse:3.72238
[11800]	train-rmse:3.70354	dev-rmse:3.72222
[11900]	train-rmse:3.7029	dev-rmse:3.72211
[12000]	train-rmse:3.70227	dev-rmse:3.72199
[12100]	train-rmse:3.70163	dev-rmse:3.72184
[12200]	train-rmse:3.70101	dev-rmse:3.72168
[12300]	train-rmse:3.70041	dev-rmse:3.72155
[12400]	train-rmse:3.69982	dev-rmse:3.72144
[12500]	train-rmse:3.69923	dev-rmse:3.72133
[12600]	train-rmse:3.69864	dev-rmse:3.72121
[12700]	train-rmse:3.69805	dev-rmse:3.72108
[12800]	train-rmse:3.69745	dev-rmse:3.72096
[12900]	train-rmse:3.69686	dev-rmse:3.72084
[13000]	train-rmse:3.6963	dev-rmse:3.72074
[13100]	train-rmse:3.69574	dev-rmse:3.72064
[13200]	train-rmse:3.69519	dev-rmse:3.72052
[13300]	train-rmse:3.69466	dev-rmse:3.72041
[13400]	train-rmse:3.69411	dev-rmse:3.72032
[13500]	train-rmse:3.69357	dev-rmse:3.72022
[13600]	train-rmse:3.69303	dev-rmse:3.72015
[13700]	train-rmse:3.69248	dev-rmse:3.72005
[13800]	train-rmse:3.69194	dev-rmse:3.71998
[13900]	train-rmse:3.69139	dev-rmse:3.71988
[14000]	train-rmse:3.69086	dev-rmse:3.71979
[14100]	train-rmse:3.69032	dev-rmse:3.71969
[14200]	train-rmse:3.6898	dev-rmse:3.71957
[14300]	train-rmse:3.68927	dev-rmse:3.71946
[14400]	train-rmse:3.68875	dev-rmse:3.71935
[14500]	train-rmse:3.68823	dev-rmse:3.71923
[14600]	train-rmse:3.68769	dev-rmse:3.71913
[14700]	train-rmse:3.68714	dev-rmse:3.71902
[14800]	train-rmse:3.68661	dev-rmse:3.71892
[14900]	train-rmse:3.68607	dev-rmse:3.71881
[15000]	train-rmse:3.68555	dev-rmse:3.71867
[15100]	train-rmse:3.68504	dev-rmse:3.71857
[15200]	train-rmse:3.68453	dev-rmse:3.71847
[15300]	train-rmse:3.68404	dev-rmse:3.71836
[15400]	train-rmse:3.68354	dev-rmse:3.71826
[15500]	train-rmse:3.68303	dev-rmse:3.71816
[15600]	train-rmse:3.68253	dev-rmse:3.71806
[15700]	train-rmse:3.68203	dev-rmse:3.71796
[15800]	train-rmse:3.68154	dev-rmse:3.71786
[15900]	train-rmse:3.68106	dev-rmse:3.71778
[16000]	train-rmse:3.68058	dev-rmse:3.71769
Stopping. Best iteration:
[16030]	train-rmse:3.68044	dev-rmse:3.71766

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 3, 'learning_rate': 0.01, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.31363	dev-rmse:5.24677
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:4.09678	dev-rmse:4.04314
[200]	train-rmse:3.86862	dev-rmse:3.82918
[300]	train-rmse:3.81048	dev-rmse:3.7813
[400]	train-rmse:3.78259	dev-rmse:3.76092
[500]	train-rmse:3.76314	dev-rmse:3.74868
[600]	train-rmse:3.74756	dev-rmse:3.74062
[700]	train-rmse:3.73475	dev-rmse:3.73407
[800]	train-rmse:3.72349	dev-rmse:3.72889
[900]	train-rmse:3.71363	dev-rmse:3.725
[1000]	train-rmse:3.70464	dev-rmse:3.72207
[1100]	train-rmse:3.69683	dev-rmse:3.71987
[1200]	train-rmse:3.68952	dev-rmse:3.71803
[1300]	train-rmse:3.68294	dev-rmse:3.7163
[1400]	train-rmse:3.67639	dev-rmse:3.71522
[1500]	train-rmse:3.67046	dev-rmse:3.7144
[1600]	train-rmse:3.66435	dev-rmse:3.71332
Stopping. Best iteration:
[1592]	train-rmse:3.6648	dev-rmse:3.71321

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 3, 'learning_rate': 0.01, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.31358	dev-rmse:5.24649
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:4.09916	dev-rmse:4.04533
[200]	train-rmse:3.87358	dev-rmse:3.83086
[300]	train-rmse:3.81647	dev-rmse:3.78118
[400]	train-rmse:3.78945	dev-rmse:3.7614
[500]	train-rmse:3.7714	dev-rmse:3.75026
[600]	train-rmse:3.7571	dev-rmse:3.74239
[700]	train-rmse:3.7445	dev-rmse:3.73597
[800]	train-rmse:3.7338	dev-rmse:3.73163
[900]	train-rmse:3.72463	dev-rmse:3.72815
[1000]	train-rmse:3.71624	dev-rmse:3.72574
[1100]	train-rmse:3.70859	dev-rmse:3.72377
[1200]	train-rmse:3.70185	dev-rmse:3.72227
[1300]	train-rmse:3.69588	dev-rmse:3.72098
[1400]	train-rmse:3.69041	dev-rmse:3.72012
[1500]	train-rmse:3.6852	dev-rmse:3.71905
[1600]	train-rmse:3.68019	dev-rmse:3.71785
[1700]	train-rmse:3.67553	dev-rmse:3.71714
[1800]	train-rmse:3.67061	dev-rmse:3.71625
[1900]	train-rmse:3.66562	dev-rmse:3.71525
Stopping. Best iteration:
[1882]	train-rmse:3.66652	dev-rmse:3.71522

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 3, 'learning_rate': 0.005, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.32598	dev-rmse:5.25901
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:4.4714	dev-rmse:4.40978
[200]	train-rmse:4.09986	dev-rmse:4.04612
[300]	train-rmse:3.94142	dev-rmse:3.89539
[400]	train-rmse:3.86987	dev-rmse:3.82955
[500]	train-rmse:3.83327	dev-rmse:3.79801
[600]	train-rmse:3.81112	dev-rmse:3.78103
[700]	train-rmse:3.79558	dev-rmse:3.76956
[800]	train-rmse:3.78308	dev-rmse:3.76121
[900]	train-rmse:3.77258	dev-rmse:3.75447
[1000]	train-rmse:3.76319	dev-rmse:3.7487
[1100]	train-rmse:3.75508	dev-rmse:3.74395
[1200]	train-rmse:3.7476	dev-rmse:3.73983
[1300]	train-rmse:3.74076	dev-rmse:3.73608
[1400]	train-rmse:3.73453	dev-rmse:3.73315
[1500]	train-rmse:3.7288	dev-rmse:3.73065
[1600]	train-rmse:3.72322	dev-rmse:3.72859
[1700]	train-rmse:3.71827	dev-rmse:3.72691
[1800]	train-rmse:3.71339	dev-rmse:3.72513
[1900]	train-rmse:3.70868	dev-rmse:3.72343
[2000]	train-rmse:3.70428	dev-rmse:3.72203
[2100]	train-rmse:3.70028	dev-rmse:3.721
[2200]	train-rmse:3.69638	dev-rmse:3.72021
[2300]	train-rmse:3.69252	dev-rmse:3.71937
[2400]	train-rmse:3.68902	dev-rmse:3.71874
[2500]	train-rmse:3.68553	dev-rmse:3.7181
[2600]	train-rmse:3.68199	dev-rmse:3.71706
[2700]	train-rmse:3.67862	dev-rmse:3.71613
[2800]	train-rmse:3.67557	dev-rmse:3.71567
[2900]	train-rmse:3.67256	dev-rmse:3.71519
Stopping. Best iteration:
[2947]	train-rmse:3.67107	dev-rmse:3.71496

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 3, 'learning_rate': 0.005, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.32596	dev-rmse:5.25886
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:4.47189	dev-rmse:4.41069
[200]	train-rmse:4.10248	dev-rmse:4.04838
[300]	train-rmse:3.945	dev-rmse:3.89737
[400]	train-rmse:3.87439	dev-rmse:3.83132
[500]	train-rmse:3.83866	dev-rmse:3.79964
[600]	train-rmse:3.8168	dev-rmse:3.78114
[700]	train-rmse:3.80151	dev-rmse:3.7694
[800]	train-rmse:3.78984	dev-rmse:3.76144
[900]	train-rmse:3.78001	dev-rmse:3.75511
[1000]	train-rmse:3.77168	dev-rmse:3.75024
[1100]	train-rmse:3.76419	dev-rmse:3.74604
[1200]	train-rmse:3.75741	dev-rmse:3.74229
[1300]	train-rmse:3.75085	dev-rmse:3.73895
[1400]	train-rmse:3.74468	dev-rmse:3.73592
[1500]	train-rmse:3.73913	dev-rmse:3.73366
[1600]	train-rmse:3.73402	dev-rmse:3.73168
[1700]	train-rmse:3.72919	dev-rmse:3.72986
[1800]	train-rmse:3.72464	dev-rmse:3.72817
[1900]	train-rmse:3.72035	dev-rmse:3.72673
[2000]	train-rmse:3.71639	dev-rmse:3.72556
[2100]	train-rmse:3.71241	dev-rmse:3.72436
[2200]	train-rmse:3.70873	dev-rmse:3.7233
[2300]	train-rmse:3.70534	dev-rmse:3.72259
[2400]	train-rmse:3.70204	dev-rmse:3.72186
[2500]	train-rmse:3.699	dev-rmse:3.72117
[2600]	train-rmse:3.696	dev-rmse:3.7206
[2700]	train-rmse:3.69318	dev-rmse:3.72013
[2800]	train-rmse:3.69049	dev-rmse:3.71959
[2900]	train-rmse:3.6878	dev-rmse:3.71914
[3000]	train-rmse:3.68515	dev-rmse:3.71869
[3100]	train-rmse:3.68262	dev-rmse:3.71822
[3200]	train-rmse:3.68018	dev-rmse:3.71776
[3300]	train-rmse:3.67776	dev-rmse:3.71734
[3400]	train-rmse:3.67536	dev-rmse:3.71697
[3500]	train-rmse:3.67304	dev-rmse:3.71652
[3600]	train-rmse:3.67072	dev-rmse:3.71609
[3700]	train-rmse:3.66854	dev-rmse:3.71564
[3800]	train-rmse:3.66631	dev-rmse:3.71529
[3900]	train-rmse:3.66416	dev-rmse:3.71505
[4000]	train-rmse:3.66201	dev-rmse:3.71469
[4100]	train-rmse:3.65994	dev-rmse:3.71438
[4200]	train-rmse:3.65795	dev-rmse:3.7141
Stopping. Best iteration:
[4181]	train-rmse:3.6583	dev-rmse:3.71408

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 3, 'learning_rate': 0.1, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.09723	dev-rmse:5.03257
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:3.70555	dev-rmse:3.72303
[200]	train-rmse:3.64535	dev-rmse:3.71358
Stopping. Best iteration:
[259]	train-rmse:3.6161	dev-rmse:3.70975

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 3, 'learning_rate': 0.1, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.09674	dev-rmse:5.02978
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:3.7121	dev-rmse:3.72552
[200]	train-rmse:3.6571	dev-rmse:3.71508
Stopping. Best iteration:
[200]	train-rmse:3.6571	dev-rmse:3.71508

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 5, 'learning_rate': 0.001, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.3358	dev-rmse:5.26878
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.09742	dev-rmse:5.03567
[200]	train-rmse:4.89224	dev-rmse:4.83618
[300]	train-rmse:4.71657	dev-rmse:4.66614
[400]	train-rmse:4.56637	dev-rmse:4.52159
[500]	train-rmse:4.43809	dev-rmse:4.39931
[600]	train-rmse:4.32923	dev-rmse:4.29609
[700]	train-rmse:4.23648	dev-rmse:4.2089
[800]	train-rmse:4.15754	dev-rmse:4.13564
[900]	train-rmse:4.09045	dev-rmse:4.07411
[1000]	train-rmse:4.03325	dev-rmse:4.02268
[1100]	train-rmse:3.98461	dev-rmse:3.97927
[1200]	train-rmse:3.94295	dev-rmse:3.94291
[1300]	train-rmse:3.9072	dev-rmse:3.91212
[1400]	train-rmse:3.87634	dev-rmse:3.88627
[1500]	train-rmse:3.84972	dev-rmse:3.86419
[1600]	train-rmse:3.82641	dev-rmse:3.84584
[1700]	train-rmse:3.80621	dev-rmse:3.8302
[1800]	train-rmse:3.78854	dev-rmse:3.8168
[1900]	train-rmse:3.77294	dev-rmse:3.80542
[2000]	train-rmse:3.75895	dev-rmse:3.79564
[2100]	train-rmse:3.74666	dev-rmse:3.78716
[2200]	train-rmse:3.73548	dev-rmse:3.77993
[2300]	train-rmse:3.72519	dev-rmse:3.77339
[2400]	train-rmse:3.71585	dev-rmse:3.76797
[2500]	train-rmse:3.70731	dev-rmse:3.76321
[2600]	train-rmse:3.6994	dev-rmse:3.75885
[2700]	train-rmse:3.69217	dev-rmse:3.75516
[2800]	train-rmse:3.68531	dev-rmse:3.75188
[2900]	train-rmse:3.6787	dev-rmse:3.74893
[3000]	train-rmse:3.67257	dev-rmse:3.74611
[3100]	train-rmse:3.66676	dev-rmse:3.74364
[3200]	train-rmse:3.66118	dev-rmse:3.74145
[3300]	train-rmse:3.65601	dev-rmse:3.73947
[3400]	train-rmse:3.65107	dev-rmse:3.73765
[3500]	train-rmse:3.64639	dev-rmse:3.73583
[3600]	train-rmse:3.6418	dev-rmse:3.7342
[3700]	train-rmse:3.63719	dev-rmse:3.73268
[3800]	train-rmse:3.6329	dev-rmse:3.73129
[3900]	train-rmse:3.62868	dev-rmse:3.72994
[4000]	train-rmse:3.62468	dev-rmse:3.72878
[4100]	train-rmse:3.6208	dev-rmse:3.7277
[4200]	train-rmse:3.617	dev-rmse:3.72667
[4300]	train-rmse:3.61331	dev-rmse:3.72567
[4400]	train-rmse:3.60972	dev-rmse:3.72473
[4500]	train-rmse:3.60613	dev-rmse:3.72388
[4600]	train-rmse:3.60259	dev-rmse:3.72303
[4700]	train-rmse:3.59902	dev-rmse:3.72215
[4800]	train-rmse:3.59556	dev-rmse:3.7215
[4900]	train-rmse:3.59227	dev-rmse:3.72094
[5000]	train-rmse:3.589	dev-rmse:3.72038
[5100]	train-rmse:3.58579	dev-rmse:3.71981
[5200]	train-rmse:3.58263	dev-rmse:3.71925
[5300]	train-rmse:3.57963	dev-rmse:3.7188
[5400]	train-rmse:3.57652	dev-rmse:3.71832
[5500]	train-rmse:3.57337	dev-rmse:3.71778
[5600]	train-rmse:3.57043	dev-rmse:3.71735
[5700]	train-rmse:3.56754	dev-rmse:3.71691
[5800]	train-rmse:3.5646	dev-rmse:3.71654
[5900]	train-rmse:3.56183	dev-rmse:3.71617
[6000]	train-rmse:3.55922	dev-rmse:3.71586
[6100]	train-rmse:3.55657	dev-rmse:3.71556
[6200]	train-rmse:3.55383	dev-rmse:3.71522
[6300]	train-rmse:3.55122	dev-rmse:3.71489
[6400]	train-rmse:3.54856	dev-rmse:3.71463
[6500]	train-rmse:3.54589	dev-rmse:3.71437
[6600]	train-rmse:3.54338	dev-rmse:3.7142
[6700]	train-rmse:3.54079	dev-rmse:3.71391
[6800]	train-rmse:3.53827	dev-rmse:3.71369
Stopping. Best iteration:
[6851]	train-rmse:3.53694	dev-rmse:3.71356

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 5, 'learning_rate': 0.001, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33579	dev-rmse:5.26875
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.09726	dev-rmse:5.03592
[200]	train-rmse:4.89269	dev-rmse:4.83668
[300]	train-rmse:4.71709	dev-rmse:4.66778
[400]	train-rmse:4.56724	dev-rmse:4.52451
[500]	train-rmse:4.43967	dev-rmse:4.4033
[600]	train-rmse:4.33124	dev-rmse:4.30019
[700]	train-rmse:4.23915	dev-rmse:4.21278
[800]	train-rmse:4.16108	dev-rmse:4.13935
[900]	train-rmse:4.0946	dev-rmse:4.07783
[1000]	train-rmse:4.03791	dev-rmse:4.02641
[1100]	train-rmse:3.98962	dev-rmse:3.98338
[1200]	train-rmse:3.94844	dev-rmse:3.94756
[1300]	train-rmse:3.91301	dev-rmse:3.91718
[1400]	train-rmse:3.88244	dev-rmse:3.89173
[1500]	train-rmse:3.85604	dev-rmse:3.87017
[1600]	train-rmse:3.83361	dev-rmse:3.85189
[1700]	train-rmse:3.81392	dev-rmse:3.83619
[1800]	train-rmse:3.79697	dev-rmse:3.82311
[1900]	train-rmse:3.78191	dev-rmse:3.81164
[2000]	train-rmse:3.76834	dev-rmse:3.8015
[2100]	train-rmse:3.7562	dev-rmse:3.79279
[2200]	train-rmse:3.74543	dev-rmse:3.78549
[2300]	train-rmse:3.7356	dev-rmse:3.77965
[2400]	train-rmse:3.72675	dev-rmse:3.7745
[2500]	train-rmse:3.71869	dev-rmse:3.76974
[2600]	train-rmse:3.71119	dev-rmse:3.76558
[2700]	train-rmse:3.7043	dev-rmse:3.76202
[2800]	train-rmse:3.69798	dev-rmse:3.75909
[2900]	train-rmse:3.69188	dev-rmse:3.75621
[3000]	train-rmse:3.68611	dev-rmse:3.75352
[3100]	train-rmse:3.68069	dev-rmse:3.75106
[3200]	train-rmse:3.67548	dev-rmse:3.7488
[3300]	train-rmse:3.67028	dev-rmse:3.74691
[3400]	train-rmse:3.66549	dev-rmse:3.74519
[3500]	train-rmse:3.66096	dev-rmse:3.74372
[3600]	train-rmse:3.65671	dev-rmse:3.74239
[3700]	train-rmse:3.65263	dev-rmse:3.74125
[3800]	train-rmse:3.64849	dev-rmse:3.74025
[3900]	train-rmse:3.64453	dev-rmse:3.73908
[4000]	train-rmse:3.64061	dev-rmse:3.73805
[4100]	train-rmse:3.63688	dev-rmse:3.73721
[4200]	train-rmse:3.63325	dev-rmse:3.73634
[4300]	train-rmse:3.62976	dev-rmse:3.73557
[4400]	train-rmse:3.62618	dev-rmse:3.7348
[4500]	train-rmse:3.62275	dev-rmse:3.73404
[4600]	train-rmse:3.61936	dev-rmse:3.73337
[4700]	train-rmse:3.61604	dev-rmse:3.73273
[4800]	train-rmse:3.61282	dev-rmse:3.73211
[4900]	train-rmse:3.60975	dev-rmse:3.73158
[5000]	train-rmse:3.60675	dev-rmse:3.73115
[5100]	train-rmse:3.60383	dev-rmse:3.73074
[5200]	train-rmse:3.601	dev-rmse:3.73038
[5300]	train-rmse:3.59829	dev-rmse:3.73004
[5400]	train-rmse:3.59576	dev-rmse:3.72964
[5500]	train-rmse:3.5933	dev-rmse:3.72929
[5600]	train-rmse:3.59082	dev-rmse:3.729
[5700]	train-rmse:3.58844	dev-rmse:3.7287
[5800]	train-rmse:3.5858	dev-rmse:3.72837
[5900]	train-rmse:3.58325	dev-rmse:3.72816
[6000]	train-rmse:3.58063	dev-rmse:3.72802
[6100]	train-rmse:3.57824	dev-rmse:3.72782
[6200]	train-rmse:3.57578	dev-rmse:3.72768
Stopping. Best iteration:
[6238]	train-rmse:3.57478	dev-rmse:3.72765

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 5, 'learning_rate': 0.01, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.31278	dev-rmse:5.24644
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:4.02702	dev-rmse:4.01584
[200]	train-rmse:3.75748	dev-rmse:3.79499
[300]	train-rmse:3.67282	dev-rmse:3.7476
[400]	train-rmse:3.6262	dev-rmse:3.7295
[500]	train-rmse:3.59195	dev-rmse:3.72116
[600]	train-rmse:3.56173	dev-rmse:3.71694
[700]	train-rmse:3.53592	dev-rmse:3.71457
Stopping. Best iteration:
[749]	train-rmse:3.52388	dev-rmse:3.71375

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 5, 'learning_rate': 0.01, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.31267	dev-rmse:5.24611
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:4.03115	dev-rmse:4.02036
[200]	train-rmse:3.76597	dev-rmse:3.79921
[300]	train-rmse:3.6842	dev-rmse:3.75177
[400]	train-rmse:3.63876	dev-rmse:3.73536
[500]	train-rmse:3.6053	dev-rmse:3.72888
[600]	train-rmse:3.57935	dev-rmse:3.72562
[700]	train-rmse:3.55591	dev-rmse:3.72375
[800]	train-rmse:3.53628	dev-rmse:3.72216
Stopping. Best iteration:
[793]	train-rmse:3.53751	dev-rmse:3.72202

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 5, 'learning_rate': 0.005, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.32556	dev-rmse:5.25884
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:4.4327	dev-rmse:4.39331
[200]	train-rmse:4.03062	dev-rmse:4.02001
[300]	train-rmse:3.84906	dev-rmse:3.8649
[400]	train-rmse:3.76008	dev-rmse:3.79615
[500]	train-rmse:3.70902	dev-rmse:3.76436
[600]	train-rmse:3.67453	dev-rmse:3.74787
[700]	train-rmse:3.64924	dev-rmse:3.73826
[800]	train-rmse:3.62747	dev-rmse:3.73166
[900]	train-rmse:3.6087	dev-rmse:3.72694
[1000]	train-rmse:3.59103	dev-rmse:3.72314
[1100]	train-rmse:3.5755	dev-rmse:3.72095
[1200]	train-rmse:3.56129	dev-rmse:3.71861
[1300]	train-rmse:3.54763	dev-rmse:3.71682
[1400]	train-rmse:3.53532	dev-rmse:3.71569
[1500]	train-rmse:3.52386	dev-rmse:3.71445
[1600]	train-rmse:3.51182	dev-rmse:3.71375
Stopping. Best iteration:
[1628]	train-rmse:3.50901	dev-rmse:3.71364

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 5, 'learning_rate': 0.005, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.3255	dev-rmse:5.25868
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:4.43381	dev-rmse:4.39783
[200]	train-rmse:4.0349	dev-rmse:4.02358
[300]	train-rmse:3.85449	dev-rmse:3.86861
[400]	train-rmse:3.76723	dev-rmse:3.80041
[500]	train-rmse:3.7182	dev-rmse:3.76835
[600]	train-rmse:3.686	dev-rmse:3.75192
[700]	train-rmse:3.65982	dev-rmse:3.74187
[800]	train-rmse:3.63955	dev-rmse:3.73578
[900]	train-rmse:3.62179	dev-rmse:3.73192
[1000]	train-rmse:3.60567	dev-rmse:3.72878
[1100]	train-rmse:3.59205	dev-rmse:3.72679
[1200]	train-rmse:3.57917	dev-rmse:3.72533
[1300]	train-rmse:3.56776	dev-rmse:3.7244
[1400]	train-rmse:3.55754	dev-rmse:3.72327
[1500]	train-rmse:3.54727	dev-rmse:3.72265
[1600]	train-rmse:3.53709	dev-rmse:3.72168
Stopping. Best iteration:
[1639]	train-rmse:3.53313	dev-rmse:3.72137

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 5, 'learning_rate': 0.1, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.08873	dev-rmse:5.02947
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:3.47546	dev-rmse:3.72307
Stopping. Best iteration:
[87]	train-rmse:3.50508	dev-rmse:3.72185

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 5, 'learning_rate': 0.1, 'objective': 'reg:linear', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.0877	dev-rmse:5.02609
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:3.49102	dev-rmse:3.7281
Stopping. Best iteration:
[94]	train-rmse:3.50453	dev-rmse:3.72699

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 2, 'learning_rate': 0.001, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33814	dev-rmse:5.27104
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.31433	dev-rmse:5.24698
[200]	train-rmse:5.28895	dev-rmse:5.22135
[300]	train-rmse:5.26192	dev-rmse:5.19403
[400]	train-rmse:5.23315	dev-rmse:5.16495
[500]	train-rmse:5.20254	dev-rmse:5.13401
[600]	train-rmse:5.17	dev-rmse:5.10113
[700]	train-rmse:5.13545	dev-rmse:5.0662
[800]	train-rmse:5.0988	dev-rmse:5.02914
[900]	train-rmse:5.05996	dev-rmse:4.98988
[1000]	train-rmse:5.01887	dev-rmse:4.94832
[1100]	train-rmse:4.97552	dev-rmse:4.90446
[1200]	train-rmse:4.92987	dev-rmse:4.85825
[1300]	train-rmse:4.88187	dev-rmse:4.80965
[1400]	train-rmse:4.83154	dev-rmse:4.75867
[1500]	train-rmse:4.77927	dev-rmse:4.70571
[1600]	train-rmse:4.72526	dev-rmse:4.65099
[1700]	train-rmse:4.66977	dev-rmse:4.59484
[1800]	train-rmse:4.61455	dev-rmse:4.53906
[1900]	train-rmse:4.56105	dev-rmse:4.48505
[2000]	train-rmse:4.5101	dev-rmse:4.43367
[2100]	train-rmse:4.4632	dev-rmse:4.38647
[2200]	train-rmse:4.42007	dev-rmse:4.34296
[2300]	train-rmse:4.37965	dev-rmse:4.30213
[2400]	train-rmse:4.3415	dev-rmse:4.26357
[2500]	train-rmse:4.30545	dev-rmse:4.2272
[2600]	train-rmse:4.27155	dev-rmse:4.19311
[2700]	train-rmse:4.23978	dev-rmse:4.16126
[2800]	train-rmse:4.21045	dev-rmse:4.13209
[2900]	train-rmse:4.18333	dev-rmse:4.10533
[3000]	train-rmse:4.15816	dev-rmse:4.08061
[3100]	train-rmse:4.13496	dev-rmse:4.05803
[3200]	train-rmse:4.11374	dev-rmse:4.03745
[3300]	train-rmse:4.09431	dev-rmse:4.01852
[3400]	train-rmse:4.07643	dev-rmse:4.00129
[3500]	train-rmse:4.05995	dev-rmse:3.98533
[3600]	train-rmse:4.04466	dev-rmse:3.97066
[3700]	train-rmse:4.03059	dev-rmse:3.95715
[3800]	train-rmse:4.01758	dev-rmse:3.94472
[3900]	train-rmse:4.00549	dev-rmse:3.93319
[4000]	train-rmse:3.99438	dev-rmse:3.92266
[4100]	train-rmse:3.98405	dev-rmse:3.91287
[4200]	train-rmse:3.97447	dev-rmse:3.90384
[4300]	train-rmse:3.9656	dev-rmse:3.89549
[4400]	train-rmse:3.95739	dev-rmse:3.88777
[4500]	train-rmse:3.94979	dev-rmse:3.8807
[4600]	train-rmse:3.94269	dev-rmse:3.87414
[4700]	train-rmse:3.93608	dev-rmse:3.86802
[4800]	train-rmse:3.92995	dev-rmse:3.86235
[4900]	train-rmse:3.92422	dev-rmse:3.85709
[5000]	train-rmse:3.91887	dev-rmse:3.85218
[5100]	train-rmse:3.91382	dev-rmse:3.84759
[5200]	train-rmse:3.90912	dev-rmse:3.84327
[5300]	train-rmse:3.90476	dev-rmse:3.83932
[5400]	train-rmse:3.90066	dev-rmse:3.83562
[5500]	train-rmse:3.89678	dev-rmse:3.83218
[5600]	train-rmse:3.89315	dev-rmse:3.82897
[5700]	train-rmse:3.88974	dev-rmse:3.82599
[5800]	train-rmse:3.88653	dev-rmse:3.82314
[5900]	train-rmse:3.88348	dev-rmse:3.8205
[6000]	train-rmse:3.88058	dev-rmse:3.81798
[6100]	train-rmse:3.87785	dev-rmse:3.81561
[6200]	train-rmse:3.87527	dev-rmse:3.81344
[6300]	train-rmse:3.87283	dev-rmse:3.81135
[6400]	train-rmse:3.87045	dev-rmse:3.80936
[6500]	train-rmse:3.8682	dev-rmse:3.80752
[6600]	train-rmse:3.86606	dev-rmse:3.80572
[6700]	train-rmse:3.86403	dev-rmse:3.80403
[6800]	train-rmse:3.86207	dev-rmse:3.8024
[6900]	train-rmse:3.86022	dev-rmse:3.8009
[7000]	train-rmse:3.85845	dev-rmse:3.79944
[7100]	train-rmse:3.8567	dev-rmse:3.798
[7200]	train-rmse:3.85507	dev-rmse:3.79666
[7300]	train-rmse:3.85346	dev-rmse:3.79533
[7400]	train-rmse:3.85194	dev-rmse:3.79405
[7500]	train-rmse:3.85042	dev-rmse:3.79283
[7600]	train-rmse:3.84895	dev-rmse:3.79171
[7700]	train-rmse:3.84756	dev-rmse:3.79056
[7800]	train-rmse:3.84617	dev-rmse:3.78944
[7900]	train-rmse:3.84484	dev-rmse:3.7884
[8000]	train-rmse:3.84353	dev-rmse:3.7874
[8100]	train-rmse:3.84226	dev-rmse:3.78644
[8200]	train-rmse:3.84104	dev-rmse:3.78548
[8300]	train-rmse:3.83988	dev-rmse:3.78462
[8400]	train-rmse:3.83873	dev-rmse:3.78368
[8500]	train-rmse:3.83759	dev-rmse:3.78282
[8600]	train-rmse:3.8365	dev-rmse:3.78201
[8700]	train-rmse:3.8354	dev-rmse:3.78118
[8800]	train-rmse:3.83434	dev-rmse:3.7804
[8900]	train-rmse:3.83331	dev-rmse:3.77956
[9000]	train-rmse:3.83231	dev-rmse:3.77885
[9100]	train-rmse:3.83134	dev-rmse:3.77811
[9200]	train-rmse:3.83037	dev-rmse:3.77738
[9300]	train-rmse:3.82942	dev-rmse:3.77663
[9400]	train-rmse:3.8285	dev-rmse:3.77593
[9500]	train-rmse:3.82758	dev-rmse:3.7753
[9600]	train-rmse:3.82668	dev-rmse:3.77465
[9700]	train-rmse:3.82579	dev-rmse:3.77399
[9800]	train-rmse:3.8249	dev-rmse:3.77331
[9900]	train-rmse:3.82405	dev-rmse:3.77267
[10000]	train-rmse:3.82324	dev-rmse:3.77207
[10100]	train-rmse:3.82239	dev-rmse:3.77144
[10200]	train-rmse:3.82158	dev-rmse:3.77088
[10300]	train-rmse:3.82079	dev-rmse:3.7703
[10400]	train-rmse:3.81999	dev-rmse:3.76974
[10500]	train-rmse:3.81921	dev-rmse:3.76914
[10600]	train-rmse:3.81846	dev-rmse:3.76858
[10700]	train-rmse:3.81771	dev-rmse:3.76804
[10800]	train-rmse:3.81693	dev-rmse:3.76749
[10900]	train-rmse:3.81621	dev-rmse:3.76701
[11000]	train-rmse:3.81548	dev-rmse:3.7665
[11100]	train-rmse:3.81478	dev-rmse:3.76596
[11200]	train-rmse:3.81407	dev-rmse:3.76546
[11300]	train-rmse:3.81337	dev-rmse:3.76498
[11400]	train-rmse:3.81267	dev-rmse:3.76452
[11500]	train-rmse:3.812	dev-rmse:3.76402
[11600]	train-rmse:3.81131	dev-rmse:3.76353
[11700]	train-rmse:3.81065	dev-rmse:3.76303
[11800]	train-rmse:3.80998	dev-rmse:3.76257
[11900]	train-rmse:3.80933	dev-rmse:3.76215
[12000]	train-rmse:3.8087	dev-rmse:3.76167
[12100]	train-rmse:3.80805	dev-rmse:3.76125
[12200]	train-rmse:3.80741	dev-rmse:3.76081
[12300]	train-rmse:3.80676	dev-rmse:3.76032
[12400]	train-rmse:3.80613	dev-rmse:3.75992
[12500]	train-rmse:3.80553	dev-rmse:3.75952
[12600]	train-rmse:3.80491	dev-rmse:3.75907
[12700]	train-rmse:3.80431	dev-rmse:3.75865
[12800]	train-rmse:3.80369	dev-rmse:3.75819
[12900]	train-rmse:3.8031	dev-rmse:3.75778
[13000]	train-rmse:3.8025	dev-rmse:3.75738
[13100]	train-rmse:3.80194	dev-rmse:3.75697
[13200]	train-rmse:3.80137	dev-rmse:3.75659
[13300]	train-rmse:3.8008	dev-rmse:3.75616
[13400]	train-rmse:3.80025	dev-rmse:3.75578
[13500]	train-rmse:3.79969	dev-rmse:3.75541
[13600]	train-rmse:3.79915	dev-rmse:3.75501
[13700]	train-rmse:3.7986	dev-rmse:3.75464
[13800]	train-rmse:3.79805	dev-rmse:3.7543
[13900]	train-rmse:3.79751	dev-rmse:3.75392
[14000]	train-rmse:3.79694	dev-rmse:3.75354
[14100]	train-rmse:3.79641	dev-rmse:3.75317
[14200]	train-rmse:3.7959	dev-rmse:3.75283
[14300]	train-rmse:3.79539	dev-rmse:3.75248
[14400]	train-rmse:3.79489	dev-rmse:3.75212
[14500]	train-rmse:3.79439	dev-rmse:3.75181
[14600]	train-rmse:3.79387	dev-rmse:3.75147
[14700]	train-rmse:3.79337	dev-rmse:3.75111
[14800]	train-rmse:3.79286	dev-rmse:3.75075
[14900]	train-rmse:3.79237	dev-rmse:3.75039
[15000]	train-rmse:3.79189	dev-rmse:3.75006
[15100]	train-rmse:3.7914	dev-rmse:3.7497
[15200]	train-rmse:3.79092	dev-rmse:3.74935
[15300]	train-rmse:3.79042	dev-rmse:3.74903
[15400]	train-rmse:3.78993	dev-rmse:3.74872
[15500]	train-rmse:3.78946	dev-rmse:3.7484
[15600]	train-rmse:3.78898	dev-rmse:3.74809
[15700]	train-rmse:3.7885	dev-rmse:3.7478
[15800]	train-rmse:3.78803	dev-rmse:3.7475
[15900]	train-rmse:3.78755	dev-rmse:3.74717
[16000]	train-rmse:3.78709	dev-rmse:3.74684
[16100]	train-rmse:3.78663	dev-rmse:3.74652
[16200]	train-rmse:3.78617	dev-rmse:3.74623
[16300]	train-rmse:3.78572	dev-rmse:3.74592
[16400]	train-rmse:3.78527	dev-rmse:3.74561
[16500]	train-rmse:3.78478	dev-rmse:3.74528
[16600]	train-rmse:3.78434	dev-rmse:3.745
[16700]	train-rmse:3.78391	dev-rmse:3.74472
[16800]	train-rmse:3.78345	dev-rmse:3.7444
[16900]	train-rmse:3.78301	dev-rmse:3.7441
[17000]	train-rmse:3.78257	dev-rmse:3.7438
[17100]	train-rmse:3.78217	dev-rmse:3.74357
[17200]	train-rmse:3.78176	dev-rmse:3.74329
[17300]	train-rmse:3.78133	dev-rmse:3.74303
[17400]	train-rmse:3.78091	dev-rmse:3.74277
[17500]	train-rmse:3.78049	dev-rmse:3.74249
[17600]	train-rmse:3.78009	dev-rmse:3.74223
[17700]	train-rmse:3.77967	dev-rmse:3.74197
[17800]	train-rmse:3.77926	dev-rmse:3.74172
[17900]	train-rmse:3.77886	dev-rmse:3.74145
[18000]	train-rmse:3.77844	dev-rmse:3.7412
[18100]	train-rmse:3.77805	dev-rmse:3.74095
[18200]	train-rmse:3.77764	dev-rmse:3.7407
[18300]	train-rmse:3.77723	dev-rmse:3.74043
[18400]	train-rmse:3.77686	dev-rmse:3.74019
[18500]	train-rmse:3.77648	dev-rmse:3.73996
[18600]	train-rmse:3.7761	dev-rmse:3.7397
[18700]	train-rmse:3.77572	dev-rmse:3.73946
[18800]	train-rmse:3.77532	dev-rmse:3.73922
[18900]	train-rmse:3.77496	dev-rmse:3.739
[19000]	train-rmse:3.77458	dev-rmse:3.73875
[19100]	train-rmse:3.7742	dev-rmse:3.73853
[19200]	train-rmse:3.77382	dev-rmse:3.7383
[19300]	train-rmse:3.77344	dev-rmse:3.73807
[19400]	train-rmse:3.77307	dev-rmse:3.73786
[19500]	train-rmse:3.77269	dev-rmse:3.73762
[19600]	train-rmse:3.77232	dev-rmse:3.73738
[19700]	train-rmse:3.77196	dev-rmse:3.73716
[19800]	train-rmse:3.77158	dev-rmse:3.73696
[19900]	train-rmse:3.77122	dev-rmse:3.73673
[20000]	train-rmse:3.77086	dev-rmse:3.73651
[20100]	train-rmse:3.7705	dev-rmse:3.7363
[20200]	train-rmse:3.77016	dev-rmse:3.73609
[20300]	train-rmse:3.7698	dev-rmse:3.73588
[20400]	train-rmse:3.76946	dev-rmse:3.73567
[20500]	train-rmse:3.76911	dev-rmse:3.73547
[20600]	train-rmse:3.76879	dev-rmse:3.73527
[20700]	train-rmse:3.76845	dev-rmse:3.73508
[20800]	train-rmse:3.76813	dev-rmse:3.73491
[20900]	train-rmse:3.7678	dev-rmse:3.73469
[21000]	train-rmse:3.76746	dev-rmse:3.73448
[21100]	train-rmse:3.76713	dev-rmse:3.7343
[21200]	train-rmse:3.76681	dev-rmse:3.73407
[21300]	train-rmse:3.76649	dev-rmse:3.73387
[21400]	train-rmse:3.76616	dev-rmse:3.73368
[21500]	train-rmse:3.76582	dev-rmse:3.73348
[21600]	train-rmse:3.76549	dev-rmse:3.73329
[21700]	train-rmse:3.76517	dev-rmse:3.73312
[21800]	train-rmse:3.76485	dev-rmse:3.73292
[21900]	train-rmse:3.76452	dev-rmse:3.73273
[22000]	train-rmse:3.76422	dev-rmse:3.73259
[22100]	train-rmse:3.7639	dev-rmse:3.73242
[22200]	train-rmse:3.76361	dev-rmse:3.73226
[22300]	train-rmse:3.7633	dev-rmse:3.73208
[22400]	train-rmse:3.76298	dev-rmse:3.7319
[22500]	train-rmse:3.76266	dev-rmse:3.73171
[22600]	train-rmse:3.76234	dev-rmse:3.73155
[22700]	train-rmse:3.76202	dev-rmse:3.73136
[22800]	train-rmse:3.76171	dev-rmse:3.73119
[22900]	train-rmse:3.76141	dev-rmse:3.73102
[23000]	train-rmse:3.76112	dev-rmse:3.73086
[23100]	train-rmse:3.76083	dev-rmse:3.73068
[23200]	train-rmse:3.76053	dev-rmse:3.73052
[23300]	train-rmse:3.76023	dev-rmse:3.73036
[23400]	train-rmse:3.75992	dev-rmse:3.7302
[23500]	train-rmse:3.75963	dev-rmse:3.73006
[23600]	train-rmse:3.75932	dev-rmse:3.72991
[23700]	train-rmse:3.75902	dev-rmse:3.72976
[23800]	train-rmse:3.75872	dev-rmse:3.7296
[23900]	train-rmse:3.75842	dev-rmse:3.72943
[24000]	train-rmse:3.75815	dev-rmse:3.72929
[24100]	train-rmse:3.75787	dev-rmse:3.72914
[24200]	train-rmse:3.75757	dev-rmse:3.72899
[24300]	train-rmse:3.75729	dev-rmse:3.72883
[24400]	train-rmse:3.75701	dev-rmse:3.72868
[24500]	train-rmse:3.75673	dev-rmse:3.72852
[24600]	train-rmse:3.75645	dev-rmse:3.72838
[24700]	train-rmse:3.75617	dev-rmse:3.72824
[24800]	train-rmse:3.7559	dev-rmse:3.72811
[24900]	train-rmse:3.75562	dev-rmse:3.72797
[25000]	train-rmse:3.75534	dev-rmse:3.72782
[25100]	train-rmse:3.75509	dev-rmse:3.72771
[25200]	train-rmse:3.75481	dev-rmse:3.72758
[25300]	train-rmse:3.75454	dev-rmse:3.72744
[25400]	train-rmse:3.75427	dev-rmse:3.7273
[25500]	train-rmse:3.75401	dev-rmse:3.72718
[25600]	train-rmse:3.75376	dev-rmse:3.72706
[25700]	train-rmse:3.75349	dev-rmse:3.7269
[25800]	train-rmse:3.75322	dev-rmse:3.72676
[25900]	train-rmse:3.75296	dev-rmse:3.72664
[26000]	train-rmse:3.7527	dev-rmse:3.72652
[26100]	train-rmse:3.75244	dev-rmse:3.72638
[26200]	train-rmse:3.75217	dev-rmse:3.72629
[26300]	train-rmse:3.75192	dev-rmse:3.72619
[26400]	train-rmse:3.75168	dev-rmse:3.72607
[26500]	train-rmse:3.75142	dev-rmse:3.72595
[26600]	train-rmse:3.75116	dev-rmse:3.72582
[26700]	train-rmse:3.75091	dev-rmse:3.72568
[26800]	train-rmse:3.75066	dev-rmse:3.72557
[26900]	train-rmse:3.75042	dev-rmse:3.72545
[27000]	train-rmse:3.75017	dev-rmse:3.72535
[27100]	train-rmse:3.74993	dev-rmse:3.72523
[27200]	train-rmse:3.74968	dev-rmse:3.72514
[27300]	train-rmse:3.74943	dev-rmse:3.72502
[27400]	train-rmse:3.74918	dev-rmse:3.72492
[27500]	train-rmse:3.74893	dev-rmse:3.7248
[27600]	train-rmse:3.74869	dev-rmse:3.72466
[27700]	train-rmse:3.74844	dev-rmse:3.72454
[27800]	train-rmse:3.74823	dev-rmse:3.72444
[27900]	train-rmse:3.748	dev-rmse:3.72435
[28000]	train-rmse:3.74777	dev-rmse:3.72426
[28100]	train-rmse:3.74754	dev-rmse:3.72415
[28200]	train-rmse:3.74731	dev-rmse:3.72404
[28300]	train-rmse:3.74709	dev-rmse:3.72394
[28400]	train-rmse:3.74685	dev-rmse:3.72384
[28500]	train-rmse:3.74662	dev-rmse:3.72374
[28600]	train-rmse:3.7464	dev-rmse:3.72364
[28700]	train-rmse:3.74618	dev-rmse:3.72356
[28800]	train-rmse:3.74595	dev-rmse:3.72346
[28900]	train-rmse:3.74572	dev-rmse:3.72337
[29000]	train-rmse:3.7455	dev-rmse:3.72325
[29100]	train-rmse:3.74527	dev-rmse:3.72316
[29200]	train-rmse:3.74505	dev-rmse:3.72307
[29300]	train-rmse:3.74482	dev-rmse:3.72296
[29400]	train-rmse:3.74459	dev-rmse:3.72289
[29500]	train-rmse:3.74438	dev-rmse:3.7228
[29600]	train-rmse:3.74416	dev-rmse:3.72272
[29700]	train-rmse:3.74394	dev-rmse:3.72261
[29800]	train-rmse:3.74372	dev-rmse:3.72253
[29900]	train-rmse:3.74349	dev-rmse:3.72244
[30000]	train-rmse:3.74328	dev-rmse:3.72234
[30100]	train-rmse:3.74307	dev-rmse:3.72226
[30200]	train-rmse:3.74286	dev-rmse:3.72218
[30300]	train-rmse:3.74265	dev-rmse:3.72211
[30400]	train-rmse:3.74244	dev-rmse:3.72203
[30500]	train-rmse:3.74223	dev-rmse:3.72194
[30600]	train-rmse:3.74201	dev-rmse:3.72186
[30700]	train-rmse:3.7418	dev-rmse:3.72177
[30800]	train-rmse:3.74159	dev-rmse:3.72168
[30900]	train-rmse:3.74138	dev-rmse:3.72161
[31000]	train-rmse:3.74117	dev-rmse:3.72153
[31100]	train-rmse:3.74096	dev-rmse:3.72146
[31200]	train-rmse:3.74075	dev-rmse:3.72137
[31300]	train-rmse:3.74055	dev-rmse:3.7213
[31400]	train-rmse:3.74034	dev-rmse:3.72121
[31500]	train-rmse:3.74014	dev-rmse:3.72113
[31600]	train-rmse:3.73993	dev-rmse:3.72106
[31700]	train-rmse:3.73972	dev-rmse:3.72097
[31800]	train-rmse:3.7395	dev-rmse:3.72088
Stopping. Best iteration:
[31821]	train-rmse:3.73946	dev-rmse:3.72086

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 2, 'learning_rate': 0.001, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33814	dev-rmse:5.27104
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.31433	dev-rmse:5.24698
[200]	train-rmse:5.28895	dev-rmse:5.22134
[300]	train-rmse:5.26192	dev-rmse:5.19403
[400]	train-rmse:5.23315	dev-rmse:5.16495
[500]	train-rmse:5.20254	dev-rmse:5.134
[600]	train-rmse:5.17	dev-rmse:5.10111
[700]	train-rmse:5.13545	dev-rmse:5.06618
[800]	train-rmse:5.09879	dev-rmse:5.02912
[900]	train-rmse:5.05996	dev-rmse:4.98985
[1000]	train-rmse:5.01887	dev-rmse:4.94829
[1100]	train-rmse:4.97552	dev-rmse:4.90444
[1200]	train-rmse:4.92987	dev-rmse:4.85824
[1300]	train-rmse:4.88186	dev-rmse:4.80962
[1400]	train-rmse:4.83151	dev-rmse:4.75863
[1500]	train-rmse:4.77929	dev-rmse:4.70571
[1600]	train-rmse:4.72529	dev-rmse:4.651
[1700]	train-rmse:4.66989	dev-rmse:4.59496
[1800]	train-rmse:4.61487	dev-rmse:4.53939
[1900]	train-rmse:4.56166	dev-rmse:4.48569
[2000]	train-rmse:4.51075	dev-rmse:4.43433
[2100]	train-rmse:4.46381	dev-rmse:4.38718
[2200]	train-rmse:4.42103	dev-rmse:4.34399
[2300]	train-rmse:4.38067	dev-rmse:4.30316
[2400]	train-rmse:4.34245	dev-rmse:4.26454
[2500]	train-rmse:4.30633	dev-rmse:4.22818
[2600]	train-rmse:4.27243	dev-rmse:4.1941
[2700]	train-rmse:4.24058	dev-rmse:4.16215
[2800]	train-rmse:4.21134	dev-rmse:4.13302
[2900]	train-rmse:4.18433	dev-rmse:4.10654
[3000]	train-rmse:4.15909	dev-rmse:4.08189
[3100]	train-rmse:4.13561	dev-rmse:4.05904
[3200]	train-rmse:4.11448	dev-rmse:4.03834
[3300]	train-rmse:4.09518	dev-rmse:4.01976
[3400]	train-rmse:4.07744	dev-rmse:4.00268
[3500]	train-rmse:4.06105	dev-rmse:3.98693
[3600]	train-rmse:4.04588	dev-rmse:3.97224
[3700]	train-rmse:4.03187	dev-rmse:3.95874
[3800]	train-rmse:4.01895	dev-rmse:3.94635
[3900]	train-rmse:4.00701	dev-rmse:3.93484
[4000]	train-rmse:3.99594	dev-rmse:3.92429
[4100]	train-rmse:3.98569	dev-rmse:3.91449
[4200]	train-rmse:3.9762	dev-rmse:3.9055
[4300]	train-rmse:3.96742	dev-rmse:3.89721
[4400]	train-rmse:3.9593	dev-rmse:3.88956
[4500]	train-rmse:3.95178	dev-rmse:3.88253
[4600]	train-rmse:3.94482	dev-rmse:3.876
[4700]	train-rmse:3.93835	dev-rmse:3.86995
[4800]	train-rmse:3.93235	dev-rmse:3.86435
[4900]	train-rmse:3.92675	dev-rmse:3.8592
[5000]	train-rmse:3.92152	dev-rmse:3.8544
[5100]	train-rmse:3.91662	dev-rmse:3.84995
[5200]	train-rmse:3.91205	dev-rmse:3.84577
[5300]	train-rmse:3.90778	dev-rmse:3.84188
[5400]	train-rmse:3.90378	dev-rmse:3.83826
[5500]	train-rmse:3.90001	dev-rmse:3.83485
[5600]	train-rmse:3.89649	dev-rmse:3.83168
[5700]	train-rmse:3.89321	dev-rmse:3.82872
[5800]	train-rmse:3.8901	dev-rmse:3.82595
[5900]	train-rmse:3.88715	dev-rmse:3.82331
[6000]	train-rmse:3.88435	dev-rmse:3.8208
[6100]	train-rmse:3.8817	dev-rmse:3.81844
[6200]	train-rmse:3.87919	dev-rmse:3.81621
[6300]	train-rmse:3.87681	dev-rmse:3.81408
[6400]	train-rmse:3.87451	dev-rmse:3.81208
[6500]	train-rmse:3.87234	dev-rmse:3.81019
[6600]	train-rmse:3.87025	dev-rmse:3.80841
[6700]	train-rmse:3.86827	dev-rmse:3.80672
[6800]	train-rmse:3.86638	dev-rmse:3.80512
[6900]	train-rmse:3.86458	dev-rmse:3.8036
[7000]	train-rmse:3.86284	dev-rmse:3.80215
[7100]	train-rmse:3.86118	dev-rmse:3.80075
[7200]	train-rmse:3.85957	dev-rmse:3.7994
[7300]	train-rmse:3.85801	dev-rmse:3.7981
[7400]	train-rmse:3.85651	dev-rmse:3.79685
[7500]	train-rmse:3.85506	dev-rmse:3.79566
[7600]	train-rmse:3.85366	dev-rmse:3.7945
[7700]	train-rmse:3.8523	dev-rmse:3.79338
[7800]	train-rmse:3.85098	dev-rmse:3.7923
[7900]	train-rmse:3.8497	dev-rmse:3.79127
[8000]	train-rmse:3.84847	dev-rmse:3.79028
[8100]	train-rmse:3.84727	dev-rmse:3.78932
[8200]	train-rmse:3.84611	dev-rmse:3.78839
[8300]	train-rmse:3.84498	dev-rmse:3.78747
[8400]	train-rmse:3.84387	dev-rmse:3.78658
[8500]	train-rmse:3.8428	dev-rmse:3.78573
[8600]	train-rmse:3.84177	dev-rmse:3.7849
[8700]	train-rmse:3.84075	dev-rmse:3.7841
[8800]	train-rmse:3.83976	dev-rmse:3.78332
[8900]	train-rmse:3.83878	dev-rmse:3.78257
[9000]	train-rmse:3.83783	dev-rmse:3.78182
[9100]	train-rmse:3.83688	dev-rmse:3.78108
[9200]	train-rmse:3.83596	dev-rmse:3.78036
[9300]	train-rmse:3.83506	dev-rmse:3.77965
[9400]	train-rmse:3.83419	dev-rmse:3.77897
[9500]	train-rmse:3.83332	dev-rmse:3.77829
[9600]	train-rmse:3.83246	dev-rmse:3.77765
[9700]	train-rmse:3.83163	dev-rmse:3.77702
[9800]	train-rmse:3.83081	dev-rmse:3.7764
[9900]	train-rmse:3.83	dev-rmse:3.77579
[10000]	train-rmse:3.8292	dev-rmse:3.77519
[10100]	train-rmse:3.82842	dev-rmse:3.7746
[10200]	train-rmse:3.82765	dev-rmse:3.77401
[10300]	train-rmse:3.82689	dev-rmse:3.77344
[10400]	train-rmse:3.82614	dev-rmse:3.77289
[10500]	train-rmse:3.8254	dev-rmse:3.77235
[10600]	train-rmse:3.82467	dev-rmse:3.77182
[10700]	train-rmse:3.82396	dev-rmse:3.7713
[10800]	train-rmse:3.82326	dev-rmse:3.77078
[10900]	train-rmse:3.82256	dev-rmse:3.77029
[11000]	train-rmse:3.82188	dev-rmse:3.76979
[11100]	train-rmse:3.82119	dev-rmse:3.76931
[11200]	train-rmse:3.82051	dev-rmse:3.7688
[11300]	train-rmse:3.81984	dev-rmse:3.76832
[11400]	train-rmse:3.81918	dev-rmse:3.76785
[11500]	train-rmse:3.81854	dev-rmse:3.76739
[11600]	train-rmse:3.8179	dev-rmse:3.76695
[11700]	train-rmse:3.81728	dev-rmse:3.76652
[11800]	train-rmse:3.81667	dev-rmse:3.7661
[11900]	train-rmse:3.81606	dev-rmse:3.76567
[12000]	train-rmse:3.81545	dev-rmse:3.76526
[12100]	train-rmse:3.81486	dev-rmse:3.76485
[12200]	train-rmse:3.81427	dev-rmse:3.76445
[12300]	train-rmse:3.8137	dev-rmse:3.76407
[12400]	train-rmse:3.81312	dev-rmse:3.76366
[12500]	train-rmse:3.81254	dev-rmse:3.76324
[12600]	train-rmse:3.81196	dev-rmse:3.76281
[12700]	train-rmse:3.81137	dev-rmse:3.76239
[12800]	train-rmse:3.81079	dev-rmse:3.76196
[12900]	train-rmse:3.81023	dev-rmse:3.76155
[13000]	train-rmse:3.80966	dev-rmse:3.76114
[13100]	train-rmse:3.80911	dev-rmse:3.76074
[13200]	train-rmse:3.80856	dev-rmse:3.76033
[13300]	train-rmse:3.80801	dev-rmse:3.75993
[13400]	train-rmse:3.80747	dev-rmse:3.75953
[13500]	train-rmse:3.80693	dev-rmse:3.75915
[13600]	train-rmse:3.8064	dev-rmse:3.75876
[13700]	train-rmse:3.80588	dev-rmse:3.75838
[13800]	train-rmse:3.80535	dev-rmse:3.758
[13900]	train-rmse:3.80485	dev-rmse:3.75764
[14000]	train-rmse:3.80435	dev-rmse:3.75727
[14100]	train-rmse:3.80385	dev-rmse:3.75692
[14200]	train-rmse:3.80335	dev-rmse:3.75656
[14300]	train-rmse:3.80287	dev-rmse:3.75621
[14400]	train-rmse:3.80238	dev-rmse:3.75587
[14500]	train-rmse:3.80191	dev-rmse:3.75555
[14600]	train-rmse:3.80142	dev-rmse:3.75521
[14700]	train-rmse:3.80095	dev-rmse:3.75489
[14800]	train-rmse:3.80048	dev-rmse:3.75457
[14900]	train-rmse:3.80002	dev-rmse:3.75424
[15000]	train-rmse:3.79955	dev-rmse:3.75392
[15100]	train-rmse:3.79909	dev-rmse:3.7536
[15200]	train-rmse:3.79865	dev-rmse:3.75328
[15300]	train-rmse:3.7982	dev-rmse:3.75297
[15400]	train-rmse:3.79776	dev-rmse:3.75265
[15500]	train-rmse:3.79731	dev-rmse:3.75234
[15600]	train-rmse:3.79687	dev-rmse:3.75201
[15700]	train-rmse:3.79642	dev-rmse:3.7517
[15800]	train-rmse:3.79599	dev-rmse:3.7514
[15900]	train-rmse:3.79555	dev-rmse:3.75108
[16000]	train-rmse:3.79512	dev-rmse:3.75078
[16100]	train-rmse:3.7947	dev-rmse:3.75048
[16200]	train-rmse:3.79427	dev-rmse:3.75019
[16300]	train-rmse:3.79385	dev-rmse:3.74988
[16400]	train-rmse:3.79344	dev-rmse:3.7496
[16500]	train-rmse:3.79303	dev-rmse:3.74932
[16600]	train-rmse:3.79262	dev-rmse:3.74904
[16700]	train-rmse:3.79221	dev-rmse:3.74876
[16800]	train-rmse:3.79181	dev-rmse:3.74848
[16900]	train-rmse:3.79141	dev-rmse:3.74821
[17000]	train-rmse:3.791	dev-rmse:3.74793
[17100]	train-rmse:3.79059	dev-rmse:3.74765
[17200]	train-rmse:3.79019	dev-rmse:3.74738
[17300]	train-rmse:3.78979	dev-rmse:3.74711
[17400]	train-rmse:3.78939	dev-rmse:3.74683
[17500]	train-rmse:3.78899	dev-rmse:3.74657
[17600]	train-rmse:3.7886	dev-rmse:3.7463
[17700]	train-rmse:3.78819	dev-rmse:3.74603
[17800]	train-rmse:3.78779	dev-rmse:3.74576
[17900]	train-rmse:3.7874	dev-rmse:3.7455
[18000]	train-rmse:3.787	dev-rmse:3.74524
[18100]	train-rmse:3.78661	dev-rmse:3.74498
[18200]	train-rmse:3.78622	dev-rmse:3.74472
[18300]	train-rmse:3.78584	dev-rmse:3.74447
[18400]	train-rmse:3.78545	dev-rmse:3.74422
[18500]	train-rmse:3.78508	dev-rmse:3.74397
[18600]	train-rmse:3.78469	dev-rmse:3.74372
[18700]	train-rmse:3.78433	dev-rmse:3.74348
[18800]	train-rmse:3.78396	dev-rmse:3.74323
[18900]	train-rmse:3.78359	dev-rmse:3.743
[19000]	train-rmse:3.78323	dev-rmse:3.74277
[19100]	train-rmse:3.78287	dev-rmse:3.74255
[19200]	train-rmse:3.78251	dev-rmse:3.74233
[19300]	train-rmse:3.78215	dev-rmse:3.74211
[19400]	train-rmse:3.7818	dev-rmse:3.74189
[19500]	train-rmse:3.78145	dev-rmse:3.74167
[19600]	train-rmse:3.7811	dev-rmse:3.74146
[19700]	train-rmse:3.78075	dev-rmse:3.74124
[19800]	train-rmse:3.78041	dev-rmse:3.74104
[19900]	train-rmse:3.78007	dev-rmse:3.74083
[20000]	train-rmse:3.77973	dev-rmse:3.74063
[20100]	train-rmse:3.77939	dev-rmse:3.74043
[20200]	train-rmse:3.77905	dev-rmse:3.74022
[20300]	train-rmse:3.77872	dev-rmse:3.74002
[20400]	train-rmse:3.77838	dev-rmse:3.73982
[20500]	train-rmse:3.77805	dev-rmse:3.73963
[20600]	train-rmse:3.77773	dev-rmse:3.73943
[20700]	train-rmse:3.7774	dev-rmse:3.73924
[20800]	train-rmse:3.77708	dev-rmse:3.73904
[20900]	train-rmse:3.77676	dev-rmse:3.73885
[21000]	train-rmse:3.77644	dev-rmse:3.73866
[21100]	train-rmse:3.77612	dev-rmse:3.73847
[21200]	train-rmse:3.7758	dev-rmse:3.73828
[21300]	train-rmse:3.77549	dev-rmse:3.7381
[21400]	train-rmse:3.77518	dev-rmse:3.73791
[21500]	train-rmse:3.77487	dev-rmse:3.73772
[21600]	train-rmse:3.77456	dev-rmse:3.73754
[21700]	train-rmse:3.77426	dev-rmse:3.73736
[21800]	train-rmse:3.77396	dev-rmse:3.73718
[21900]	train-rmse:3.77366	dev-rmse:3.737
[22000]	train-rmse:3.77336	dev-rmse:3.73683
[22100]	train-rmse:3.77306	dev-rmse:3.73665
[22200]	train-rmse:3.77277	dev-rmse:3.7365
[22300]	train-rmse:3.77248	dev-rmse:3.73634
[22400]	train-rmse:3.77219	dev-rmse:3.73618
[22500]	train-rmse:3.77189	dev-rmse:3.73603
[22600]	train-rmse:3.77161	dev-rmse:3.73587
[22700]	train-rmse:3.77132	dev-rmse:3.73571
[22800]	train-rmse:3.77104	dev-rmse:3.73556
[22900]	train-rmse:3.77074	dev-rmse:3.7354
[23000]	train-rmse:3.77046	dev-rmse:3.73524
[23100]	train-rmse:3.77017	dev-rmse:3.73508
[23200]	train-rmse:3.76989	dev-rmse:3.73492
[23300]	train-rmse:3.76961	dev-rmse:3.73477
[23400]	train-rmse:3.76932	dev-rmse:3.73461
[23500]	train-rmse:3.76905	dev-rmse:3.73445
[23600]	train-rmse:3.76876	dev-rmse:3.73429
[23700]	train-rmse:3.76849	dev-rmse:3.73414
[23800]	train-rmse:3.76821	dev-rmse:3.73399
[23900]	train-rmse:3.76794	dev-rmse:3.73385
[24000]	train-rmse:3.76767	dev-rmse:3.73371
[24100]	train-rmse:3.76739	dev-rmse:3.73356
[24200]	train-rmse:3.76712	dev-rmse:3.73341
[24300]	train-rmse:3.76685	dev-rmse:3.73327
[24400]	train-rmse:3.76658	dev-rmse:3.73313
[24500]	train-rmse:3.76632	dev-rmse:3.733
[24600]	train-rmse:3.76605	dev-rmse:3.73286
[24700]	train-rmse:3.7658	dev-rmse:3.73273
[24800]	train-rmse:3.76554	dev-rmse:3.73262
[24900]	train-rmse:3.76528	dev-rmse:3.73249
[25000]	train-rmse:3.76503	dev-rmse:3.73237
[25100]	train-rmse:3.76477	dev-rmse:3.73224
[25200]	train-rmse:3.76452	dev-rmse:3.73213
[25300]	train-rmse:3.76427	dev-rmse:3.732
[25400]	train-rmse:3.76402	dev-rmse:3.73188
[25500]	train-rmse:3.76377	dev-rmse:3.73177
[25600]	train-rmse:3.76352	dev-rmse:3.73164
[25700]	train-rmse:3.76327	dev-rmse:3.73151
[25800]	train-rmse:3.76302	dev-rmse:3.73139
[25900]	train-rmse:3.76278	dev-rmse:3.73127
[26000]	train-rmse:3.76254	dev-rmse:3.73115
[26100]	train-rmse:3.76229	dev-rmse:3.73103
[26200]	train-rmse:3.76205	dev-rmse:3.73091
[26300]	train-rmse:3.7618	dev-rmse:3.73078
[26400]	train-rmse:3.76157	dev-rmse:3.73067
[26500]	train-rmse:3.76133	dev-rmse:3.73056
[26600]	train-rmse:3.7611	dev-rmse:3.73044
[26700]	train-rmse:3.76086	dev-rmse:3.73033
[26800]	train-rmse:3.76062	dev-rmse:3.73021
[26900]	train-rmse:3.76038	dev-rmse:3.73008
[27000]	train-rmse:3.76015	dev-rmse:3.72997
[27100]	train-rmse:3.75991	dev-rmse:3.72985
[27200]	train-rmse:3.75967	dev-rmse:3.72974
[27300]	train-rmse:3.75944	dev-rmse:3.72963
[27400]	train-rmse:3.75921	dev-rmse:3.72951
[27500]	train-rmse:3.75897	dev-rmse:3.72939
[27600]	train-rmse:3.75874	dev-rmse:3.72928
[27700]	train-rmse:3.75851	dev-rmse:3.72917
[27800]	train-rmse:3.75828	dev-rmse:3.72906
[27900]	train-rmse:3.75805	dev-rmse:3.72895
[28000]	train-rmse:3.75782	dev-rmse:3.72884
[28100]	train-rmse:3.7576	dev-rmse:3.72873
[28200]	train-rmse:3.75738	dev-rmse:3.72863
[28300]	train-rmse:3.75715	dev-rmse:3.72852
[28400]	train-rmse:3.75693	dev-rmse:3.72841
[28500]	train-rmse:3.7567	dev-rmse:3.72831
[28600]	train-rmse:3.75648	dev-rmse:3.7282
[28700]	train-rmse:3.75626	dev-rmse:3.72809
[28800]	train-rmse:3.75604	dev-rmse:3.72798
[28900]	train-rmse:3.75582	dev-rmse:3.72789
[29000]	train-rmse:3.7556	dev-rmse:3.72778
[29100]	train-rmse:3.75538	dev-rmse:3.72767
[29200]	train-rmse:3.75517	dev-rmse:3.72757
[29300]	train-rmse:3.75495	dev-rmse:3.72746
[29400]	train-rmse:3.75473	dev-rmse:3.72737
[29500]	train-rmse:3.75452	dev-rmse:3.72728
[29600]	train-rmse:3.7543	dev-rmse:3.72718
[29700]	train-rmse:3.75409	dev-rmse:3.72709
[29800]	train-rmse:3.75388	dev-rmse:3.727
[29900]	train-rmse:3.75367	dev-rmse:3.72691
[30000]	train-rmse:3.75346	dev-rmse:3.72682
[30100]	train-rmse:3.75325	dev-rmse:3.72673
[30200]	train-rmse:3.75304	dev-rmse:3.72665
[30300]	train-rmse:3.75284	dev-rmse:3.72656
[30400]	train-rmse:3.75264	dev-rmse:3.72648
[30500]	train-rmse:3.75243	dev-rmse:3.72639
[30600]	train-rmse:3.75223	dev-rmse:3.72631
[30700]	train-rmse:3.75202	dev-rmse:3.72623
[30800]	train-rmse:3.75182	dev-rmse:3.72615
[30900]	train-rmse:3.75162	dev-rmse:3.72608
[31000]	train-rmse:3.75142	dev-rmse:3.726
[31100]	train-rmse:3.75122	dev-rmse:3.72591
[31200]	train-rmse:3.75102	dev-rmse:3.72585
[31300]	train-rmse:3.75082	dev-rmse:3.72578
[31400]	train-rmse:3.75062	dev-rmse:3.72569
[31500]	train-rmse:3.75042	dev-rmse:3.72562
[31600]	train-rmse:3.75023	dev-rmse:3.72554
[31700]	train-rmse:3.75003	dev-rmse:3.72547
[31800]	train-rmse:3.74984	dev-rmse:3.7254
[31900]	train-rmse:3.74965	dev-rmse:3.72533
[32000]	train-rmse:3.74945	dev-rmse:3.72525
[32100]	train-rmse:3.74926	dev-rmse:3.72519
[32200]	train-rmse:3.74907	dev-rmse:3.72512
[32300]	train-rmse:3.74889	dev-rmse:3.72505
[32400]	train-rmse:3.7487	dev-rmse:3.72499
[32500]	train-rmse:3.74851	dev-rmse:3.72492
[32600]	train-rmse:3.74833	dev-rmse:3.72486
[32700]	train-rmse:3.74814	dev-rmse:3.72479
[32800]	train-rmse:3.74796	dev-rmse:3.72473
[32900]	train-rmse:3.74777	dev-rmse:3.72466
[33000]	train-rmse:3.74758	dev-rmse:3.72459
[33100]	train-rmse:3.7474	dev-rmse:3.72452
[33200]	train-rmse:3.74722	dev-rmse:3.72446
[33300]	train-rmse:3.74703	dev-rmse:3.72439
[33400]	train-rmse:3.74684	dev-rmse:3.72432
[33500]	train-rmse:3.74666	dev-rmse:3.72425
[33600]	train-rmse:3.74648	dev-rmse:3.72419
[33700]	train-rmse:3.74629	dev-rmse:3.72412
[33800]	train-rmse:3.74611	dev-rmse:3.72406
[33900]	train-rmse:3.74593	dev-rmse:3.72399
[34000]	train-rmse:3.74575	dev-rmse:3.72392
[34100]	train-rmse:3.74557	dev-rmse:3.72387
[34200]	train-rmse:3.74539	dev-rmse:3.72379
[34300]	train-rmse:3.74522	dev-rmse:3.72374
[34400]	train-rmse:3.74504	dev-rmse:3.72368
[34500]	train-rmse:3.74486	dev-rmse:3.72361
[34600]	train-rmse:3.74469	dev-rmse:3.72355
[34700]	train-rmse:3.74451	dev-rmse:3.7235
[34800]	train-rmse:3.74434	dev-rmse:3.72343
[34900]	train-rmse:3.74416	dev-rmse:3.72336
[35000]	train-rmse:3.74399	dev-rmse:3.7233
[35100]	train-rmse:3.74382	dev-rmse:3.72325
[35200]	train-rmse:3.74365	dev-rmse:3.72319
[35300]	train-rmse:3.74347	dev-rmse:3.72313
[35400]	train-rmse:3.7433	dev-rmse:3.72306
[35500]	train-rmse:3.74314	dev-rmse:3.72301
[35600]	train-rmse:3.74297	dev-rmse:3.72295
[35700]	train-rmse:3.74281	dev-rmse:3.7229
[35800]	train-rmse:3.74264	dev-rmse:3.72284
[35900]	train-rmse:3.74248	dev-rmse:3.72278
[36000]	train-rmse:3.74231	dev-rmse:3.72272
[36100]	train-rmse:3.74215	dev-rmse:3.72267
[36200]	train-rmse:3.74198	dev-rmse:3.72261
[36300]	train-rmse:3.74182	dev-rmse:3.72255
[36400]	train-rmse:3.74166	dev-rmse:3.7225
[36500]	train-rmse:3.74149	dev-rmse:3.72246
[36600]	train-rmse:3.74133	dev-rmse:3.72241
[36700]	train-rmse:3.74117	dev-rmse:3.72236
[36800]	train-rmse:3.74101	dev-rmse:3.72231
[36900]	train-rmse:3.74085	dev-rmse:3.72226
[37000]	train-rmse:3.74069	dev-rmse:3.72222
[37100]	train-rmse:3.74052	dev-rmse:3.72217
[37200]	train-rmse:3.74036	dev-rmse:3.72211
[37300]	train-rmse:3.7402	dev-rmse:3.72207
[37400]	train-rmse:3.74004	dev-rmse:3.72202
[37500]	train-rmse:3.73987	dev-rmse:3.72197
[37600]	train-rmse:3.73971	dev-rmse:3.72191
[37700]	train-rmse:3.73955	dev-rmse:3.72187
[37800]	train-rmse:3.73938	dev-rmse:3.72181
[37900]	train-rmse:3.73922	dev-rmse:3.72176
[38000]	train-rmse:3.73906	dev-rmse:3.72171
[38100]	train-rmse:3.73889	dev-rmse:3.72166
[38200]	train-rmse:3.73874	dev-rmse:3.72161
[38300]	train-rmse:3.73857	dev-rmse:3.72156
[38400]	train-rmse:3.73842	dev-rmse:3.72152
[38500]	train-rmse:3.73826	dev-rmse:3.72148
[38600]	train-rmse:3.7381	dev-rmse:3.72143
[38700]	train-rmse:3.73794	dev-rmse:3.72138
[38800]	train-rmse:3.73778	dev-rmse:3.72133
[38900]	train-rmse:3.73762	dev-rmse:3.72128
[39000]	train-rmse:3.73747	dev-rmse:3.72124
[39100]	train-rmse:3.73731	dev-rmse:3.72119
[39200]	train-rmse:3.73715	dev-rmse:3.72115
[39300]	train-rmse:3.73699	dev-rmse:3.7211
[39400]	train-rmse:3.73684	dev-rmse:3.72106
[39500]	train-rmse:3.73669	dev-rmse:3.72101
[39600]	train-rmse:3.73653	dev-rmse:3.72097
[39700]	train-rmse:3.73638	dev-rmse:3.72093
[39800]	train-rmse:3.73623	dev-rmse:3.72088
[39900]	train-rmse:3.73608	dev-rmse:3.72085
[40000]	train-rmse:3.73592	dev-rmse:3.72079
[40100]	train-rmse:3.73577	dev-rmse:3.72076
[40200]	train-rmse:3.73562	dev-rmse:3.72071
[40300]	train-rmse:3.73547	dev-rmse:3.72067
[40400]	train-rmse:3.73532	dev-rmse:3.72063
[40500]	train-rmse:3.73517	dev-rmse:3.72058
[40600]	train-rmse:3.73502	dev-rmse:3.72053
[40700]	train-rmse:3.73488	dev-rmse:3.72049
[40800]	train-rmse:3.73473	dev-rmse:3.72044
[40900]	train-rmse:3.73459	dev-rmse:3.72039
[41000]	train-rmse:3.73444	dev-rmse:3.72035
[41100]	train-rmse:3.73429	dev-rmse:3.72031
[41200]	train-rmse:3.73415	dev-rmse:3.72026
[41300]	train-rmse:3.734	dev-rmse:3.72021
[41400]	train-rmse:3.73386	dev-rmse:3.72017
[41500]	train-rmse:3.73372	dev-rmse:3.72013
[41600]	train-rmse:3.73357	dev-rmse:3.72008
[41700]	train-rmse:3.73343	dev-rmse:3.72004
Stopping. Best iteration:
[41729]	train-rmse:3.73339	dev-rmse:3.72002

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 2, 'learning_rate': 0.01, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33605	dev-rmse:5.26894
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.01506	dev-rmse:4.94445
[200]	train-rmse:4.50525	dev-rmse:4.42874
[300]	train-rmse:4.15517	dev-rmse:4.07794
[400]	train-rmse:3.99267	dev-rmse:3.92086
[500]	train-rmse:3.9178	dev-rmse:3.85104
[600]	train-rmse:3.88015	dev-rmse:3.81756
[700]	train-rmse:3.85833	dev-rmse:3.79916
[800]	train-rmse:3.84331	dev-rmse:3.78718
[900]	train-rmse:3.83207	dev-rmse:3.77844
[1000]	train-rmse:3.82289	dev-rmse:3.77147
[1100]	train-rmse:3.81513	dev-rmse:3.76594
[1200]	train-rmse:3.80861	dev-rmse:3.76114
[1300]	train-rmse:3.80236	dev-rmse:3.75682
[1400]	train-rmse:3.79674	dev-rmse:3.7529
[1500]	train-rmse:3.79178	dev-rmse:3.74946
[1600]	train-rmse:3.78691	dev-rmse:3.74639
[1700]	train-rmse:3.78251	dev-rmse:3.74368
[1800]	train-rmse:3.77849	dev-rmse:3.74095
[1900]	train-rmse:3.77453	dev-rmse:3.73836
[2000]	train-rmse:3.7708	dev-rmse:3.73604
[2100]	train-rmse:3.76751	dev-rmse:3.73426
[2200]	train-rmse:3.76424	dev-rmse:3.73245
[2300]	train-rmse:3.76105	dev-rmse:3.73071
[2400]	train-rmse:3.75813	dev-rmse:3.72926
[2500]	train-rmse:3.75545	dev-rmse:3.7278
[2600]	train-rmse:3.75279	dev-rmse:3.7263
[2700]	train-rmse:3.75017	dev-rmse:3.72503
[2800]	train-rmse:3.7478	dev-rmse:3.72388
[2900]	train-rmse:3.74551	dev-rmse:3.72293
[3000]	train-rmse:3.7433	dev-rmse:3.72193
[3100]	train-rmse:3.74109	dev-rmse:3.72103
[3200]	train-rmse:3.73895	dev-rmse:3.72028
[3300]	train-rmse:3.73706	dev-rmse:3.7196
[3400]	train-rmse:3.73516	dev-rmse:3.71897
[3500]	train-rmse:3.73342	dev-rmse:3.71839
[3600]	train-rmse:3.73167	dev-rmse:3.71797
[3700]	train-rmse:3.72995	dev-rmse:3.71753
[3800]	train-rmse:3.7283	dev-rmse:3.71699
[3900]	train-rmse:3.72681	dev-rmse:3.71653
[4000]	train-rmse:3.72532	dev-rmse:3.7161
[4100]	train-rmse:3.72389	dev-rmse:3.71557
[4200]	train-rmse:3.72242	dev-rmse:3.71521
[4300]	train-rmse:3.72096	dev-rmse:3.71472
[4400]	train-rmse:3.71972	dev-rmse:3.71447
[4500]	train-rmse:3.71846	dev-rmse:3.71416
Stopping. Best iteration:
[4531]	train-rmse:3.71805	dev-rmse:3.71402

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 2, 'learning_rate': 0.01, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33605	dev-rmse:5.26894
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.01505	dev-rmse:4.94444
[200]	train-rmse:4.50581	dev-rmse:4.42933
[300]	train-rmse:4.15592	dev-rmse:4.0787
[400]	train-rmse:3.99424	dev-rmse:3.9226
[500]	train-rmse:3.92053	dev-rmse:3.8535
[600]	train-rmse:3.88368	dev-rmse:3.82022
[700]	train-rmse:3.86225	dev-rmse:3.80165
[800]	train-rmse:3.84791	dev-rmse:3.78983
[900]	train-rmse:3.83738	dev-rmse:3.7813
[1000]	train-rmse:3.8287	dev-rmse:3.77463
[1100]	train-rmse:3.82134	dev-rmse:3.76921
[1200]	train-rmse:3.81498	dev-rmse:3.76467
[1300]	train-rmse:3.80927	dev-rmse:3.76073
[1400]	train-rmse:3.80396	dev-rmse:3.75692
[1500]	train-rmse:3.79913	dev-rmse:3.75348
[1600]	train-rmse:3.79449	dev-rmse:3.75004
[1700]	train-rmse:3.79044	dev-rmse:3.74733
[1800]	train-rmse:3.78662	dev-rmse:3.74478
[1900]	train-rmse:3.7828	dev-rmse:3.74227
[2000]	train-rmse:3.77932	dev-rmse:3.74017
[2100]	train-rmse:3.77604	dev-rmse:3.73818
[2200]	train-rmse:3.77297	dev-rmse:3.73641
[2300]	train-rmse:3.77002	dev-rmse:3.73469
[2400]	train-rmse:3.76725	dev-rmse:3.73316
[2500]	train-rmse:3.76465	dev-rmse:3.73193
[2600]	train-rmse:3.7622	dev-rmse:3.73079
[2700]	train-rmse:3.75975	dev-rmse:3.7296
[2800]	train-rmse:3.75741	dev-rmse:3.72842
[2900]	train-rmse:3.75518	dev-rmse:3.72739
[3000]	train-rmse:3.75303	dev-rmse:3.72637
[3100]	train-rmse:3.751	dev-rmse:3.72555
[3200]	train-rmse:3.74903	dev-rmse:3.72476
[3300]	train-rmse:3.74715	dev-rmse:3.72398
[3400]	train-rmse:3.74532	dev-rmse:3.72331
[3500]	train-rmse:3.74358	dev-rmse:3.72272
[3600]	train-rmse:3.74191	dev-rmse:3.72218
[3700]	train-rmse:3.74029	dev-rmse:3.72168
[3800]	train-rmse:3.73864	dev-rmse:3.72117
[3900]	train-rmse:3.73706	dev-rmse:3.72073
[4000]	train-rmse:3.73555	dev-rmse:3.72031
[4100]	train-rmse:3.73406	dev-rmse:3.71988
[4200]	train-rmse:3.73265	dev-rmse:3.71943
[4300]	train-rmse:3.73131	dev-rmse:3.71907
[4400]	train-rmse:3.72993	dev-rmse:3.71875
[4500]	train-rmse:3.72861	dev-rmse:3.7184
[4600]	train-rmse:3.72734	dev-rmse:3.71804
[4700]	train-rmse:3.72609	dev-rmse:3.71775
[4800]	train-rmse:3.72487	dev-rmse:3.7175
[4900]	train-rmse:3.72372	dev-rmse:3.71733
[5000]	train-rmse:3.72259	dev-rmse:3.71717
Stopping. Best iteration:
[5003]	train-rmse:3.72255	dev-rmse:3.71715

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 2, 'learning_rate': 0.005, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33721	dev-rmse:5.27011
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.20127	dev-rmse:5.13273
[200]	train-rmse:5.01718	dev-rmse:4.9466
[300]	train-rmse:4.77711	dev-rmse:4.70351
[400]	train-rmse:4.50785	dev-rmse:4.43139
[500]	train-rmse:4.30367	dev-rmse:4.22535
[600]	train-rmse:4.1567	dev-rmse:4.0792
[700]	train-rmse:4.0589	dev-rmse:3.98399
[800]	train-rmse:3.99367	dev-rmse:3.92175
[900]	train-rmse:3.94924	dev-rmse:3.88002
[1000]	train-rmse:3.91835	dev-rmse:3.8517
[1100]	train-rmse:3.89635	dev-rmse:3.832
[1200]	train-rmse:3.88043	dev-rmse:3.81774
[1300]	train-rmse:3.86796	dev-rmse:3.8071
[1400]	train-rmse:3.85808	dev-rmse:3.79895
[1500]	train-rmse:3.85005	dev-rmse:3.79254
[1600]	train-rmse:3.84311	dev-rmse:3.7872
[1700]	train-rmse:3.83726	dev-rmse:3.78269
[1800]	train-rmse:3.83218	dev-rmse:3.77851
[1900]	train-rmse:3.82738	dev-rmse:3.77487
[2000]	train-rmse:3.82304	dev-rmse:3.77178
[2100]	train-rmse:3.81914	dev-rmse:3.76899
[2200]	train-rmse:3.81544	dev-rmse:3.76639
[2300]	train-rmse:3.81182	dev-rmse:3.76386
[2400]	train-rmse:3.80846	dev-rmse:3.76154
[2500]	train-rmse:3.80538	dev-rmse:3.75932
[2600]	train-rmse:3.80235	dev-rmse:3.75723
[2700]	train-rmse:3.79944	dev-rmse:3.75527
[2800]	train-rmse:3.79677	dev-rmse:3.75341
[2900]	train-rmse:3.7941	dev-rmse:3.75176
[3000]	train-rmse:3.79155	dev-rmse:3.74993
[3100]	train-rmse:3.7891	dev-rmse:3.74821
[3200]	train-rmse:3.78669	dev-rmse:3.7467
[3300]	train-rmse:3.78452	dev-rmse:3.74523
[3400]	train-rmse:3.78235	dev-rmse:3.74376
[3500]	train-rmse:3.78031	dev-rmse:3.74226
[3600]	train-rmse:3.77832	dev-rmse:3.74099
[3700]	train-rmse:3.77633	dev-rmse:3.73979
[3800]	train-rmse:3.77433	dev-rmse:3.73853
[3900]	train-rmse:3.77252	dev-rmse:3.73729
[4000]	train-rmse:3.77074	dev-rmse:3.73626
[4100]	train-rmse:3.76905	dev-rmse:3.73516
[4200]	train-rmse:3.76733	dev-rmse:3.73416
[4300]	train-rmse:3.76567	dev-rmse:3.73321
[4400]	train-rmse:3.76417	dev-rmse:3.73231
[4500]	train-rmse:3.76261	dev-rmse:3.73143
[4600]	train-rmse:3.76104	dev-rmse:3.73057
[4700]	train-rmse:3.75955	dev-rmse:3.72972
[4800]	train-rmse:3.75806	dev-rmse:3.72901
[4900]	train-rmse:3.75666	dev-rmse:3.72833
[5000]	train-rmse:3.7552	dev-rmse:3.72758
[5100]	train-rmse:3.75384	dev-rmse:3.72684
[5200]	train-rmse:3.75251	dev-rmse:3.72617
[5300]	train-rmse:3.75124	dev-rmse:3.72559
[5400]	train-rmse:3.75005	dev-rmse:3.72493
[5500]	train-rmse:3.74877	dev-rmse:3.72431
[5600]	train-rmse:3.74755	dev-rmse:3.7237
[5700]	train-rmse:3.74644	dev-rmse:3.7233
[5800]	train-rmse:3.74537	dev-rmse:3.72293
[5900]	train-rmse:3.74424	dev-rmse:3.7225
[6000]	train-rmse:3.74314	dev-rmse:3.72223
[6100]	train-rmse:3.74207	dev-rmse:3.72188
[6200]	train-rmse:3.74104	dev-rmse:3.72148
[6300]	train-rmse:3.73999	dev-rmse:3.72108
[6400]	train-rmse:3.73901	dev-rmse:3.72073
[6500]	train-rmse:3.73803	dev-rmse:3.72038
[6600]	train-rmse:3.73708	dev-rmse:3.72006
[6700]	train-rmse:3.73614	dev-rmse:3.71962
[6800]	train-rmse:3.73519	dev-rmse:3.71933
[6900]	train-rmse:3.73428	dev-rmse:3.71902
[7000]	train-rmse:3.73341	dev-rmse:3.71877
[7100]	train-rmse:3.7325	dev-rmse:3.71852
[7200]	train-rmse:3.73171	dev-rmse:3.71819
[7300]	train-rmse:3.73085	dev-rmse:3.71792
[7400]	train-rmse:3.73005	dev-rmse:3.71761
[7500]	train-rmse:3.72923	dev-rmse:3.71736
[7600]	train-rmse:3.7284	dev-rmse:3.71712
[7700]	train-rmse:3.72759	dev-rmse:3.71687
[7800]	train-rmse:3.72678	dev-rmse:3.71673
[7900]	train-rmse:3.72602	dev-rmse:3.71652
[8000]	train-rmse:3.72531	dev-rmse:3.7164
[8100]	train-rmse:3.72454	dev-rmse:3.71617
[8200]	train-rmse:3.72375	dev-rmse:3.71595
[8300]	train-rmse:3.72306	dev-rmse:3.71574
[8400]	train-rmse:3.72236	dev-rmse:3.71549
Stopping. Best iteration:
[8474]	train-rmse:3.72184	dev-rmse:3.71529

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 2, 'learning_rate': 0.005, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33721	dev-rmse:5.27011
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.20127	dev-rmse:5.13273
[200]	train-rmse:5.01717	dev-rmse:4.94658
[300]	train-rmse:4.77714	dev-rmse:4.70353
[400]	train-rmse:4.50854	dev-rmse:4.43209
[500]	train-rmse:4.3045	dev-rmse:4.22632
[600]	train-rmse:4.15767	dev-rmse:4.08047
[700]	train-rmse:4.06	dev-rmse:3.98591
[800]	train-rmse:3.99518	dev-rmse:3.92355
[900]	train-rmse:3.95123	dev-rmse:3.882
[1000]	train-rmse:3.9211	dev-rmse:3.85402
[1100]	train-rmse:3.89967	dev-rmse:3.83456
[1200]	train-rmse:3.88411	dev-rmse:3.82059
[1300]	train-rmse:3.87204	dev-rmse:3.8099
[1400]	train-rmse:3.86261	dev-rmse:3.80192
[1500]	train-rmse:3.85487	dev-rmse:3.79543
[1600]	train-rmse:3.84826	dev-rmse:3.79005
[1700]	train-rmse:3.84264	dev-rmse:3.78551
[1800]	train-rmse:3.83766	dev-rmse:3.78157
[1900]	train-rmse:3.83314	dev-rmse:3.77801
[2000]	train-rmse:3.82902	dev-rmse:3.7749
[2100]	train-rmse:3.82523	dev-rmse:3.77205
[2200]	train-rmse:3.82169	dev-rmse:3.76943
[2300]	train-rmse:3.8184	dev-rmse:3.76709
[2400]	train-rmse:3.81526	dev-rmse:3.76494
[2500]	train-rmse:3.81239	dev-rmse:3.76296
[2600]	train-rmse:3.80954	dev-rmse:3.76088
[2700]	train-rmse:3.8068	dev-rmse:3.75893
[2800]	train-rmse:3.80419	dev-rmse:3.757
[2900]	train-rmse:3.80176	dev-rmse:3.75529
[3000]	train-rmse:3.7994	dev-rmse:3.75362
[3100]	train-rmse:3.79712	dev-rmse:3.75199
[3200]	train-rmse:3.79486	dev-rmse:3.75032
[3300]	train-rmse:3.79278	dev-rmse:3.74887
[3400]	train-rmse:3.79075	dev-rmse:3.74749
[3500]	train-rmse:3.7888	dev-rmse:3.74619
[3600]	train-rmse:3.78686	dev-rmse:3.74487
[3700]	train-rmse:3.78491	dev-rmse:3.74358
[3800]	train-rmse:3.78308	dev-rmse:3.74244
[3900]	train-rmse:3.78129	dev-rmse:3.74136
[4000]	train-rmse:3.77956	dev-rmse:3.74029
[4100]	train-rmse:3.77787	dev-rmse:3.7393
[4200]	train-rmse:3.77625	dev-rmse:3.73833
[4300]	train-rmse:3.77468	dev-rmse:3.73738
[4400]	train-rmse:3.77318	dev-rmse:3.7365
[4500]	train-rmse:3.77171	dev-rmse:3.73566
[4600]	train-rmse:3.77028	dev-rmse:3.73489
[4700]	train-rmse:3.76888	dev-rmse:3.73412
[4800]	train-rmse:3.7675	dev-rmse:3.73337
[4900]	train-rmse:3.76615	dev-rmse:3.73267
[5000]	train-rmse:3.76484	dev-rmse:3.73198
[5100]	train-rmse:3.76359	dev-rmse:3.7314
[5200]	train-rmse:3.76236	dev-rmse:3.73082
[5300]	train-rmse:3.76118	dev-rmse:3.73021
[5400]	train-rmse:3.75998	dev-rmse:3.72964
[5500]	train-rmse:3.75883	dev-rmse:3.72911
[5600]	train-rmse:3.75769	dev-rmse:3.72857
[5700]	train-rmse:3.75658	dev-rmse:3.72809
[5800]	train-rmse:3.7555	dev-rmse:3.72763
[5900]	train-rmse:3.75442	dev-rmse:3.72716
[6000]	train-rmse:3.75337	dev-rmse:3.72666
[6100]	train-rmse:3.75232	dev-rmse:3.7262
[6200]	train-rmse:3.7513	dev-rmse:3.72573
[6300]	train-rmse:3.7503	dev-rmse:3.7253
[6400]	train-rmse:3.74932	dev-rmse:3.72491
[6500]	train-rmse:3.74836	dev-rmse:3.72456
[6600]	train-rmse:3.74741	dev-rmse:3.72421
[6700]	train-rmse:3.74649	dev-rmse:3.72386
[6800]	train-rmse:3.7456	dev-rmse:3.72354
[6900]	train-rmse:3.74473	dev-rmse:3.72326
[7000]	train-rmse:3.74386	dev-rmse:3.72297
[7100]	train-rmse:3.74303	dev-rmse:3.72267
[7200]	train-rmse:3.7422	dev-rmse:3.72241
[7300]	train-rmse:3.74137	dev-rmse:3.72216
[7400]	train-rmse:3.74054	dev-rmse:3.7219
[7500]	train-rmse:3.73972	dev-rmse:3.72167
[7600]	train-rmse:3.73891	dev-rmse:3.72142
[7700]	train-rmse:3.73811	dev-rmse:3.72119
[7800]	train-rmse:3.73733	dev-rmse:3.72096
[7900]	train-rmse:3.73656	dev-rmse:3.72073
[8000]	train-rmse:3.7358	dev-rmse:3.72051
[8100]	train-rmse:3.73506	dev-rmse:3.72032
[8200]	train-rmse:3.73432	dev-rmse:3.72009
[8300]	train-rmse:3.7336	dev-rmse:3.71986
[8400]	train-rmse:3.73289	dev-rmse:3.71965
[8500]	train-rmse:3.73221	dev-rmse:3.71947
[8600]	train-rmse:3.73153	dev-rmse:3.71926
[8700]	train-rmse:3.73082	dev-rmse:3.71907
[8800]	train-rmse:3.73015	dev-rmse:3.71888
[8900]	train-rmse:3.72948	dev-rmse:3.7187
[9000]	train-rmse:3.72882	dev-rmse:3.71854
[9100]	train-rmse:3.72816	dev-rmse:3.71835
[9200]	train-rmse:3.72752	dev-rmse:3.71818
[9300]	train-rmse:3.7269	dev-rmse:3.71803
[9400]	train-rmse:3.72628	dev-rmse:3.71789
[9500]	train-rmse:3.72567	dev-rmse:3.71776
[9600]	train-rmse:3.72505	dev-rmse:3.71764
[9700]	train-rmse:3.72446	dev-rmse:3.71752
[9800]	train-rmse:3.72389	dev-rmse:3.71742
Stopping. Best iteration:
[9807]	train-rmse:3.72385	dev-rmse:3.7174

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 2, 'learning_rate': 0.1, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.31457	dev-rmse:5.24723
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:3.823	dev-rmse:3.76995
[200]	train-rmse:3.772	dev-rmse:3.73767
[300]	train-rmse:3.74528	dev-rmse:3.72297
[400]	train-rmse:3.72726	dev-rmse:3.71744
[500]	train-rmse:3.71342	dev-rmse:3.71418
[600]	train-rmse:3.70286	dev-rmse:3.71174
[700]	train-rmse:3.69369	dev-rmse:3.70912
Stopping. Best iteration:
[710]	train-rmse:3.69274	dev-rmse:3.70901

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 2, 'learning_rate': 0.1, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.31457	dev-rmse:5.24723
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:3.82658	dev-rmse:3.77321
[200]	train-rmse:3.77725	dev-rmse:3.73891
[300]	train-rmse:3.75119	dev-rmse:3.72559
[400]	train-rmse:3.73385	dev-rmse:3.71986
Stopping. Best iteration:
[447]	train-rmse:3.7273	dev-rmse:3.71798

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 3, 'learning_rate': 0.001, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33814	dev-rmse:5.27104
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.31433	dev-rmse:5.24698
[200]	train-rmse:5.28895	dev-rmse:5.22135
[300]	train-rmse:5.26192	dev-rmse:5.19403
[400]	train-rmse:5.23315	dev-rmse:5.16495
[500]	train-rmse:5.20254	dev-rmse:5.13401
[600]	train-rmse:5.17	dev-rmse:5.10113
[700]	train-rmse:5.13545	dev-rmse:5.0662
[800]	train-rmse:5.0988	dev-rmse:5.02914
[900]	train-rmse:5.05996	dev-rmse:4.98988
[1000]	train-rmse:5.01887	dev-rmse:4.94832
[1100]	train-rmse:4.97551	dev-rmse:4.90446
[1200]	train-rmse:4.92985	dev-rmse:4.85826
[1300]	train-rmse:4.88186	dev-rmse:4.80969
[1400]	train-rmse:4.83156	dev-rmse:4.75879
[1500]	train-rmse:4.77933	dev-rmse:4.70589
[1600]	train-rmse:4.72535	dev-rmse:4.65126
[1700]	train-rmse:4.67012	dev-rmse:4.59541
[1800]	train-rmse:4.61493	dev-rmse:4.53972
[1900]	train-rmse:4.5614	dev-rmse:4.48576
[2000]	train-rmse:4.51025	dev-rmse:4.43417
[2100]	train-rmse:4.4621	dev-rmse:4.38571
[2200]	train-rmse:4.41739	dev-rmse:4.34075
[2300]	train-rmse:4.37547	dev-rmse:4.29861
[2400]	train-rmse:4.33589	dev-rmse:4.2588
[2500]	train-rmse:4.29855	dev-rmse:4.22131
[2600]	train-rmse:4.26362	dev-rmse:4.18648
[2700]	train-rmse:4.23099	dev-rmse:4.15424
[2800]	train-rmse:4.20061	dev-rmse:4.12451
[2900]	train-rmse:4.17232	dev-rmse:4.09709
[3000]	train-rmse:4.14589	dev-rmse:4.07158
[3100]	train-rmse:4.12126	dev-rmse:4.04805
[3200]	train-rmse:4.0983	dev-rmse:4.02627
[3300]	train-rmse:4.07713	dev-rmse:4.00608
[3400]	train-rmse:4.05757	dev-rmse:3.98769
[3500]	train-rmse:4.03958	dev-rmse:3.97077
[3600]	train-rmse:4.02292	dev-rmse:3.95524
[3700]	train-rmse:4.00754	dev-rmse:3.94096
[3800]	train-rmse:3.99328	dev-rmse:3.92782
[3900]	train-rmse:3.98007	dev-rmse:3.91571
[4000]	train-rmse:3.96781	dev-rmse:3.90459
[4100]	train-rmse:3.95641	dev-rmse:3.89428
[4200]	train-rmse:3.9458	dev-rmse:3.88481
[4300]	train-rmse:3.93591	dev-rmse:3.876
[4400]	train-rmse:3.92668	dev-rmse:3.86782
[4500]	train-rmse:3.91811	dev-rmse:3.86035
[4600]	train-rmse:3.91006	dev-rmse:3.85333
[4700]	train-rmse:3.90255	dev-rmse:3.84684
[4800]	train-rmse:3.89555	dev-rmse:3.84086
[4900]	train-rmse:3.88895	dev-rmse:3.83525
[5000]	train-rmse:3.88273	dev-rmse:3.82998
[5100]	train-rmse:3.87685	dev-rmse:3.82509
[5200]	train-rmse:3.87133	dev-rmse:3.82055
[5300]	train-rmse:3.86622	dev-rmse:3.81629
[5400]	train-rmse:3.86133	dev-rmse:3.8123
[5500]	train-rmse:3.85666	dev-rmse:3.80859
[5600]	train-rmse:3.85228	dev-rmse:3.80513
[5700]	train-rmse:3.84816	dev-rmse:3.80194
[5800]	train-rmse:3.8443	dev-rmse:3.79892
[5900]	train-rmse:3.84059	dev-rmse:3.7961
[6000]	train-rmse:3.83706	dev-rmse:3.79334
[6100]	train-rmse:3.8337	dev-rmse:3.79077
[6200]	train-rmse:3.8305	dev-rmse:3.78844
[6300]	train-rmse:3.82749	dev-rmse:3.78615
[6400]	train-rmse:3.82455	dev-rmse:3.78402
[6500]	train-rmse:3.82176	dev-rmse:3.78203
[6600]	train-rmse:3.81907	dev-rmse:3.78013
[6700]	train-rmse:3.81653	dev-rmse:3.77831
[6800]	train-rmse:3.81407	dev-rmse:3.77659
[6900]	train-rmse:3.81175	dev-rmse:3.77497
[7000]	train-rmse:3.80949	dev-rmse:3.77338
[7100]	train-rmse:3.80726	dev-rmse:3.77181
[7200]	train-rmse:3.80514	dev-rmse:3.7704
[7300]	train-rmse:3.80308	dev-rmse:3.76897
[7400]	train-rmse:3.80111	dev-rmse:3.76759
[7500]	train-rmse:3.79917	dev-rmse:3.76632
[7600]	train-rmse:3.79729	dev-rmse:3.7651
[7700]	train-rmse:3.79546	dev-rmse:3.7639
[7800]	train-rmse:3.79368	dev-rmse:3.76277
[7900]	train-rmse:3.79197	dev-rmse:3.76167
[8000]	train-rmse:3.79031	dev-rmse:3.76058
[8100]	train-rmse:3.78867	dev-rmse:3.75957
[8200]	train-rmse:3.78706	dev-rmse:3.75861
[8300]	train-rmse:3.78555	dev-rmse:3.75773
[8400]	train-rmse:3.78405	dev-rmse:3.75676
[8500]	train-rmse:3.78255	dev-rmse:3.75584
[8600]	train-rmse:3.78115	dev-rmse:3.75503
[8700]	train-rmse:3.77974	dev-rmse:3.75415
[8800]	train-rmse:3.77835	dev-rmse:3.75332
[8900]	train-rmse:3.77702	dev-rmse:3.75247
[9000]	train-rmse:3.77571	dev-rmse:3.75172
[9100]	train-rmse:3.77442	dev-rmse:3.75094
[9200]	train-rmse:3.77317	dev-rmse:3.75019
[9300]	train-rmse:3.7719	dev-rmse:3.74943
[9400]	train-rmse:3.77066	dev-rmse:3.74872
[9500]	train-rmse:3.76945	dev-rmse:3.74806
[9600]	train-rmse:3.76828	dev-rmse:3.74739
[9700]	train-rmse:3.76709	dev-rmse:3.74673
[9800]	train-rmse:3.76594	dev-rmse:3.74606
[9900]	train-rmse:3.76481	dev-rmse:3.74538
[10000]	train-rmse:3.76371	dev-rmse:3.74477
[10100]	train-rmse:3.76258	dev-rmse:3.74412
[10200]	train-rmse:3.76153	dev-rmse:3.74357
[10300]	train-rmse:3.76048	dev-rmse:3.74302
[10400]	train-rmse:3.75942	dev-rmse:3.74249
[10500]	train-rmse:3.7584	dev-rmse:3.74192
[10600]	train-rmse:3.75742	dev-rmse:3.74135
[10700]	train-rmse:3.7564	dev-rmse:3.74079
[10800]	train-rmse:3.75537	dev-rmse:3.74025
[10900]	train-rmse:3.75442	dev-rmse:3.73975
[11000]	train-rmse:3.75346	dev-rmse:3.73924
[11100]	train-rmse:3.7525	dev-rmse:3.73872
[11200]	train-rmse:3.75157	dev-rmse:3.73826
[11300]	train-rmse:3.75063	dev-rmse:3.73779
[11400]	train-rmse:3.7497	dev-rmse:3.73733
[11500]	train-rmse:3.74878	dev-rmse:3.73685
[11600]	train-rmse:3.74783	dev-rmse:3.73636
[11700]	train-rmse:3.74694	dev-rmse:3.73591
[11800]	train-rmse:3.74607	dev-rmse:3.73549
[11900]	train-rmse:3.74521	dev-rmse:3.73511
[12000]	train-rmse:3.74434	dev-rmse:3.73465
[12100]	train-rmse:3.74348	dev-rmse:3.73425
[12200]	train-rmse:3.74259	dev-rmse:3.73382
[12300]	train-rmse:3.74173	dev-rmse:3.73341
[12400]	train-rmse:3.74091	dev-rmse:3.73303
[12500]	train-rmse:3.7401	dev-rmse:3.73265
[12600]	train-rmse:3.73924	dev-rmse:3.73225
[12700]	train-rmse:3.73842	dev-rmse:3.73185
[12800]	train-rmse:3.7376	dev-rmse:3.73147
[12900]	train-rmse:3.73678	dev-rmse:3.7311
[13000]	train-rmse:3.73599	dev-rmse:3.73076
[13100]	train-rmse:3.73522	dev-rmse:3.73037
[13200]	train-rmse:3.73445	dev-rmse:3.73004
[13300]	train-rmse:3.7337	dev-rmse:3.72971
[13400]	train-rmse:3.73295	dev-rmse:3.72935
[13500]	train-rmse:3.73218	dev-rmse:3.72908
[13600]	train-rmse:3.73144	dev-rmse:3.72876
[13700]	train-rmse:3.73073	dev-rmse:3.72843
[13800]	train-rmse:3.72996	dev-rmse:3.72816
[13900]	train-rmse:3.72925	dev-rmse:3.72783
[14000]	train-rmse:3.7285	dev-rmse:3.7275
[14100]	train-rmse:3.72779	dev-rmse:3.72721
[14200]	train-rmse:3.72709	dev-rmse:3.72693
[14300]	train-rmse:3.72641	dev-rmse:3.72667
[14400]	train-rmse:3.7257	dev-rmse:3.72638
[14500]	train-rmse:3.72503	dev-rmse:3.7261
[14600]	train-rmse:3.72432	dev-rmse:3.72582
[14700]	train-rmse:3.72364	dev-rmse:3.72554
[14800]	train-rmse:3.72295	dev-rmse:3.72526
[14900]	train-rmse:3.72227	dev-rmse:3.72499
[15000]	train-rmse:3.7216	dev-rmse:3.72474
[15100]	train-rmse:3.72093	dev-rmse:3.72449
[15200]	train-rmse:3.72029	dev-rmse:3.72423
[15300]	train-rmse:3.7196	dev-rmse:3.72397
[15400]	train-rmse:3.71895	dev-rmse:3.72373
[15500]	train-rmse:3.71833	dev-rmse:3.72352
[15600]	train-rmse:3.71771	dev-rmse:3.72333
[15700]	train-rmse:3.71708	dev-rmse:3.72311
[15800]	train-rmse:3.71646	dev-rmse:3.72289
[15900]	train-rmse:3.71584	dev-rmse:3.72267
[16000]	train-rmse:3.71523	dev-rmse:3.72241
[16100]	train-rmse:3.71463	dev-rmse:3.72219
[16200]	train-rmse:3.71402	dev-rmse:3.72198
[16300]	train-rmse:3.71345	dev-rmse:3.7218
[16400]	train-rmse:3.71283	dev-rmse:3.7216
[16500]	train-rmse:3.7122	dev-rmse:3.72136
[16600]	train-rmse:3.71162	dev-rmse:3.72116
[16700]	train-rmse:3.71105	dev-rmse:3.72097
[16800]	train-rmse:3.71046	dev-rmse:3.72076
[16900]	train-rmse:3.70989	dev-rmse:3.72054
[17000]	train-rmse:3.70929	dev-rmse:3.72032
[17100]	train-rmse:3.70874	dev-rmse:3.72016
[17200]	train-rmse:3.70818	dev-rmse:3.71998
[17300]	train-rmse:3.70761	dev-rmse:3.71981
[17400]	train-rmse:3.70706	dev-rmse:3.71965
[17500]	train-rmse:3.70652	dev-rmse:3.71947
[17600]	train-rmse:3.70599	dev-rmse:3.71932
[17700]	train-rmse:3.70542	dev-rmse:3.71918
[17800]	train-rmse:3.70485	dev-rmse:3.71901
[17900]	train-rmse:3.70428	dev-rmse:3.71883
[18000]	train-rmse:3.70376	dev-rmse:3.71868
[18100]	train-rmse:3.70319	dev-rmse:3.71853
[18200]	train-rmse:3.70265	dev-rmse:3.71837
[18300]	train-rmse:3.7021	dev-rmse:3.71821
[18400]	train-rmse:3.7016	dev-rmse:3.71807
[18500]	train-rmse:3.70106	dev-rmse:3.71792
[18600]	train-rmse:3.70055	dev-rmse:3.7178
[18700]	train-rmse:3.70002	dev-rmse:3.71764
[18800]	train-rmse:3.6995	dev-rmse:3.71751
[18900]	train-rmse:3.699	dev-rmse:3.71738
[19000]	train-rmse:3.6985	dev-rmse:3.71724
[19100]	train-rmse:3.69798	dev-rmse:3.71711
[19200]	train-rmse:3.69746	dev-rmse:3.71698
[19300]	train-rmse:3.69695	dev-rmse:3.71686
[19400]	train-rmse:3.69647	dev-rmse:3.71674
[19500]	train-rmse:3.69595	dev-rmse:3.71664
[19600]	train-rmse:3.69545	dev-rmse:3.71648
[19700]	train-rmse:3.69495	dev-rmse:3.71635
[19800]	train-rmse:3.69443	dev-rmse:3.71624
[19900]	train-rmse:3.69394	dev-rmse:3.71615
[20000]	train-rmse:3.69344	dev-rmse:3.71604
[20100]	train-rmse:3.69296	dev-rmse:3.71592
[20200]	train-rmse:3.69249	dev-rmse:3.71578
[20300]	train-rmse:3.69199	dev-rmse:3.71567
[20400]	train-rmse:3.69151	dev-rmse:3.71554
[20500]	train-rmse:3.69103	dev-rmse:3.71543
[20600]	train-rmse:3.69058	dev-rmse:3.71531
[20700]	train-rmse:3.6901	dev-rmse:3.71523
[20800]	train-rmse:3.68965	dev-rmse:3.71515
[20900]	train-rmse:3.68918	dev-rmse:3.71504
[21000]	train-rmse:3.68875	dev-rmse:3.71493
[21100]	train-rmse:3.68828	dev-rmse:3.71485
[21200]	train-rmse:3.68784	dev-rmse:3.71474
[21300]	train-rmse:3.68737	dev-rmse:3.71461
[21400]	train-rmse:3.68691	dev-rmse:3.71451
[21500]	train-rmse:3.68646	dev-rmse:3.7144
[21600]	train-rmse:3.68602	dev-rmse:3.71429
[21700]	train-rmse:3.68557	dev-rmse:3.71421
[21800]	train-rmse:3.68513	dev-rmse:3.7141
[21900]	train-rmse:3.68466	dev-rmse:3.71399
[22000]	train-rmse:3.68425	dev-rmse:3.71393
[22100]	train-rmse:3.68381	dev-rmse:3.71385
Stopping. Best iteration:
[22124]	train-rmse:3.68371	dev-rmse:3.71382

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 3, 'learning_rate': 0.001, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33814	dev-rmse:5.27104
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.31433	dev-rmse:5.24698
[200]	train-rmse:5.28895	dev-rmse:5.22134
[300]	train-rmse:5.26192	dev-rmse:5.19403
[400]	train-rmse:5.23315	dev-rmse:5.16495
[500]	train-rmse:5.20254	dev-rmse:5.134
[600]	train-rmse:5.17	dev-rmse:5.10111
[700]	train-rmse:5.13545	dev-rmse:5.06618
[800]	train-rmse:5.09879	dev-rmse:5.02912
[900]	train-rmse:5.05996	dev-rmse:4.98985
[1000]	train-rmse:5.01887	dev-rmse:4.94829
[1100]	train-rmse:4.97551	dev-rmse:4.90444
[1200]	train-rmse:4.92984	dev-rmse:4.85823
[1300]	train-rmse:4.88184	dev-rmse:4.80966
[1400]	train-rmse:4.83153	dev-rmse:4.75877
[1500]	train-rmse:4.7793	dev-rmse:4.70586
[1600]	train-rmse:4.72534	dev-rmse:4.65124
[1700]	train-rmse:4.67016	dev-rmse:4.59546
[1800]	train-rmse:4.61505	dev-rmse:4.53986
[1900]	train-rmse:4.56173	dev-rmse:4.48608
[2000]	train-rmse:4.5106	dev-rmse:4.43449
[2100]	train-rmse:4.46244	dev-rmse:4.38599
[2200]	train-rmse:4.41789	dev-rmse:4.34123
[2300]	train-rmse:4.37603	dev-rmse:4.29923
[2400]	train-rmse:4.33644	dev-rmse:4.25948
[2500]	train-rmse:4.29905	dev-rmse:4.22183
[2600]	train-rmse:4.26441	dev-rmse:4.18755
[2700]	train-rmse:4.23187	dev-rmse:4.15554
[2800]	train-rmse:4.20161	dev-rmse:4.126
[2900]	train-rmse:4.17348	dev-rmse:4.09878
[3000]	train-rmse:4.14709	dev-rmse:4.07335
[3100]	train-rmse:4.12231	dev-rmse:4.04962
[3200]	train-rmse:4.09947	dev-rmse:4.02774
[3300]	train-rmse:4.07836	dev-rmse:4.00778
[3400]	train-rmse:4.05891	dev-rmse:3.98964
[3500]	train-rmse:4.04104	dev-rmse:3.97286
[3600]	train-rmse:4.02459	dev-rmse:3.95743
[3700]	train-rmse:4.00949	dev-rmse:3.94349
[3800]	train-rmse:3.99546	dev-rmse:3.93062
[3900]	train-rmse:3.98244	dev-rmse:3.91874
[4000]	train-rmse:3.97038	dev-rmse:3.90782
[4100]	train-rmse:3.95911	dev-rmse:3.89771
[4200]	train-rmse:3.94859	dev-rmse:3.88835
[4300]	train-rmse:3.93878	dev-rmse:3.87965
[4400]	train-rmse:3.92957	dev-rmse:3.87165
[4500]	train-rmse:3.92101	dev-rmse:3.86429
[4600]	train-rmse:3.91304	dev-rmse:3.85728
[4700]	train-rmse:3.90568	dev-rmse:3.8507
[4800]	train-rmse:3.89878	dev-rmse:3.84454
[4900]	train-rmse:3.89235	dev-rmse:3.83874
[5000]	train-rmse:3.88635	dev-rmse:3.83334
[5100]	train-rmse:3.8807	dev-rmse:3.82838
[5200]	train-rmse:3.87537	dev-rmse:3.82382
[5300]	train-rmse:3.87037	dev-rmse:3.81955
[5400]	train-rmse:3.86567	dev-rmse:3.81555
[5500]	train-rmse:3.86124	dev-rmse:3.81181
[5600]	train-rmse:3.85709	dev-rmse:3.80832
[5700]	train-rmse:3.85316	dev-rmse:3.80505
[5800]	train-rmse:3.84941	dev-rmse:3.80195
[5900]	train-rmse:3.84588	dev-rmse:3.79905
[6000]	train-rmse:3.84252	dev-rmse:3.79631
[6100]	train-rmse:3.8393	dev-rmse:3.79373
[6200]	train-rmse:3.83623	dev-rmse:3.7913
[6300]	train-rmse:3.83329	dev-rmse:3.789
[6400]	train-rmse:3.83048	dev-rmse:3.78682
[6500]	train-rmse:3.82777	dev-rmse:3.78476
[6600]	train-rmse:3.82521	dev-rmse:3.78282
[6700]	train-rmse:3.82278	dev-rmse:3.78101
[6800]	train-rmse:3.82043	dev-rmse:3.77928
[6900]	train-rmse:3.81819	dev-rmse:3.77766
[7000]	train-rmse:3.81602	dev-rmse:3.77612
[7100]	train-rmse:3.81393	dev-rmse:3.77467
[7200]	train-rmse:3.8119	dev-rmse:3.77329
[7300]	train-rmse:3.80994	dev-rmse:3.77196
[7400]	train-rmse:3.80805	dev-rmse:3.77068
[7500]	train-rmse:3.80623	dev-rmse:3.76944
[7600]	train-rmse:3.80448	dev-rmse:3.76826
[7700]	train-rmse:3.80278	dev-rmse:3.76712
[7800]	train-rmse:3.80113	dev-rmse:3.76604
[7900]	train-rmse:3.79957	dev-rmse:3.76503
[8000]	train-rmse:3.79804	dev-rmse:3.76406
[8100]	train-rmse:3.79653	dev-rmse:3.76309
[8200]	train-rmse:3.79506	dev-rmse:3.76213
[8300]	train-rmse:3.79362	dev-rmse:3.76119
[8400]	train-rmse:3.79222	dev-rmse:3.76031
[8500]	train-rmse:3.79085	dev-rmse:3.75945
[8600]	train-rmse:3.7895	dev-rmse:3.75861
[8700]	train-rmse:3.78818	dev-rmse:3.75779
[8800]	train-rmse:3.78691	dev-rmse:3.757
[8900]	train-rmse:3.78565	dev-rmse:3.75622
[9000]	train-rmse:3.78442	dev-rmse:3.7555
[9100]	train-rmse:3.78321	dev-rmse:3.75477
[9200]	train-rmse:3.78202	dev-rmse:3.75407
[9300]	train-rmse:3.78087	dev-rmse:3.75341
[9400]	train-rmse:3.77974	dev-rmse:3.75278
[9500]	train-rmse:3.77863	dev-rmse:3.75217
[9600]	train-rmse:3.77754	dev-rmse:3.75156
[9700]	train-rmse:3.77643	dev-rmse:3.75094
[9800]	train-rmse:3.77534	dev-rmse:3.75032
[9900]	train-rmse:3.77427	dev-rmse:3.74971
[10000]	train-rmse:3.77321	dev-rmse:3.74912
[10100]	train-rmse:3.77217	dev-rmse:3.74855
[10200]	train-rmse:3.77115	dev-rmse:3.74801
[10300]	train-rmse:3.77014	dev-rmse:3.74747
[10400]	train-rmse:3.76918	dev-rmse:3.74695
[10500]	train-rmse:3.76823	dev-rmse:3.74646
[10600]	train-rmse:3.76728	dev-rmse:3.74596
[10700]	train-rmse:3.76636	dev-rmse:3.74547
[10800]	train-rmse:3.76546	dev-rmse:3.74499
[10900]	train-rmse:3.76456	dev-rmse:3.74452
[11000]	train-rmse:3.76368	dev-rmse:3.74406
[11100]	train-rmse:3.76281	dev-rmse:3.7436
[11200]	train-rmse:3.76194	dev-rmse:3.74314
[11300]	train-rmse:3.76108	dev-rmse:3.74267
[11400]	train-rmse:3.76024	dev-rmse:3.74222
[11500]	train-rmse:3.7594	dev-rmse:3.74179
[11600]	train-rmse:3.75858	dev-rmse:3.74135
[11700]	train-rmse:3.75776	dev-rmse:3.74095
[11800]	train-rmse:3.75695	dev-rmse:3.74052
[11900]	train-rmse:3.75615	dev-rmse:3.74011
[12000]	train-rmse:3.75537	dev-rmse:3.73971
[12100]	train-rmse:3.75459	dev-rmse:3.73932
[12200]	train-rmse:3.75381	dev-rmse:3.73892
[12300]	train-rmse:3.75303	dev-rmse:3.73852
[12400]	train-rmse:3.75224	dev-rmse:3.73812
[12500]	train-rmse:3.75147	dev-rmse:3.73774
[12600]	train-rmse:3.75069	dev-rmse:3.73736
[12700]	train-rmse:3.74993	dev-rmse:3.737
[12800]	train-rmse:3.74917	dev-rmse:3.73663
[12900]	train-rmse:3.74841	dev-rmse:3.73626
[13000]	train-rmse:3.74763	dev-rmse:3.73586
[13100]	train-rmse:3.74686	dev-rmse:3.73548
[13200]	train-rmse:3.7461	dev-rmse:3.73511
[13300]	train-rmse:3.74534	dev-rmse:3.73474
[13400]	train-rmse:3.74459	dev-rmse:3.73439
[13500]	train-rmse:3.74385	dev-rmse:3.73405
[13600]	train-rmse:3.74312	dev-rmse:3.73372
[13700]	train-rmse:3.7424	dev-rmse:3.73339
[13800]	train-rmse:3.74168	dev-rmse:3.73307
[13900]	train-rmse:3.74097	dev-rmse:3.73275
[14000]	train-rmse:3.74027	dev-rmse:3.73244
[14100]	train-rmse:3.73958	dev-rmse:3.73211
[14200]	train-rmse:3.7389	dev-rmse:3.73181
[14300]	train-rmse:3.73824	dev-rmse:3.73153
[14400]	train-rmse:3.73758	dev-rmse:3.73123
[14500]	train-rmse:3.73695	dev-rmse:3.73097
[14600]	train-rmse:3.73631	dev-rmse:3.7307
[14700]	train-rmse:3.73568	dev-rmse:3.73042
[14800]	train-rmse:3.73504	dev-rmse:3.73013
[14900]	train-rmse:3.73441	dev-rmse:3.72986
[15000]	train-rmse:3.73378	dev-rmse:3.72959
[15100]	train-rmse:3.73316	dev-rmse:3.72933
[15200]	train-rmse:3.73253	dev-rmse:3.72904
[15300]	train-rmse:3.73189	dev-rmse:3.72874
[15400]	train-rmse:3.73125	dev-rmse:3.72843
[15500]	train-rmse:3.73063	dev-rmse:3.72814
[15600]	train-rmse:3.73001	dev-rmse:3.72786
[15700]	train-rmse:3.7294	dev-rmse:3.72762
[15800]	train-rmse:3.72881	dev-rmse:3.72739
[15900]	train-rmse:3.72822	dev-rmse:3.72717
[16000]	train-rmse:3.72763	dev-rmse:3.72694
[16100]	train-rmse:3.72704	dev-rmse:3.72671
[16200]	train-rmse:3.72646	dev-rmse:3.72649
[16300]	train-rmse:3.72588	dev-rmse:3.72627
[16400]	train-rmse:3.72531	dev-rmse:3.72606
[16500]	train-rmse:3.72475	dev-rmse:3.72586
[16600]	train-rmse:3.72419	dev-rmse:3.72565
[16700]	train-rmse:3.72363	dev-rmse:3.72544
[16800]	train-rmse:3.72307	dev-rmse:3.72523
[16900]	train-rmse:3.72252	dev-rmse:3.72504
[17000]	train-rmse:3.72198	dev-rmse:3.72485
[17100]	train-rmse:3.72144	dev-rmse:3.72466
[17200]	train-rmse:3.72092	dev-rmse:3.72449
[17300]	train-rmse:3.72039	dev-rmse:3.72431
[17400]	train-rmse:3.71987	dev-rmse:3.72414
[17500]	train-rmse:3.71936	dev-rmse:3.72398
[17600]	train-rmse:3.71885	dev-rmse:3.72382
[17700]	train-rmse:3.71834	dev-rmse:3.72365
[17800]	train-rmse:3.71785	dev-rmse:3.7235
[17900]	train-rmse:3.71735	dev-rmse:3.72334
[18000]	train-rmse:3.71685	dev-rmse:3.7232
[18100]	train-rmse:3.71636	dev-rmse:3.72304
[18200]	train-rmse:3.71587	dev-rmse:3.7229
[18300]	train-rmse:3.71538	dev-rmse:3.72276
[18400]	train-rmse:3.7149	dev-rmse:3.72263
[18500]	train-rmse:3.71442	dev-rmse:3.7225
[18600]	train-rmse:3.71394	dev-rmse:3.72236
[18700]	train-rmse:3.71347	dev-rmse:3.72223
[18800]	train-rmse:3.713	dev-rmse:3.72211
[18900]	train-rmse:3.71253	dev-rmse:3.72199
[19000]	train-rmse:3.71207	dev-rmse:3.72187
[19100]	train-rmse:3.7116	dev-rmse:3.72175
[19200]	train-rmse:3.71114	dev-rmse:3.72162
[19300]	train-rmse:3.71066	dev-rmse:3.7215
[19400]	train-rmse:3.7102	dev-rmse:3.72138
[19500]	train-rmse:3.70973	dev-rmse:3.72125
[19600]	train-rmse:3.70926	dev-rmse:3.72112
[19700]	train-rmse:3.7088	dev-rmse:3.72099
[19800]	train-rmse:3.70834	dev-rmse:3.72087
[19900]	train-rmse:3.70789	dev-rmse:3.72075
[20000]	train-rmse:3.70744	dev-rmse:3.72065
[20100]	train-rmse:3.707	dev-rmse:3.72054
[20200]	train-rmse:3.70655	dev-rmse:3.72044
[20300]	train-rmse:3.70611	dev-rmse:3.72034
[20400]	train-rmse:3.70567	dev-rmse:3.72024
[20500]	train-rmse:3.70523	dev-rmse:3.72015
[20600]	train-rmse:3.70481	dev-rmse:3.72006
[20700]	train-rmse:3.70438	dev-rmse:3.71996
[20800]	train-rmse:3.70395	dev-rmse:3.71988
[20900]	train-rmse:3.70353	dev-rmse:3.71979
[21000]	train-rmse:3.70311	dev-rmse:3.71971
[21100]	train-rmse:3.70269	dev-rmse:3.71963
[21200]	train-rmse:3.70227	dev-rmse:3.71954
[21300]	train-rmse:3.70185	dev-rmse:3.71945
[21400]	train-rmse:3.70143	dev-rmse:3.71935
[21500]	train-rmse:3.70103	dev-rmse:3.71925
[21600]	train-rmse:3.70062	dev-rmse:3.71915
[21700]	train-rmse:3.70022	dev-rmse:3.71906
[21800]	train-rmse:3.69981	dev-rmse:3.71898
[21900]	train-rmse:3.69942	dev-rmse:3.7189
[22000]	train-rmse:3.69902	dev-rmse:3.71882
[22100]	train-rmse:3.69862	dev-rmse:3.71873
[22200]	train-rmse:3.69823	dev-rmse:3.71865
[22300]	train-rmse:3.69784	dev-rmse:3.71857
[22400]	train-rmse:3.69745	dev-rmse:3.71849
[22500]	train-rmse:3.69705	dev-rmse:3.71841
[22600]	train-rmse:3.69666	dev-rmse:3.71832
[22700]	train-rmse:3.69626	dev-rmse:3.71823
[22800]	train-rmse:3.69587	dev-rmse:3.71814
[22900]	train-rmse:3.69549	dev-rmse:3.71806
[23000]	train-rmse:3.6951	dev-rmse:3.71798
[23100]	train-rmse:3.69471	dev-rmse:3.71791
[23200]	train-rmse:3.69432	dev-rmse:3.71783
[23300]	train-rmse:3.69393	dev-rmse:3.71775
[23400]	train-rmse:3.69355	dev-rmse:3.71768
[23500]	train-rmse:3.69317	dev-rmse:3.71761
[23600]	train-rmse:3.69278	dev-rmse:3.71753
[23700]	train-rmse:3.69239	dev-rmse:3.71746
[23800]	train-rmse:3.692	dev-rmse:3.71738
[23900]	train-rmse:3.6916	dev-rmse:3.71731
[24000]	train-rmse:3.6912	dev-rmse:3.71723
[24100]	train-rmse:3.6908	dev-rmse:3.71716
[24200]	train-rmse:3.69041	dev-rmse:3.71707
[24300]	train-rmse:3.69	dev-rmse:3.71699
[24400]	train-rmse:3.68961	dev-rmse:3.71691
[24500]	train-rmse:3.68922	dev-rmse:3.71684
[24600]	train-rmse:3.68884	dev-rmse:3.71677
[24700]	train-rmse:3.68845	dev-rmse:3.71671
[24800]	train-rmse:3.68807	dev-rmse:3.71663
[24900]	train-rmse:3.68769	dev-rmse:3.71657
[25000]	train-rmse:3.68731	dev-rmse:3.7165
[25100]	train-rmse:3.68694	dev-rmse:3.71645
[25200]	train-rmse:3.68656	dev-rmse:3.71639
[25300]	train-rmse:3.68619	dev-rmse:3.71633
[25400]	train-rmse:3.68581	dev-rmse:3.71628
[25500]	train-rmse:3.68545	dev-rmse:3.71623
[25600]	train-rmse:3.68509	dev-rmse:3.71617
[25700]	train-rmse:3.68474	dev-rmse:3.71613
[25800]	train-rmse:3.68438	dev-rmse:3.71608
[25900]	train-rmse:3.68403	dev-rmse:3.71603
[26000]	train-rmse:3.68368	dev-rmse:3.71599
[26100]	train-rmse:3.68334	dev-rmse:3.71594
[26200]	train-rmse:3.683	dev-rmse:3.71587
[26300]	train-rmse:3.68265	dev-rmse:3.71581
[26400]	train-rmse:3.6823	dev-rmse:3.71575
[26500]	train-rmse:3.68196	dev-rmse:3.71569
[26600]	train-rmse:3.68162	dev-rmse:3.71563
[26700]	train-rmse:3.68128	dev-rmse:3.71557
[26800]	train-rmse:3.68094	dev-rmse:3.71551
[26900]	train-rmse:3.6806	dev-rmse:3.71545
[27000]	train-rmse:3.68026	dev-rmse:3.71539
[27100]	train-rmse:3.67992	dev-rmse:3.71533
[27200]	train-rmse:3.67958	dev-rmse:3.71527
[27300]	train-rmse:3.67925	dev-rmse:3.71522
[27400]	train-rmse:3.67892	dev-rmse:3.71517
[27500]	train-rmse:3.67859	dev-rmse:3.71512
[27600]	train-rmse:3.67827	dev-rmse:3.71507
[27700]	train-rmse:3.67794	dev-rmse:3.71502
[27800]	train-rmse:3.67762	dev-rmse:3.71498
[27900]	train-rmse:3.67729	dev-rmse:3.71493
[28000]	train-rmse:3.67698	dev-rmse:3.71489
[28100]	train-rmse:3.67666	dev-rmse:3.71484
[28200]	train-rmse:3.67634	dev-rmse:3.71479
[28300]	train-rmse:3.67603	dev-rmse:3.71474
[28400]	train-rmse:3.67571	dev-rmse:3.71469
[28500]	train-rmse:3.67541	dev-rmse:3.71465
[28600]	train-rmse:3.67508	dev-rmse:3.7146
[28700]	train-rmse:3.67476	dev-rmse:3.71456
[28800]	train-rmse:3.67444	dev-rmse:3.71453
[28900]	train-rmse:3.67413	dev-rmse:3.7145
[29000]	train-rmse:3.67381	dev-rmse:3.71448
Stopping. Best iteration:
[28980]	train-rmse:3.67388	dev-rmse:3.71447

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 3, 'learning_rate': 0.01, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33605	dev-rmse:5.26894
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.01506	dev-rmse:4.94445
[200]	train-rmse:4.50547	dev-rmse:4.42934
[300]	train-rmse:4.14291	dev-rmse:4.06889
[400]	train-rmse:3.96646	dev-rmse:3.9032
[500]	train-rmse:3.88194	dev-rmse:3.82902
[600]	train-rmse:3.83684	dev-rmse:3.79325
[700]	train-rmse:3.80962	dev-rmse:3.77298
[800]	train-rmse:3.79056	dev-rmse:3.76046
[900]	train-rmse:3.77606	dev-rmse:3.75145
[1000]	train-rmse:3.76381	dev-rmse:3.74437
[1100]	train-rmse:3.75319	dev-rmse:3.73867
[1200]	train-rmse:3.74414	dev-rmse:3.73379
[1300]	train-rmse:3.73586	dev-rmse:3.72962
[1400]	train-rmse:3.72831	dev-rmse:3.72643
[1500]	train-rmse:3.72174	dev-rmse:3.7238
[1600]	train-rmse:3.71536	dev-rmse:3.72172
[1700]	train-rmse:3.70959	dev-rmse:3.71998
[1800]	train-rmse:3.70405	dev-rmse:3.71822
[1900]	train-rmse:3.69867	dev-rmse:3.71648
[2000]	train-rmse:3.69356	dev-rmse:3.7153
[2100]	train-rmse:3.68893	dev-rmse:3.71446
[2200]	train-rmse:3.68428	dev-rmse:3.71371
[2300]	train-rmse:3.67989	dev-rmse:3.71288
[2400]	train-rmse:3.67599	dev-rmse:3.71216
[2500]	train-rmse:3.67204	dev-rmse:3.7116
[2600]	train-rmse:3.66794	dev-rmse:3.71078
[2700]	train-rmse:3.66428	dev-rmse:3.71016
Stopping. Best iteration:
[2711]	train-rmse:3.66388	dev-rmse:3.71008

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 3, 'learning_rate': 0.01, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33605	dev-rmse:5.26894
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.01505	dev-rmse:4.94444
[200]	train-rmse:4.50563	dev-rmse:4.42945
[300]	train-rmse:4.14379	dev-rmse:4.07004
[400]	train-rmse:3.96849	dev-rmse:3.9061
[500]	train-rmse:3.8852	dev-rmse:3.83246
[600]	train-rmse:3.84172	dev-rmse:3.79593
[700]	train-rmse:3.81542	dev-rmse:3.77581
[800]	train-rmse:3.79744	dev-rmse:3.76376
[900]	train-rmse:3.78398	dev-rmse:3.75533
[1000]	train-rmse:3.77284	dev-rmse:3.74891
[1100]	train-rmse:3.76334	dev-rmse:3.74389
[1200]	train-rmse:3.75498	dev-rmse:3.73947
[1300]	train-rmse:3.74725	dev-rmse:3.73567
[1400]	train-rmse:3.73982	dev-rmse:3.73232
[1500]	train-rmse:3.73339	dev-rmse:3.72948
[1600]	train-rmse:3.72717	dev-rmse:3.72668
[1700]	train-rmse:3.72158	dev-rmse:3.72467
[1800]	train-rmse:3.71628	dev-rmse:3.72293
[1900]	train-rmse:3.71147	dev-rmse:3.72159
[2000]	train-rmse:3.70688	dev-rmse:3.72037
[2100]	train-rmse:3.70254	dev-rmse:3.71926
[2200]	train-rmse:3.69842	dev-rmse:3.71839
[2300]	train-rmse:3.69445	dev-rmse:3.71755
[2400]	train-rmse:3.69061	dev-rmse:3.71693
[2500]	train-rmse:3.68681	dev-rmse:3.71625
[2600]	train-rmse:3.68316	dev-rmse:3.71563
[2700]	train-rmse:3.67979	dev-rmse:3.71515
[2800]	train-rmse:3.67653	dev-rmse:3.71467
[2900]	train-rmse:3.67337	dev-rmse:3.71426
[3000]	train-rmse:3.67028	dev-rmse:3.71392
[3100]	train-rmse:3.66717	dev-rmse:3.71344
[3200]	train-rmse:3.66414	dev-rmse:3.71305
[3300]	train-rmse:3.66111	dev-rmse:3.71263
[3400]	train-rmse:3.65828	dev-rmse:3.71221
[3500]	train-rmse:3.65554	dev-rmse:3.71192
Stopping. Best iteration:
[3501]	train-rmse:3.65551	dev-rmse:3.71191

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 3, 'learning_rate': 0.005, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33721	dev-rmse:5.27011
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.20127	dev-rmse:5.13273
[200]	train-rmse:5.01718	dev-rmse:4.9466
[300]	train-rmse:4.77717	dev-rmse:4.70372
[400]	train-rmse:4.50817	dev-rmse:4.43211
[500]	train-rmse:4.2969	dev-rmse:4.21968
[600]	train-rmse:4.14463	dev-rmse:4.07048
[700]	train-rmse:4.03858	dev-rmse:3.96984
[800]	train-rmse:3.9669	dev-rmse:3.90392
[900]	train-rmse:3.91738	dev-rmse:3.85989
[1000]	train-rmse:3.88222	dev-rmse:3.83017
[1100]	train-rmse:3.85649	dev-rmse:3.80912
[1200]	train-rmse:3.83688	dev-rmse:3.79356
[1300]	train-rmse:3.8215	dev-rmse:3.78211
[1400]	train-rmse:3.80907	dev-rmse:3.77331
[1500]	train-rmse:3.79884	dev-rmse:3.76631
[1600]	train-rmse:3.79007	dev-rmse:3.76083
[1700]	train-rmse:3.78253	dev-rmse:3.75612
[1800]	train-rmse:3.77572	dev-rmse:3.75185
[1900]	train-rmse:3.76937	dev-rmse:3.74803
[2000]	train-rmse:3.76381	dev-rmse:3.74502
[2100]	train-rmse:3.75861	dev-rmse:3.74223
[2200]	train-rmse:3.75368	dev-rmse:3.73979
[2300]	train-rmse:3.74892	dev-rmse:3.73725
[2400]	train-rmse:3.7445	dev-rmse:3.73509
[2500]	train-rmse:3.7404	dev-rmse:3.7331
[2600]	train-rmse:3.73626	dev-rmse:3.73107
[2700]	train-rmse:3.73237	dev-rmse:3.72936
[2800]	train-rmse:3.72864	dev-rmse:3.72786
[2900]	train-rmse:3.72512	dev-rmse:3.72649
[3000]	train-rmse:3.72179	dev-rmse:3.72509
[3100]	train-rmse:3.71854	dev-rmse:3.72389
[3200]	train-rmse:3.71546	dev-rmse:3.72268
[3300]	train-rmse:3.71247	dev-rmse:3.72156
[3400]	train-rmse:3.7095	dev-rmse:3.72064
[3500]	train-rmse:3.70678	dev-rmse:3.71966
[3600]	train-rmse:3.70395	dev-rmse:3.71886
[3700]	train-rmse:3.70124	dev-rmse:3.71806
[3800]	train-rmse:3.69856	dev-rmse:3.71729
[3900]	train-rmse:3.69616	dev-rmse:3.71666
[4000]	train-rmse:3.69381	dev-rmse:3.71608
[4100]	train-rmse:3.69156	dev-rmse:3.7155
[4200]	train-rmse:3.6892	dev-rmse:3.71483
[4300]	train-rmse:3.68697	dev-rmse:3.7143
[4400]	train-rmse:3.68478	dev-rmse:3.71385
[4500]	train-rmse:3.68264	dev-rmse:3.71351
[4600]	train-rmse:3.68039	dev-rmse:3.713
[4700]	train-rmse:3.67827	dev-rmse:3.71266
[4800]	train-rmse:3.67624	dev-rmse:3.71231
[4900]	train-rmse:3.67428	dev-rmse:3.71205
Stopping. Best iteration:
[4967]	train-rmse:3.6729	dev-rmse:3.71182

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 3, 'learning_rate': 0.005, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33721	dev-rmse:5.27011
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.20127	dev-rmse:5.13273
[200]	train-rmse:5.01717	dev-rmse:4.94658
[300]	train-rmse:4.77715	dev-rmse:4.70369
[400]	train-rmse:4.50837	dev-rmse:4.43224
[500]	train-rmse:4.29718	dev-rmse:4.21994
[600]	train-rmse:4.14562	dev-rmse:4.07191
[700]	train-rmse:4.0399	dev-rmse:3.97179
[800]	train-rmse:3.96953	dev-rmse:3.90706
[900]	train-rmse:3.92037	dev-rmse:3.86377
[1000]	train-rmse:3.88584	dev-rmse:3.83296
[1100]	train-rmse:3.86086	dev-rmse:3.81153
[1200]	train-rmse:3.8422	dev-rmse:3.79611
[1300]	train-rmse:3.8275	dev-rmse:3.78459
[1400]	train-rmse:3.81579	dev-rmse:3.77598
[1500]	train-rmse:3.80599	dev-rmse:3.76931
[1600]	train-rmse:3.7978	dev-rmse:3.7639
[1700]	train-rmse:3.79062	dev-rmse:3.75928
[1800]	train-rmse:3.78423	dev-rmse:3.75535
[1900]	train-rmse:3.77848	dev-rmse:3.75206
[2000]	train-rmse:3.77306	dev-rmse:3.74908
[2100]	train-rmse:3.76809	dev-rmse:3.74639
[2200]	train-rmse:3.76354	dev-rmse:3.744
[2300]	train-rmse:3.75928	dev-rmse:3.74175
[2400]	train-rmse:3.75527	dev-rmse:3.7397
[2500]	train-rmse:3.75132	dev-rmse:3.7377
[2600]	train-rmse:3.74744	dev-rmse:3.73578
[2700]	train-rmse:3.74365	dev-rmse:3.734
[2800]	train-rmse:3.74007	dev-rmse:3.73241
[2900]	train-rmse:3.73676	dev-rmse:3.73094
[3000]	train-rmse:3.73358	dev-rmse:3.72952
[3100]	train-rmse:3.73049	dev-rmse:3.72812
[3200]	train-rmse:3.72742	dev-rmse:3.72689
[3300]	train-rmse:3.72458	dev-rmse:3.72582
[3400]	train-rmse:3.72181	dev-rmse:3.72481
[3500]	train-rmse:3.71916	dev-rmse:3.72394
[3600]	train-rmse:3.71668	dev-rmse:3.72314
[3700]	train-rmse:3.71422	dev-rmse:3.72243
[3800]	train-rmse:3.71185	dev-rmse:3.72175
[3900]	train-rmse:3.70949	dev-rmse:3.7211
[4000]	train-rmse:3.70723	dev-rmse:3.72049
[4100]	train-rmse:3.70501	dev-rmse:3.71996
[4200]	train-rmse:3.70286	dev-rmse:3.71952
[4300]	train-rmse:3.70076	dev-rmse:3.71899
[4400]	train-rmse:3.69876	dev-rmse:3.71855
[4500]	train-rmse:3.69678	dev-rmse:3.71811
[4600]	train-rmse:3.69482	dev-rmse:3.71773
[4700]	train-rmse:3.69289	dev-rmse:3.71735
[4800]	train-rmse:3.69093	dev-rmse:3.71697
[4900]	train-rmse:3.68901	dev-rmse:3.7166
[5000]	train-rmse:3.6871	dev-rmse:3.71631
[5100]	train-rmse:3.68523	dev-rmse:3.71602
[5200]	train-rmse:3.68342	dev-rmse:3.71576
[5300]	train-rmse:3.68164	dev-rmse:3.7155
[5400]	train-rmse:3.67996	dev-rmse:3.7152
[5500]	train-rmse:3.67829	dev-rmse:3.7149
[5600]	train-rmse:3.67672	dev-rmse:3.7146
[5700]	train-rmse:3.67523	dev-rmse:3.71441
Stopping. Best iteration:
[5750]	train-rmse:3.67447	dev-rmse:3.71427

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 3, 'learning_rate': 0.1, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.31457	dev-rmse:5.24723
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:3.76383	dev-rmse:3.74385
[200]	train-rmse:3.69477	dev-rmse:3.7178
[300]	train-rmse:3.65693	dev-rmse:3.71179
Stopping. Best iteration:
[373]	train-rmse:3.63421	dev-rmse:3.70937

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 3, 'learning_rate': 0.1, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.31457	dev-rmse:5.24723
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:3.76991	dev-rmse:3.74783
[200]	train-rmse:3.70462	dev-rmse:3.72189
[300]	train-rmse:3.66835	dev-rmse:3.71474
Stopping. Best iteration:
[329]	train-rmse:3.65977	dev-rmse:3.71387

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 5, 'learning_rate': 0.001, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33814	dev-rmse:5.27104
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.31433	dev-rmse:5.24698
[200]	train-rmse:5.28895	dev-rmse:5.22135
[300]	train-rmse:5.26192	dev-rmse:5.19403
[400]	train-rmse:5.23315	dev-rmse:5.16495
[500]	train-rmse:5.20254	dev-rmse:5.13401
[600]	train-rmse:5.17	dev-rmse:5.10113
[700]	train-rmse:5.13545	dev-rmse:5.0662
[800]	train-rmse:5.0988	dev-rmse:5.02914
[900]	train-rmse:5.05996	dev-rmse:4.98988
[1000]	train-rmse:5.01887	dev-rmse:4.94832
[1100]	train-rmse:4.97551	dev-rmse:4.90447
[1200]	train-rmse:4.92985	dev-rmse:4.85831
[1300]	train-rmse:4.88185	dev-rmse:4.80986
[1400]	train-rmse:4.83156	dev-rmse:4.75917
[1500]	train-rmse:4.77947	dev-rmse:4.70662
[1600]	train-rmse:4.7257	dev-rmse:4.65246
[1700]	train-rmse:4.6706	dev-rmse:4.59707
[1800]	train-rmse:4.61542	dev-rmse:4.54174
[1900]	train-rmse:4.56156	dev-rmse:4.48792
[2000]	train-rmse:4.50968	dev-rmse:4.43624
[2100]	train-rmse:4.46021	dev-rmse:4.38702
[2200]	train-rmse:4.41344	dev-rmse:4.3406
[2300]	train-rmse:4.3691	dev-rmse:4.29676
[2400]	train-rmse:4.32715	dev-rmse:4.25546
[2500]	train-rmse:4.28764	dev-rmse:4.21681
[2600]	train-rmse:4.25041	dev-rmse:4.18063
[2700]	train-rmse:4.21535	dev-rmse:4.1468
[2800]	train-rmse:4.1824	dev-rmse:4.11559
[2900]	train-rmse:4.15145	dev-rmse:4.08681
[3000]	train-rmse:4.12231	dev-rmse:4.06012
[3100]	train-rmse:4.09488	dev-rmse:4.03548
[3200]	train-rmse:4.06907	dev-rmse:4.01264
[3300]	train-rmse:4.04481	dev-rmse:3.99146
[3400]	train-rmse:4.02201	dev-rmse:3.97194
[3500]	train-rmse:4.0006	dev-rmse:3.95383
[3600]	train-rmse:3.98044	dev-rmse:3.93709
[3700]	train-rmse:3.96155	dev-rmse:3.92169
[3800]	train-rmse:3.94381	dev-rmse:3.90745
[3900]	train-rmse:3.92722	dev-rmse:3.89428
[4000]	train-rmse:3.91161	dev-rmse:3.88213
[4100]	train-rmse:3.89699	dev-rmse:3.87085
[4200]	train-rmse:3.88326	dev-rmse:3.86053
[4300]	train-rmse:3.87029	dev-rmse:3.85094
[4400]	train-rmse:3.85814	dev-rmse:3.84202
[4500]	train-rmse:3.84674	dev-rmse:3.83386
[4600]	train-rmse:3.83588	dev-rmse:3.82624
[4700]	train-rmse:3.82558	dev-rmse:3.81916
[4800]	train-rmse:3.81575	dev-rmse:3.81262
[4900]	train-rmse:3.80649	dev-rmse:3.80656
[5000]	train-rmse:3.79766	dev-rmse:3.80092
[5100]	train-rmse:3.7892	dev-rmse:3.79571
[5200]	train-rmse:3.78109	dev-rmse:3.79088
[5300]	train-rmse:3.77353	dev-rmse:3.78627
[5400]	train-rmse:3.76611	dev-rmse:3.78202
[5500]	train-rmse:3.75891	dev-rmse:3.7781
[5600]	train-rmse:3.75212	dev-rmse:3.7744
[5700]	train-rmse:3.74565	dev-rmse:3.77099
[5800]	train-rmse:3.73942	dev-rmse:3.76785
[5900]	train-rmse:3.73345	dev-rmse:3.7648
[6000]	train-rmse:3.72765	dev-rmse:3.76189
[6100]	train-rmse:3.72215	dev-rmse:3.75919
[6200]	train-rmse:3.71681	dev-rmse:3.75672
[6300]	train-rmse:3.71176	dev-rmse:3.75435
[6400]	train-rmse:3.70687	dev-rmse:3.75217
[6500]	train-rmse:3.70204	dev-rmse:3.75015
[6600]	train-rmse:3.69742	dev-rmse:3.7482
[6700]	train-rmse:3.69304	dev-rmse:3.74629
[6800]	train-rmse:3.68881	dev-rmse:3.74461
[6900]	train-rmse:3.68476	dev-rmse:3.74297
[7000]	train-rmse:3.68073	dev-rmse:3.74137
[7100]	train-rmse:3.67685	dev-rmse:3.7399
[7200]	train-rmse:3.67307	dev-rmse:3.73847
[7300]	train-rmse:3.66942	dev-rmse:3.73713
[7400]	train-rmse:3.66586	dev-rmse:3.73583
[7500]	train-rmse:3.66237	dev-rmse:3.7346
[7600]	train-rmse:3.65901	dev-rmse:3.73344
[7700]	train-rmse:3.65575	dev-rmse:3.73238
[7800]	train-rmse:3.65257	dev-rmse:3.73132
[7900]	train-rmse:3.64949	dev-rmse:3.73038
[8000]	train-rmse:3.64644	dev-rmse:3.72944
[8100]	train-rmse:3.6435	dev-rmse:3.72851
[8200]	train-rmse:3.64049	dev-rmse:3.72771
[8300]	train-rmse:3.63765	dev-rmse:3.7269
[8400]	train-rmse:3.63491	dev-rmse:3.72611
[8500]	train-rmse:3.63219	dev-rmse:3.72533
[8600]	train-rmse:3.62954	dev-rmse:3.72463
[8700]	train-rmse:3.62696	dev-rmse:3.72392
[8800]	train-rmse:3.62433	dev-rmse:3.72326
[8900]	train-rmse:3.62171	dev-rmse:3.72256
[9000]	train-rmse:3.61924	dev-rmse:3.72201
[9100]	train-rmse:3.61681	dev-rmse:3.7214
[9200]	train-rmse:3.61448	dev-rmse:3.72088
[9300]	train-rmse:3.61205	dev-rmse:3.72034
[9400]	train-rmse:3.60969	dev-rmse:3.71981
[9500]	train-rmse:3.60736	dev-rmse:3.71932
[9600]	train-rmse:3.60508	dev-rmse:3.71883
[9700]	train-rmse:3.60278	dev-rmse:3.71842
[9800]	train-rmse:3.60061	dev-rmse:3.71795
[9900]	train-rmse:3.59846	dev-rmse:3.71753
[10000]	train-rmse:3.59627	dev-rmse:3.71712
[10100]	train-rmse:3.5941	dev-rmse:3.7167
[10200]	train-rmse:3.59206	dev-rmse:3.71636
[10300]	train-rmse:3.59003	dev-rmse:3.71603
[10400]	train-rmse:3.5879	dev-rmse:3.71573
[10500]	train-rmse:3.58593	dev-rmse:3.71539
[10600]	train-rmse:3.58392	dev-rmse:3.71505
[10700]	train-rmse:3.58198	dev-rmse:3.71472
[10800]	train-rmse:3.57993	dev-rmse:3.71441
[10900]	train-rmse:3.57802	dev-rmse:3.71409
[11000]	train-rmse:3.57606	dev-rmse:3.7138
[11100]	train-rmse:3.57415	dev-rmse:3.71348
[11200]	train-rmse:3.57231	dev-rmse:3.71318
[11300]	train-rmse:3.57046	dev-rmse:3.71295
[11400]	train-rmse:3.56858	dev-rmse:3.71269
[11500]	train-rmse:3.56677	dev-rmse:3.71246
[11600]	train-rmse:3.56495	dev-rmse:3.71223
[11700]	train-rmse:3.56316	dev-rmse:3.712
[11800]	train-rmse:3.56138	dev-rmse:3.71178
[11900]	train-rmse:3.55963	dev-rmse:3.71158
[12000]	train-rmse:3.55793	dev-rmse:3.71137
[12100]	train-rmse:3.5562	dev-rmse:3.71117
[12200]	train-rmse:3.55447	dev-rmse:3.71096
[12300]	train-rmse:3.55277	dev-rmse:3.71076
[12400]	train-rmse:3.55113	dev-rmse:3.71057
[12500]	train-rmse:3.54951	dev-rmse:3.71042
[12600]	train-rmse:3.54778	dev-rmse:3.71022
[12700]	train-rmse:3.54616	dev-rmse:3.71005
[12800]	train-rmse:3.54446	dev-rmse:3.70992
[12900]	train-rmse:3.54281	dev-rmse:3.70974
[13000]	train-rmse:3.54125	dev-rmse:3.70963
[13100]	train-rmse:3.53963	dev-rmse:3.70946
[13200]	train-rmse:3.53807	dev-rmse:3.7093
[13300]	train-rmse:3.53647	dev-rmse:3.70912
[13400]	train-rmse:3.53484	dev-rmse:3.70902
[13500]	train-rmse:3.53326	dev-rmse:3.70893
[13600]	train-rmse:3.53166	dev-rmse:3.70881
[13700]	train-rmse:3.53016	dev-rmse:3.70867
[13800]	train-rmse:3.52863	dev-rmse:3.70858
[13900]	train-rmse:3.52717	dev-rmse:3.70843
[14000]	train-rmse:3.52562	dev-rmse:3.70825
[14100]	train-rmse:3.52415	dev-rmse:3.70815
[14200]	train-rmse:3.52268	dev-rmse:3.70803
[14300]	train-rmse:3.52128	dev-rmse:3.70789
[14400]	train-rmse:3.5198	dev-rmse:3.70778
[14500]	train-rmse:3.51842	dev-rmse:3.70772
[14600]	train-rmse:3.517	dev-rmse:3.70764
[14700]	train-rmse:3.51558	dev-rmse:3.70756
Stopping. Best iteration:
[14776]	train-rmse:3.51448	dev-rmse:3.70748

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 5, 'learning_rate': 0.001, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33814	dev-rmse:5.27104
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.31433	dev-rmse:5.24698
[200]	train-rmse:5.28895	dev-rmse:5.22134
[300]	train-rmse:5.26192	dev-rmse:5.19403
[400]	train-rmse:5.23315	dev-rmse:5.16495
[500]	train-rmse:5.20254	dev-rmse:5.134
[600]	train-rmse:5.17	dev-rmse:5.10111
[700]	train-rmse:5.13545	dev-rmse:5.06618
[800]	train-rmse:5.09879	dev-rmse:5.02912
[900]	train-rmse:5.05996	dev-rmse:4.98985
[1000]	train-rmse:5.01887	dev-rmse:4.94829
[1100]	train-rmse:4.97551	dev-rmse:4.90446
[1200]	train-rmse:4.92983	dev-rmse:4.85829
[1300]	train-rmse:4.88182	dev-rmse:4.80987
[1400]	train-rmse:4.83149	dev-rmse:4.75921
[1500]	train-rmse:4.77936	dev-rmse:4.70664
[1600]	train-rmse:4.72555	dev-rmse:4.65236
[1700]	train-rmse:4.67053	dev-rmse:4.59699
[1800]	train-rmse:4.61527	dev-rmse:4.54156
[1900]	train-rmse:4.56157	dev-rmse:4.48787
[2000]	train-rmse:4.50974	dev-rmse:4.43625
[2100]	train-rmse:4.46031	dev-rmse:4.38709
[2200]	train-rmse:4.41368	dev-rmse:4.34089
[2300]	train-rmse:4.36942	dev-rmse:4.29713
[2400]	train-rmse:4.32754	dev-rmse:4.25596
[2500]	train-rmse:4.28808	dev-rmse:4.2175
[2600]	train-rmse:4.25102	dev-rmse:4.18165
[2700]	train-rmse:4.21602	dev-rmse:4.14793
[2800]	train-rmse:4.18313	dev-rmse:4.11657
[2900]	train-rmse:4.15221	dev-rmse:4.08787
[3000]	train-rmse:4.12294	dev-rmse:4.06125
[3100]	train-rmse:4.09553	dev-rmse:4.0369
[3200]	train-rmse:4.06974	dev-rmse:4.01427
[3300]	train-rmse:4.04566	dev-rmse:3.99362
[3400]	train-rmse:4.02297	dev-rmse:3.97438
[3500]	train-rmse:4.00148	dev-rmse:3.95589
[3600]	train-rmse:3.98123	dev-rmse:3.93911
[3700]	train-rmse:3.96231	dev-rmse:3.92381
[3800]	train-rmse:3.94464	dev-rmse:3.90963
[3900]	train-rmse:3.92813	dev-rmse:3.89638
[4000]	train-rmse:3.9129	dev-rmse:3.88392
[4100]	train-rmse:3.89856	dev-rmse:3.87263
[4200]	train-rmse:3.88513	dev-rmse:3.86232
[4300]	train-rmse:3.87263	dev-rmse:3.85269
[4400]	train-rmse:3.86052	dev-rmse:3.84371
[4500]	train-rmse:3.8493	dev-rmse:3.83546
[4600]	train-rmse:3.83848	dev-rmse:3.82796
[4700]	train-rmse:3.82814	dev-rmse:3.82089
[4800]	train-rmse:3.81826	dev-rmse:3.81436
[4900]	train-rmse:3.80896	dev-rmse:3.80842
[5000]	train-rmse:3.80021	dev-rmse:3.80289
[5100]	train-rmse:3.79185	dev-rmse:3.79773
[5200]	train-rmse:3.78386	dev-rmse:3.79291
[5300]	train-rmse:3.77627	dev-rmse:3.78841
[5400]	train-rmse:3.76926	dev-rmse:3.78413
[5500]	train-rmse:3.76253	dev-rmse:3.78019
[5600]	train-rmse:3.75609	dev-rmse:3.77652
[5700]	train-rmse:3.74995	dev-rmse:3.77309
[5800]	train-rmse:3.7442	dev-rmse:3.76989
[5900]	train-rmse:3.7387	dev-rmse:3.76691
[6000]	train-rmse:3.73342	dev-rmse:3.7641
[6100]	train-rmse:3.7283	dev-rmse:3.7614
[6200]	train-rmse:3.72328	dev-rmse:3.75889
[6300]	train-rmse:3.71849	dev-rmse:3.75655
[6400]	train-rmse:3.71383	dev-rmse:3.75433
[6500]	train-rmse:3.70943	dev-rmse:3.75227
[6600]	train-rmse:3.70511	dev-rmse:3.75035
[6700]	train-rmse:3.70099	dev-rmse:3.74852
[6800]	train-rmse:3.69696	dev-rmse:3.74691
[6900]	train-rmse:3.693	dev-rmse:3.7454
[7000]	train-rmse:3.68918	dev-rmse:3.74397
[7100]	train-rmse:3.68568	dev-rmse:3.7426
[7200]	train-rmse:3.6823	dev-rmse:3.74124
[7300]	train-rmse:3.67907	dev-rmse:3.73993
[7400]	train-rmse:3.67593	dev-rmse:3.73868
[7500]	train-rmse:3.67283	dev-rmse:3.73744
[7600]	train-rmse:3.66982	dev-rmse:3.73622
[7700]	train-rmse:3.66688	dev-rmse:3.73512
[7800]	train-rmse:3.66398	dev-rmse:3.73405
[7900]	train-rmse:3.66098	dev-rmse:3.73292
[8000]	train-rmse:3.6581	dev-rmse:3.73182
[8100]	train-rmse:3.65532	dev-rmse:3.73083
[8200]	train-rmse:3.65258	dev-rmse:3.72994
[8300]	train-rmse:3.64991	dev-rmse:3.72905
[8400]	train-rmse:3.64727	dev-rmse:3.7282
[8500]	train-rmse:3.64472	dev-rmse:3.72746
[8600]	train-rmse:3.64219	dev-rmse:3.72676
[8700]	train-rmse:3.63972	dev-rmse:3.72612
[8800]	train-rmse:3.63735	dev-rmse:3.72552
[8900]	train-rmse:3.63504	dev-rmse:3.72492
[9000]	train-rmse:3.63274	dev-rmse:3.72429
[9100]	train-rmse:3.63057	dev-rmse:3.72372
[9200]	train-rmse:3.62842	dev-rmse:3.72322
[9300]	train-rmse:3.62624	dev-rmse:3.72271
[9400]	train-rmse:3.62409	dev-rmse:3.7222
[9500]	train-rmse:3.62193	dev-rmse:3.72174
[9600]	train-rmse:3.6199	dev-rmse:3.72127
[9700]	train-rmse:3.61785	dev-rmse:3.72084
[9800]	train-rmse:3.6158	dev-rmse:3.72042
[9900]	train-rmse:3.61375	dev-rmse:3.71999
[10000]	train-rmse:3.61163	dev-rmse:3.71959
[10100]	train-rmse:3.60953	dev-rmse:3.71922
[10200]	train-rmse:3.60748	dev-rmse:3.71886
[10300]	train-rmse:3.60553	dev-rmse:3.71854
[10400]	train-rmse:3.60356	dev-rmse:3.71823
[10500]	train-rmse:3.60152	dev-rmse:3.71793
[10600]	train-rmse:3.59965	dev-rmse:3.71764
[10700]	train-rmse:3.59785	dev-rmse:3.71738
[10800]	train-rmse:3.59597	dev-rmse:3.7171
[10900]	train-rmse:3.59411	dev-rmse:3.71686
[11000]	train-rmse:3.59239	dev-rmse:3.71664
[11100]	train-rmse:3.59069	dev-rmse:3.71643
[11200]	train-rmse:3.58901	dev-rmse:3.71625
[11300]	train-rmse:3.58729	dev-rmse:3.71601
[11400]	train-rmse:3.58552	dev-rmse:3.7158
[11500]	train-rmse:3.58387	dev-rmse:3.71562
[11600]	train-rmse:3.58219	dev-rmse:3.71543
[11700]	train-rmse:3.58054	dev-rmse:3.71523
[11800]	train-rmse:3.57894	dev-rmse:3.71502
[11900]	train-rmse:3.57735	dev-rmse:3.71484
[12000]	train-rmse:3.57579	dev-rmse:3.71465
[12100]	train-rmse:3.57424	dev-rmse:3.71445
[12200]	train-rmse:3.57274	dev-rmse:3.71428
[12300]	train-rmse:3.57126	dev-rmse:3.71414
[12400]	train-rmse:3.5698	dev-rmse:3.71396
[12500]	train-rmse:3.56836	dev-rmse:3.71378
[12600]	train-rmse:3.56695	dev-rmse:3.71362
[12700]	train-rmse:3.56554	dev-rmse:3.71344
[12800]	train-rmse:3.56407	dev-rmse:3.7133
[12900]	train-rmse:3.56262	dev-rmse:3.71315
[13000]	train-rmse:3.56117	dev-rmse:3.71301
[13100]	train-rmse:3.55964	dev-rmse:3.71286
[13200]	train-rmse:3.55811	dev-rmse:3.71271
[13300]	train-rmse:3.5567	dev-rmse:3.71258
[13400]	train-rmse:3.55528	dev-rmse:3.71246
[13500]	train-rmse:3.55395	dev-rmse:3.71235
[13600]	train-rmse:3.55267	dev-rmse:3.71224
[13700]	train-rmse:3.55132	dev-rmse:3.71215
[13800]	train-rmse:3.55003	dev-rmse:3.71202
[13900]	train-rmse:3.54878	dev-rmse:3.71195
Stopping. Best iteration:
[13937]	train-rmse:3.54832	dev-rmse:3.71191

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 5, 'learning_rate': 0.01, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33605	dev-rmse:5.26894
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.01506	dev-rmse:4.94445
[200]	train-rmse:4.50476	dev-rmse:4.43126
[300]	train-rmse:4.11899	dev-rmse:4.05722
[400]	train-rmse:3.9092	dev-rmse:3.88063
[500]	train-rmse:3.7961	dev-rmse:3.80009
[600]	train-rmse:3.72793	dev-rmse:3.76204
[700]	train-rmse:3.68237	dev-rmse:3.74186
[800]	train-rmse:3.64754	dev-rmse:3.73035
[900]	train-rmse:3.62064	dev-rmse:3.72312
[1000]	train-rmse:3.5971	dev-rmse:3.71825
[1100]	train-rmse:3.57696	dev-rmse:3.71506
[1200]	train-rmse:3.55918	dev-rmse:3.71262
[1300]	train-rmse:3.54186	dev-rmse:3.71043
[1400]	train-rmse:3.52612	dev-rmse:3.70926
[1500]	train-rmse:3.51309	dev-rmse:3.70838
[1600]	train-rmse:3.499	dev-rmse:3.70763
Stopping. Best iteration:
[1629]	train-rmse:3.49535	dev-rmse:3.7075

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 5, 'learning_rate': 0.01, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33605	dev-rmse:5.26894
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.01505	dev-rmse:4.94444
[200]	train-rmse:4.5047	dev-rmse:4.43121
[300]	train-rmse:4.11935	dev-rmse:4.05781
[400]	train-rmse:3.91049	dev-rmse:3.88205
[500]	train-rmse:3.79864	dev-rmse:3.80221
[600]	train-rmse:3.73227	dev-rmse:3.76358
[700]	train-rmse:3.68838	dev-rmse:3.74392
[800]	train-rmse:3.65714	dev-rmse:3.73261
[900]	train-rmse:3.63203	dev-rmse:3.72526
[1000]	train-rmse:3.61101	dev-rmse:3.72034
[1100]	train-rmse:3.59125	dev-rmse:3.71757
[1200]	train-rmse:3.57502	dev-rmse:3.71566
[1300]	train-rmse:3.55977	dev-rmse:3.71401
[1400]	train-rmse:3.54588	dev-rmse:3.7129
[1500]	train-rmse:3.5325	dev-rmse:3.71224
[1600]	train-rmse:3.52019	dev-rmse:3.71156
[1700]	train-rmse:3.50931	dev-rmse:3.71081
Stopping. Best iteration:
[1696]	train-rmse:3.50968	dev-rmse:3.71081

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 5, 'learning_rate': 0.005, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33721	dev-rmse:5.27011
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.20127	dev-rmse:5.13273
[200]	train-rmse:5.01718	dev-rmse:4.9466
[300]	train-rmse:4.7773	dev-rmse:4.70444
[400]	train-rmse:4.50739	dev-rmse:4.43394
[500]	train-rmse:4.28566	dev-rmse:4.21469
[600]	train-rmse:4.12066	dev-rmse:4.05873
[700]	train-rmse:3.99938	dev-rmse:3.95309
[800]	train-rmse:3.91024	dev-rmse:3.88178
[900]	train-rmse:3.84522	dev-rmse:3.83344
[1000]	train-rmse:3.79647	dev-rmse:3.80049
[1100]	train-rmse:3.75788	dev-rmse:3.77784
[1200]	train-rmse:3.72655	dev-rmse:3.7613
[1300]	train-rmse:3.70144	dev-rmse:3.74968
[1400]	train-rmse:3.67993	dev-rmse:3.74079
[1500]	train-rmse:3.66217	dev-rmse:3.73429
[1600]	train-rmse:3.64636	dev-rmse:3.72927
[1700]	train-rmse:3.63232	dev-rmse:3.72529
[1800]	train-rmse:3.61897	dev-rmse:3.72188
[1900]	train-rmse:3.60692	dev-rmse:3.71922
[2000]	train-rmse:3.596	dev-rmse:3.71708
[2100]	train-rmse:3.58615	dev-rmse:3.71555
[2200]	train-rmse:3.57628	dev-rmse:3.71387
[2300]	train-rmse:3.56698	dev-rmse:3.71245
[2400]	train-rmse:3.55807	dev-rmse:3.7112
[2500]	train-rmse:3.54993	dev-rmse:3.71046
[2600]	train-rmse:3.54138	dev-rmse:3.70955
[2700]	train-rmse:3.53354	dev-rmse:3.70886
[2800]	train-rmse:3.5261	dev-rmse:3.70833
[2900]	train-rmse:3.51887	dev-rmse:3.70792
[3000]	train-rmse:3.51174	dev-rmse:3.70735
[3100]	train-rmse:3.50499	dev-rmse:3.70687
Stopping. Best iteration:
[3114]	train-rmse:3.50405	dev-rmse:3.7068

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 5, 'learning_rate': 0.005, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.33721	dev-rmse:5.27011
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:5.20127	dev-rmse:5.13273
[200]	train-rmse:5.01717	dev-rmse:4.94658
[300]	train-rmse:4.77721	dev-rmse:4.70447
[400]	train-rmse:4.50753	dev-rmse:4.43403
[500]	train-rmse:4.28616	dev-rmse:4.21563
[600]	train-rmse:4.12137	dev-rmse:4.05978
[700]	train-rmse:4.00022	dev-rmse:3.95476
[800]	train-rmse:3.91193	dev-rmse:3.88316
[900]	train-rmse:3.84856	dev-rmse:3.83488
[1000]	train-rmse:3.79962	dev-rmse:3.80254
[1100]	train-rmse:3.76201	dev-rmse:3.77989
[1200]	train-rmse:3.73299	dev-rmse:3.76379
[1300]	train-rmse:3.70904	dev-rmse:3.75211
[1400]	train-rmse:3.68929	dev-rmse:3.74381
[1500]	train-rmse:3.67249	dev-rmse:3.73746
[1600]	train-rmse:3.65793	dev-rmse:3.73201
[1700]	train-rmse:3.64446	dev-rmse:3.72771
[1800]	train-rmse:3.63257	dev-rmse:3.7244
[1900]	train-rmse:3.62193	dev-rmse:3.72179
[2000]	train-rmse:3.61176	dev-rmse:3.7197
[2100]	train-rmse:3.60202	dev-rmse:3.71811
[2200]	train-rmse:3.59265	dev-rmse:3.71693
[2300]	train-rmse:3.58422	dev-rmse:3.71594
[2400]	train-rmse:3.5763	dev-rmse:3.71503
[2500]	train-rmse:3.56875	dev-rmse:3.7144
[2600]	train-rmse:3.56072	dev-rmse:3.71362
[2700]	train-rmse:3.55313	dev-rmse:3.71277
[2800]	train-rmse:3.54657	dev-rmse:3.71243
[2900]	train-rmse:3.54002	dev-rmse:3.71202
[3000]	train-rmse:3.5335	dev-rmse:3.71155
[3100]	train-rmse:3.5274	dev-rmse:3.71124
Stopping. Best iteration:
[3166]	train-rmse:3.52356	dev-rmse:3.71095

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 5, 'learning_rate': 0.1, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.31457	dev-rmse:5.24723
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:3.60268	dev-rmse:3.71967
Stopping. Best iteration:
[140]	train-rmse:3.5338	dev-rmse:3.71209

Now we are training and validating hyperperameters: {'min_child_weight': 1, 'subsample': 1, 'max_depth': 5, 'learning_rate': 0.1, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True}.
[0]	train-rmse:5.31457	dev-rmse:5.24723
Multiple eval metrics have been passed: 'dev-rmse' will be used for early stopping.

Will train until dev-rmse hasn't improved in 20 rounds.
[100]	train-rmse:3.6067	dev-rmse:3.71944
[200]	train-rmse:3.47456	dev-rmse:3.70846
Stopping. Best iteration:
[184]	train-rmse:3.48825	dev-rmse:3.70799

Best parameters are: {'min_child_weight': 1, 'subsample': 0.8, 'max_depth': 5, 'learning_rate': 0.005, 'objective': 'count:poisson', 'booster': 'gbtree', 'eval_metric': 'rmse', 'silent': True} with num_boost_round as 3115 and rmse as 3.706802
Saving parameters...
